{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy kernel fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from typing import Callable, Tuple, Dict, Any, Optional\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from kl_decomposition import rectangle_rule, gauss_legendre_rule_multilevel\n",
    "\n",
    "# Enable 64-bit (double) precision in JAX\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8852ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(\n",
    "    user_fn: Callable[[np.ndarray], Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],\n",
    "    initial_mean: np.ndarray,\n",
    "    initial_std: np.ndarray,\n",
    "    pop_size: int = 100,\n",
    "    elite_frac: float = 0.2,\n",
    "    alpha_mean: float = 0.7,\n",
    "    alpha_std: float = 0.7,\n",
    "    n_iters: int = 100,\n",
    "    random_state: Optional[int] = None,\n",
    "    verbose: bool = False,\n",
    "    tol_std: float = 1e-6,\n",
    "    tol_g: float = 1e-6\n",
    ") -> Tuple[np.ndarray, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Method with independent Gaussian distribution and stopping criteria.\n",
    "\n",
    "    Args:\n",
    "        user_fn: function taking samples (shape Nxd) and returning\n",
    "            (updated_samples, scores, info_dict)\n",
    "        initial_mean: initial mean vector (d,)\n",
    "        initial_std: initial std deviation vector (d,)\n",
    "        pop_size: number of samples per iteration\n",
    "        elite_frac: fraction of samples selected as elites\n",
    "        alpha_mean: learning rate for mean update\n",
    "        alpha_std: learning rate for std update\n",
    "        n_iters: maximum number of iterations\n",
    "        random_state: RNG seed for reproducibility\n",
    "        verbose: print debug info each iteration\n",
    "        tol_std: stop if max(std) <= tol_std\n",
    "        tol_g: stop if info_dict['normg'] <= tol_g\n",
    "\n",
    "    Returns:\n",
    "        best_sample: best found sample (d,)\n",
    "        best_score: best objective value\n",
    "        info: dict with:\n",
    "            'iter': iteration index at stop\n",
    "            'best_score_history': list of best_score after each iteration\n",
    "            'max_std_history': list of max_std after each iteration\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    mean = np.array(initial_mean, dtype=float)\n",
    "    std = np.array(initial_std, dtype=float)\n",
    "    n_elite = max(1, int(np.ceil(pop_size * elite_frac)))\n",
    "    best_sample: Optional[np.ndarray] = None\n",
    "    best_score = np.inf\n",
    "\n",
    "    # history lists\n",
    "    best_score_history: list = []\n",
    "    max_std_history: list = []\n",
    "\n",
    "    for iteration in range(n_iters):\n",
    "        samples = rng.randn(pop_size, mean.size) * std + mean\n",
    "        samples, scores, info_dict = user_fn(samples)\n",
    "        idx = int(np.argmin(scores))\n",
    "        # update best if improved\n",
    "        if scores[idx] < best_score:\n",
    "            best_score = float(scores[idx])\n",
    "            best_sample = samples[idx]\n",
    "        # select elites and update distribution\n",
    "        elite = samples[np.argsort(scores)[:n_elite]]\n",
    "        elite_mean = elite.mean(axis=0)\n",
    "        elite_std = elite.std(axis=0, ddof=0)\n",
    "        mean = alpha_mean * elite_mean + (1 - alpha_mean) * mean\n",
    "        std = alpha_std * elite_std + (1 - alpha_std) * std\n",
    "        # record history\n",
    "        max_std = float(np.max(std))\n",
    "        best_score_history.append(best_score)\n",
    "        max_std_history.append(max_std)\n",
    "        # debug print\n",
    "        if verbose:\n",
    "            print(f\"Iter {iteration}: best_score={best_score}, max_std={max_std}, info={info_dict}\")\n",
    "        # stopping criteria\n",
    "        if max_std <= tol_std:\n",
    "            break\n",
    "        if info_dict.get('normg', np.inf) <= tol_g:\n",
    "            break\n",
    "\n",
    "    info = {\n",
    "        'iter': iteration,\n",
    "        'best_score_history': best_score_history,\n",
    "        'max_std_history': max_std_history\n",
    "    }\n",
    "    return best_sample, best_score, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a84814de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup squared exponential approximation\n",
    "# x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "x, w = gauss_legendre_rule_multilevel(0.0, 1.0, 100, L=5, ratio=0.2)\n",
    "\n",
    "target = np.exp(-x)\n",
    "\n",
    "# Convert to JAX arrays with float64 dtype\n",
    "x_j = jnp.array(x, dtype=jnp.float64)\n",
    "w_j = jnp.array(w, dtype=jnp.float64)\n",
    "target_j = jnp.array(target, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def obj_jax(p):\n",
    "    # p is already float64\n",
    "    n_param = p.shape[0] // 2\n",
    "    a = p[:n_param]\n",
    "    b = jnp.exp(p[n_param:])\n",
    "    # compute prediction and weighted RMS error\n",
    "    pred = jnp.sum(a[:, None] * jnp.exp(-b[:, None] * x_j[None, :] ** 2), axis=0)\n",
    "    diff = pred - target_j\n",
    "    return jnp.sum(w_j * diff ** 2)\n",
    "\n",
    "\n",
    "# gradients and Hessians in double precision\n",
    "grad_jax = jax.jit(jax.grad(obj_jax))\n",
    "hess_jax = jax.jit(jax.hessian(obj_jax))\n",
    "obj_jax_jit = jax.jit(obj_jax)\n",
    "\n",
    "\n",
    "def obj(p):\n",
    "    # ensure NumPyâ†’JAX conversion to float64\n",
    "    return float(obj_jax_jit(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def grad(p):\n",
    "    return np.array(grad_jax(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def hess(p):\n",
    "    return np.array(hess_jax(jnp.array(p, dtype=jnp.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e6be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def golden_section_search(f, a=0.0, b=1.0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Golden-section search to find the minimum of a unimodal function f on [a, b].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        The objective function to minimize.\n",
    "    a : float\n",
    "        Left endpoint of the initial interval.\n",
    "    b : float\n",
    "        Right endpoint of the initial interval.\n",
    "    tol : float\n",
    "        Tolerance for the interval width (stopping criterion).\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_min : float\n",
    "        Estimated position of the minimum.\n",
    "    f_min : float\n",
    "        Value of f at x_min.\n",
    "    \"\"\"\n",
    "    # Golden ratio constant\n",
    "    phi = (np.sqrt(5.0) - 1) / 2\n",
    "\n",
    "    # Initialize interior points\n",
    "    c = b - phi * (b - a)\n",
    "    d = a + phi * (b - a)\n",
    "    f_c = f(c)\n",
    "    f_d = f(d)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        if abs(b - a) < tol:\n",
    "            break\n",
    "\n",
    "        if f_c < f_d:\n",
    "            b, d, f_d = d, c, f_c\n",
    "            c = b - phi * (b - a)\n",
    "            f_c = f(c)\n",
    "        else:\n",
    "            a, c, f_c = c, d, f_d\n",
    "            d = a + phi * (b - a)\n",
    "            f_d = f(d)\n",
    "\n",
    "    # Choose the best of the final points\n",
    "    if f_c < f_d:\n",
    "        x_min, f_min = c, f_c\n",
    "    else:\n",
    "        x_min, f_min = d, f_d\n",
    "    # print(f\"Iterations: {_ + 1}\")\n",
    "    return x_min, f_min\n",
    "\n",
    "\n",
    "def newton(f, grad, hess, x0,\n",
    "                                   tol_grad=1e-6,\n",
    "                                   tol_step=1e-8,\n",
    "                                   stall_iter=5,\n",
    "                                   max_iter=100,\n",
    "                                   ls_bounds=(-1.0, 1.0),\n",
    "                                   verbose=False):\n",
    "    \"\"\"\n",
    "    Newton's method with backtracking line search and fallback to gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Objective function f(x).\n",
    "    grad : callable\n",
    "        Gradient of f: grad(x).\n",
    "    hess : callable\n",
    "        Hessian of f: hess(x).\n",
    "    x0 : ndarray\n",
    "        Initial guess.\n",
    "    tol_grad : float, optional\n",
    "        Tolerance for gradient norm.\n",
    "    tol_step : float, optional\n",
    "        Tolerance for step size norm for stall criterion.\n",
    "    stall_iter : int, optional\n",
    "        Number of consecutive small steps to trigger stop.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations.\n",
    "    ls_bounds : (float, float), optional\n",
    "        Initial interval for line search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : ndarray\n",
    "        Estimated minimizer.\n",
    "    grad_norm : float\n",
    "        Norm of gradient at the solution.\n",
    "    f_history : list of float\n",
    "        History of f values.\n",
    "    grad_norm_history : list of float\n",
    "        History of gradient norms.\n",
    "    \"\"\"\n",
    "    x = x0.astype(float)\n",
    "    f_history = []\n",
    "    grad_norm_history = []\n",
    "    fx = f(x)\n",
    "\n",
    "    stall_count = 0\n",
    "    trust = 1e-6 * np.eye(len(x))  # trust region to ensure positive definiteness\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        gx = grad(x)\n",
    "        grad_norm = np.linalg.norm(gx)\n",
    "\n",
    "        f_history.append(fx)\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "        # Check gradient convergence\n",
    "        if grad_norm < tol_grad:\n",
    "            break\n",
    "\n",
    "        # Try Newton step\n",
    "        try:\n",
    "            Hx = hess(x)\n",
    "            p = -np.linalg.solve(Hx + trust, gx)\n",
    "            trust /= 2\n",
    "            # ensure descent direction\n",
    "            # if np.dot(p, gx) >= 0:\n",
    "            #     raise np.linalg.LinAlgError\n",
    "        except np.linalg.LinAlgError:\n",
    "            # fallback to steepest descent\n",
    "            print(\"Hessian not positive definite, using gradient descent\")\n",
    "            p = -gx\n",
    "            trust *= 2\n",
    "        # Backtracking line search\n",
    "        alpha, fx = golden_section_search(\n",
    "            lambda alpha: f(x + alpha * p),\n",
    "            a=ls_bounds[0], b=ls_bounds[1], tol=1e-12, max_iter=100\n",
    "        )\n",
    "\n",
    "        # Update\n",
    "        x_new = x + alpha * p\n",
    "        step_norm = np.linalg.norm(x_new - x)\n",
    "        x = x_new\n",
    "\n",
    "        # Stall criterion\n",
    "        if step_norm < tol_step:\n",
    "            stall_count += 1\n",
    "            if stall_count >= stall_iter:\n",
    "                break\n",
    "        else:\n",
    "            stall_count = 0\n",
    "\n",
    "        # print debug info\n",
    "        if verbose:\n",
    "            print(f\"Iter {k}: f={fx:.6e}, grad_norm={grad_norm:.6e}, step_norm={step_norm:.6e}, alpha={alpha:.6f}\")\n",
    "\n",
    "    return x, grad_norm, f_history, grad_norm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910c5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "\n",
    "def optimal_a(d: np.ndarray,\n",
    "              w: np.ndarray,\n",
    "              target: np.ndarray,\n",
    "              b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve for a >= 0 minimizing\n",
    "        sum_j w[j] * (sum_k a[k] * exp(-b[k] * d[j]**2) - target[j])**2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : (J,) array\n",
    "        Sample points x_j.\n",
    "    w : (J,) array\n",
    "        Quadrature weights.\n",
    "    target : (J,) array\n",
    "        Target values at each d[j].\n",
    "    b : (K,) array\n",
    "        Exponents b_k.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : (K,) array\n",
    "        Non-negative coefficient vector minimizing the weighted LS error.\n",
    "    \"\"\"\n",
    "    # Build design matrix Î¦[j,k] = exp(-b[k] * d[j]**2)\n",
    "    # Note: np.outer(d**2, b) yields shape (J, K)\n",
    "    Phi = np.exp(-np.outer(d**2, b))     # shape (J, K)\n",
    "\n",
    "    # Incorporate weights by scaling rows\n",
    "    W_sqrt = np.sqrt(w)                  # shape (J,)\n",
    "    Phi_w = Phi * W_sqrt[:, None]        # shape (J, K)\n",
    "    y_w = target * W_sqrt              # shape (J,)\n",
    "\n",
    "    a, _ = nnls(Phi_w, y_w)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "270cb850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal a: [0.45757188 0.14340726 0.1858919  0.085486   0.05802303 0.02829363\n",
      " 0.01921406 0.00867671 0.00633343 0.00307163 0.00109343 0.00233199]\n",
      "Iter 0: f=3.089823e-09, grad_norm=1.683832e-07, step_norm=4.225435e-02, alpha=0.745473\n",
      "Iter 1: f=9.174800e-10, grad_norm=3.411903e-05, step_norm=3.807852e-02, alpha=1.385765\n",
      "Iter 2: f=5.221088e-10, grad_norm=5.715637e-06, step_norm=3.657300e-02, alpha=0.829631\n",
      "Iter 3: f=3.405776e-10, grad_norm=1.222625e-05, step_norm=3.876889e-02, alpha=2.000000\n",
      "Iter 4: f=2.985170e-10, grad_norm=1.664150e-06, step_norm=2.418213e-02, alpha=0.801900\n",
      "Iter 5: f=2.635481e-10, grad_norm=5.975671e-06, step_norm=3.227149e-02, alpha=2.000000\n",
      "Iter 6: f=2.511809e-10, grad_norm=7.353893e-07, step_norm=2.197126e-02, alpha=0.544414\n",
      "Iter 7: f=2.255864e-10, grad_norm=3.994145e-06, step_norm=6.327883e-02, alpha=2.000000\n",
      "Iter 8: f=2.113498e-10, grad_norm=2.504246e-06, step_norm=5.998495e-02, alpha=1.087297\n",
      "Iter 9: f=2.087246e-10, grad_norm=1.231766e-06, step_norm=1.512231e-02, alpha=-0.062920\n",
      "Iter 10: f=1.650667e-10, grad_norm=2.264491e-06, step_norm=2.419799e-01, alpha=1.442887\n",
      "Iter 11: f=1.315502e-10, grad_norm=7.469881e-06, step_norm=2.365158e-01, alpha=1.049156\n",
      "Iter 12: f=1.112758e-10, grad_norm=5.713731e-07, step_norm=1.841570e-01, alpha=0.476216\n",
      "Iter 13: f=8.538075e-11, grad_norm=3.094078e-06, step_norm=3.057045e-01, alpha=0.919286\n",
      "Iter 14: f=6.854441e-11, grad_norm=4.815738e-06, step_norm=3.299347e-01, alpha=1.162109\n",
      "Iter 15: f=5.820971e-11, grad_norm=3.510480e-06, step_norm=2.837839e-01, alpha=0.926097\n",
      "Iter 16: f=5.034703e-11, grad_norm=4.100404e-06, step_norm=3.026052e-01, alpha=0.911781\n",
      "Iter 17: f=4.367146e-11, grad_norm=2.875051e-06, step_norm=2.948864e-01, alpha=1.247728\n",
      "Iter 18: f=4.029687e-11, grad_norm=2.955587e-06, step_norm=2.463947e-01, alpha=0.684583\n",
      "Iter 19: f=3.731960e-11, grad_norm=2.053369e-06, step_norm=1.397145e-01, alpha=1.024654\n",
      "Iter 20: f=3.428139e-11, grad_norm=2.434941e-06, step_norm=2.743557e-01, alpha=2.000000\n",
      "Iter 21: f=3.307983e-11, grad_norm=1.038598e-06, step_norm=4.248675e-02, alpha=0.544810\n",
      "Iter 22: f=3.091269e-11, grad_norm=2.034943e-06, step_norm=1.523343e-01, alpha=2.000000\n",
      "Iter 23: f=2.964554e-11, grad_norm=1.512809e-06, step_norm=4.008671e-02, alpha=1.411687\n",
      "Iter 24: f=2.854616e-11, grad_norm=1.687203e-06, step_norm=8.450358e-02, alpha=1.310105\n",
      "Iter 25: f=2.781793e-11, grad_norm=1.157028e-06, step_norm=1.591928e-02, alpha=0.668622\n",
      "Iter 26: f=2.670676e-11, grad_norm=1.480678e-06, step_norm=5.999070e-02, alpha=1.910193\n",
      "Iter 27: f=2.605459e-11, grad_norm=1.164700e-06, step_norm=2.527467e-02, alpha=1.023321\n",
      "Iter 28: f=2.550345e-11, grad_norm=1.181688e-06, step_norm=5.714182e-02, alpha=1.000697\n",
      "Iter 29: f=2.500817e-11, grad_norm=1.024115e-06, step_norm=1.657081e-02, alpha=0.972137\n",
      "Iter 30: f=2.451920e-11, grad_norm=1.081009e-06, step_norm=5.511079e-02, alpha=1.234494\n",
      "Iter 31: f=2.415271e-11, grad_norm=8.691025e-07, step_norm=1.334008e-02, alpha=0.832019\n",
      "Iter 32: f=2.370491e-11, grad_norm=9.824834e-07, step_norm=4.825992e-02, alpha=1.595837\n",
      "Iter 33: f=2.341915e-11, grad_norm=7.732327e-07, step_norm=1.311413e-02, alpha=0.854218\n",
      "Iter 34: f=2.309345e-11, grad_norm=8.405248e-07, step_norm=4.494626e-02, alpha=1.513414\n",
      "Iter 35: f=2.287729e-11, grad_norm=6.751352e-07, step_norm=1.189901e-02, alpha=0.862693\n",
      "Iter 36: f=2.262917e-11, grad_norm=7.288765e-07, step_norm=4.087008e-02, alpha=1.555483\n",
      "Iter 37: f=2.246728e-11, grad_norm=5.855352e-07, step_norm=1.103811e-02, alpha=0.877888\n",
      "Iter 38: f=2.228392e-11, grad_norm=6.222855e-07, step_norm=3.711462e-02, alpha=1.575294\n",
      "Iter 39: f=2.216501e-11, grad_norm=5.032208e-07, step_norm=1.015648e-02, alpha=0.899073\n",
      "Iter 40: f=2.203296e-11, grad_norm=5.235408e-07, step_norm=3.337794e-02, alpha=1.594524\n",
      "Iter 41: f=2.194786e-11, grad_norm=4.260900e-07, step_norm=9.351840e-03, alpha=0.927005\n",
      "Iter 42: f=2.185623e-11, grad_norm=4.303432e-07, step_norm=2.970872e-02, alpha=1.610011\n",
      "Iter 43: f=2.179761e-11, grad_norm=3.531658e-07, step_norm=8.644690e-03, alpha=0.964679\n",
      "Iter 44: f=2.173759e-11, grad_norm=3.401021e-07, step_norm=2.605346e-02, alpha=1.612499\n",
      "Iter 45: f=2.169976e-11, grad_norm=2.818300e-07, step_norm=8.196508e-03, alpha=1.015014\n",
      "Iter 46: f=2.166706e-11, grad_norm=2.327601e-07, step_norm=2.076973e-02, alpha=1.452017\n",
      "Iter 47: f=2.164654e-11, grad_norm=1.431776e-07, step_norm=5.242350e-03, alpha=0.443224\n",
      "Iter 48: f=2.162126e-11, grad_norm=2.528769e-07, step_norm=1.561943e-02, alpha=2.000000\n",
      "Iter 49: f=2.161168e-11, grad_norm=3.571888e-08, step_norm=6.330653e-03, alpha=0.004779\n",
      "Iter 50: f=2.160970e-11, grad_norm=1.255403e-07, step_norm=5.515040e-03, alpha=0.500217\n",
      "Iter 51: f=2.160053e-11, grad_norm=4.333518e-08, step_norm=9.608370e-03, alpha=0.651119\n",
      "Iter 52: f=2.159957e-11, grad_norm=1.399066e-07, step_norm=2.176462e-03, alpha=0.347715\n",
      "Iter 53: f=2.159552e-11, grad_norm=7.122474e-08, step_norm=3.216297e-03, alpha=0.416868\n",
      "Iter 54: f=2.159299e-11, grad_norm=3.365525e-08, step_norm=4.314888e-03, alpha=0.084795\n",
      "Iter 55: f=2.158861e-11, grad_norm=6.732078e-08, step_norm=4.882770e-03, alpha=0.784505\n",
      "Iter 56: f=2.158803e-11, grad_norm=4.310586e-08, step_norm=1.908780e-03, alpha=0.313841\n",
      "Iter 57: f=2.158541e-11, grad_norm=1.653257e-08, step_norm=5.560285e-03, alpha=0.499586\n",
      "Iter 58: f=2.158337e-11, grad_norm=6.381267e-08, step_norm=2.033130e-03, alpha=-0.271832\n",
      "Iter 59: f=2.157953e-11, grad_norm=3.662199e-08, step_norm=4.393403e-03, alpha=1.411663\n",
      "Iter 60: f=2.157655e-11, grad_norm=6.486024e-08, step_norm=1.894273e-03, alpha=1.751488\n",
      "Iter 61: f=2.157344e-11, grad_norm=3.846304e-08, step_norm=1.207253e-03, alpha=1.885001\n",
      "Iter 62: f=2.157057e-11, grad_norm=6.101229e-08, step_norm=4.453173e-03, alpha=1.891986\n",
      "Iter 63: f=2.156950e-11, grad_norm=2.319566e-08, step_norm=3.826501e-03, alpha=0.216042\n",
      "Iter 64: f=2.156789e-11, grad_norm=3.276750e-08, step_norm=7.649812e-04, alpha=0.300261\n",
      "Iter 65: f=2.156429e-11, grad_norm=2.530996e-08, step_norm=6.221287e-03, alpha=2.000000\n",
      "Iter 66: f=2.156346e-11, grad_norm=2.696245e-08, step_norm=3.742292e-03, alpha=0.152387\n",
      "Iter 67: f=2.156189e-11, grad_norm=2.209360e-08, step_norm=7.595558e-04, alpha=0.386273\n",
      "Iter 68: f=2.156187e-11, grad_norm=3.071439e-08, step_norm=1.381132e-03, alpha=-0.009097\n",
      "Iter 69: f=2.155975e-11, grad_norm=2.631167e-08, step_norm=4.893924e-03, alpha=1.131095\n",
      "Iter 70: f=2.155734e-11, grad_norm=3.582940e-08, step_norm=2.315512e-03, alpha=-0.164468\n",
      "Iter 71: f=2.155636e-11, grad_norm=2.822000e-08, step_norm=3.617171e-03, alpha=0.016062\n",
      "Iter 72: f=2.155434e-11, grad_norm=3.349386e-08, step_norm=1.035404e-03, alpha=0.601836\n",
      "Iter 73: f=2.155303e-11, grad_norm=3.657011e-08, step_norm=3.509214e-03, alpha=0.707167\n",
      "Iter 74: f=2.155140e-11, grad_norm=2.019252e-08, step_norm=1.000147e-03, alpha=0.177116\n",
      "Iter 75: f=2.155051e-11, grad_norm=2.732188e-08, step_norm=3.676917e-03, alpha=0.071838\n",
      "Iter 76: f=2.154870e-11, grad_norm=2.702159e-08, step_norm=8.643233e-04, alpha=0.531152\n",
      "Iter 77: f=2.154766e-11, grad_norm=3.476912e-08, step_norm=3.428944e-03, alpha=0.537040\n",
      "Iter 78: f=2.154591e-11, grad_norm=1.835641e-08, step_norm=1.354235e-03, alpha=0.322493\n",
      "Iter 79: f=2.154543e-11, grad_norm=3.092134e-08, step_norm=3.320990e-03, alpha=-0.026528\n",
      "Iter 80: f=2.154318e-11, grad_norm=1.883847e-08, step_norm=1.045216e-03, alpha=0.916717\n",
      "Iter 81: f=2.153959e-11, grad_norm=5.123318e-08, step_norm=2.403898e-03, alpha=2.000000\n",
      "Iter 82: f=2.153940e-11, grad_norm=3.189745e-08, step_norm=3.136101e-03, alpha=-0.093860\n",
      "Iter 83: f=2.153782e-11, grad_norm=1.347352e-08, step_norm=1.223523e-03, alpha=0.567689\n",
      "Iter 84: f=2.153454e-11, grad_norm=4.465207e-08, step_norm=1.343646e-03, alpha=1.882546\n",
      "Iter 85: f=2.153114e-11, grad_norm=5.935058e-08, step_norm=2.901144e-03, alpha=2.000000\n",
      "Iter 86: f=2.153104e-11, grad_norm=3.298427e-08, step_norm=2.553084e-03, alpha=-0.112312\n",
      "Iter 87: f=2.152839e-11, grad_norm=2.150329e-08, step_norm=4.451379e-03, alpha=1.306253\n",
      "Iter 88: f=2.152819e-11, grad_norm=3.198167e-08, step_norm=2.766490e-03, alpha=-0.029060\n",
      "Iter 89: f=2.152562e-11, grad_norm=1.783345e-08, step_norm=1.110794e-03, alpha=1.168386\n",
      "Iter 90: f=2.152195e-11, grad_norm=5.942220e-08, step_norm=2.032667e-03, alpha=2.000000\n",
      "Iter 91: f=2.152164e-11, grad_norm=3.573193e-08, step_norm=2.871640e-03, alpha=0.256561\n",
      "Iter 92: f=2.151905e-11, grad_norm=1.107545e-08, step_norm=5.201973e-03, alpha=0.633499\n",
      "Iter 93: f=2.151660e-11, grad_norm=2.654292e-08, step_norm=9.346928e-04, alpha=0.834810\n",
      "Iter 94: f=2.151292e-11, grad_norm=4.864097e-08, step_norm=3.147079e-03, alpha=2.000000\n",
      "Iter 95: f=2.151174e-11, grad_norm=2.263507e-08, step_norm=4.399464e-03, alpha=0.139941\n",
      "Iter 96: f=2.150932e-11, grad_norm=3.016011e-08, step_norm=1.285707e-03, alpha=0.063994\n",
      "Iter 97: f=2.150717e-11, grad_norm=8.154262e-08, step_norm=1.562196e-03, alpha=-2.000000\n",
      "Iter 98: f=2.150699e-11, grad_norm=1.174248e-07, step_norm=8.811489e-04, alpha=0.275189\n",
      "Iter 99: f=2.150696e-11, grad_norm=7.800515e-08, step_norm=4.744680e-04, alpha=-0.052440\n",
      "Iter 100: f=2.150506e-11, grad_norm=8.713715e-08, step_norm=2.445097e-03, alpha=-0.553863\n",
      "Iter 101: f=2.149980e-11, grad_norm=7.393998e-08, step_norm=1.313836e-03, alpha=1.198544\n",
      "Iter 102: f=2.149714e-11, grad_norm=6.340609e-08, step_norm=3.418548e-03, alpha=1.582482\n",
      "Iter 103: f=2.149614e-11, grad_norm=2.604702e-08, step_norm=3.779229e-03, alpha=0.062123\n",
      "Iter 104: f=2.149430e-11, grad_norm=3.065210e-08, step_norm=9.704323e-04, alpha=0.407509\n",
      "Iter 105: f=2.149425e-11, grad_norm=3.116713e-08, step_norm=1.695016e-03, alpha=0.055762\n",
      "Iter 106: f=2.149282e-11, grad_norm=2.264744e-08, step_norm=3.932336e-03, alpha=0.383458\n",
      "Iter 107: f=2.149095e-11, grad_norm=3.872474e-08, step_norm=6.878768e-04, alpha=0.210875\n",
      "Iter 108: f=2.148765e-11, grad_norm=2.316858e-08, step_norm=1.007569e-03, alpha=1.515274\n",
      "Iter 109: f=2.148463e-11, grad_norm=6.824762e-08, step_norm=4.495533e-03, alpha=1.705913\n",
      "Iter 110: f=2.148358e-11, grad_norm=2.441962e-08, step_norm=3.746814e-03, alpha=0.146647\n",
      "Iter 111: f=2.148170e-11, grad_norm=3.323800e-08, step_norm=8.980554e-04, alpha=0.365400\n",
      "Iter 112: f=2.148141e-11, grad_norm=2.967303e-08, step_norm=2.706503e-03, alpha=0.062844\n",
      "Iter 113: f=2.147849e-11, grad_norm=1.900577e-08, step_norm=1.379022e-03, alpha=1.062389\n",
      "Iter 114: f=2.147504e-11, grad_norm=5.424317e-08, step_norm=5.748667e-03, alpha=2.000000\n",
      "Iter 115: f=2.147226e-11, grad_norm=1.417326e-08, step_norm=4.179443e-03, alpha=0.576083\n",
      "Iter 116: f=2.147021e-11, grad_norm=2.485304e-08, step_norm=4.535547e-03, alpha=0.658372\n",
      "Iter 117: f=2.146680e-11, grad_norm=4.533723e-08, step_norm=1.151398e-03, alpha=0.164347\n",
      "Iter 118: f=2.146229e-11, grad_norm=2.462211e-08, step_norm=2.828337e-03, alpha=2.000000\n",
      "Iter 119: f=2.146229e-11, grad_norm=3.616526e-08, step_norm=1.164683e-03, alpha=0.037748\n",
      "Iter 120: f=2.146172e-11, grad_norm=3.268088e-08, step_norm=4.605385e-03, alpha=-0.090335\n",
      "Iter 121: f=2.145972e-11, grad_norm=5.091452e-09, step_norm=9.931793e-04, alpha=-0.159534\n",
      "Iter 122: f=2.145855e-11, grad_norm=4.270958e-08, step_norm=5.561312e-03, alpha=0.589827\n",
      "Iter 123: f=2.145618e-11, grad_norm=1.848582e-08, step_norm=1.984515e-03, alpha=0.324607\n",
      "Iter 124: f=2.145604e-11, grad_norm=3.501158e-08, step_norm=2.653920e-03, alpha=-0.138959\n",
      "Iter 125: f=2.145403e-11, grad_norm=2.568990e-08, step_norm=3.817116e-03, alpha=0.650825\n",
      "Iter 126: f=2.145127e-11, grad_norm=4.248750e-08, step_norm=8.645082e-04, alpha=0.128556\n",
      "Iter 127: f=2.144759e-11, grad_norm=2.386932e-08, step_norm=4.698446e-03, alpha=1.541147\n",
      "Iter 128: f=2.144577e-11, grad_norm=3.887080e-08, step_norm=9.670398e-04, alpha=-0.400809\n",
      "Iter 129: f=2.144261e-11, grad_norm=1.611478e-08, step_norm=4.090417e-03, alpha=0.583926\n",
      "Iter 130: f=2.144165e-11, grad_norm=6.215647e-08, step_norm=1.679679e-03, alpha=-0.174185\n",
      "Iter 131: f=2.144158e-11, grad_norm=9.789247e-08, step_norm=1.039140e-03, alpha=-0.969199\n",
      "Iter 132: f=2.144028e-11, grad_norm=1.431603e-07, step_norm=8.272062e-04, alpha=0.478409\n",
      "Iter 133: f=2.143809e-11, grad_norm=5.522924e-08, step_norm=7.412821e-04, alpha=0.191890\n",
      "Iter 134: f=2.143301e-11, grad_norm=2.357518e-08, step_norm=6.239425e-03, alpha=2.000000\n",
      "Iter 135: f=2.143044e-11, grad_norm=3.114504e-08, step_norm=1.309997e-03, alpha=0.125937\n",
      "Iter 136: f=2.143037e-11, grad_norm=8.115649e-08, step_norm=4.674703e-04, alpha=0.274799\n",
      "Iter 137: f=2.142763e-11, grad_norm=5.607227e-08, step_norm=1.912581e-03, alpha=-1.028052\n",
      "Iter 138: f=2.142432e-11, grad_norm=2.966510e-08, step_norm=8.573035e-04, alpha=0.594322\n",
      "Iter 139: f=2.142064e-11, grad_norm=4.453033e-08, step_norm=7.347106e-03, alpha=1.930984\n",
      "Iter 140: f=2.141853e-11, grad_norm=5.872933e-08, step_norm=1.562215e-03, alpha=-0.378131\n",
      "Iter 141: f=2.141368e-11, grad_norm=3.676613e-08, step_norm=4.112762e-03, alpha=1.115738\n",
      "Iter 142: f=2.140874e-11, grad_norm=4.548416e-08, step_norm=4.929586e-03, alpha=2.000000\n",
      "Iter 143: f=2.140669e-11, grad_norm=4.129445e-08, step_norm=1.588278e-03, alpha=0.641959\n",
      "Iter 144: f=2.140415e-11, grad_norm=5.766972e-08, step_norm=4.481203e-03, alpha=1.656948\n",
      "Iter 145: f=2.140197e-11, grad_norm=2.711676e-08, step_norm=1.132644e-03, alpha=0.153759\n",
      "Iter 146: f=2.140047e-11, grad_norm=2.799378e-08, step_norm=4.096610e-03, alpha=0.031145\n",
      "Iter 147: f=2.139796e-11, grad_norm=4.130352e-08, step_norm=1.249416e-03, alpha=0.497168\n",
      "Iter 148: f=2.139793e-11, grad_norm=3.408028e-08, step_norm=1.463759e-03, alpha=0.070509\n",
      "Iter 149: f=2.139674e-11, grad_norm=2.634975e-08, step_norm=3.732122e-03, alpha=0.135659\n",
      "Iter 150: f=2.139440e-11, grad_norm=3.595112e-08, step_norm=9.315259e-04, alpha=0.477098\n",
      "Iter 151: f=2.139310e-11, grad_norm=3.519336e-08, step_norm=3.378740e-03, alpha=0.516591\n",
      "Iter 152: f=2.139089e-11, grad_norm=2.277276e-08, step_norm=1.424087e-03, alpha=0.320564\n",
      "Iter 153: f=2.139057e-11, grad_norm=3.316325e-08, step_norm=3.089791e-03, alpha=-0.091636\n",
      "Iter 154: f=2.138693e-11, grad_norm=2.086097e-08, step_norm=2.728957e-03, alpha=1.297600\n",
      "Iter 155: f=2.138222e-11, grad_norm=6.440091e-08, step_norm=1.381574e-03, alpha=2.000000\n",
      "Iter 156: f=2.137960e-11, grad_norm=4.243069e-08, step_norm=5.326106e-03, alpha=1.283703\n",
      "Iter 157: f=2.137845e-11, grad_norm=2.537780e-08, step_norm=3.769329e-03, alpha=0.064036\n",
      "Iter 158: f=2.137626e-11, grad_norm=3.647876e-08, step_norm=1.010252e-03, alpha=0.373463\n",
      "Iter 159: f=2.137608e-11, grad_norm=3.178398e-08, step_norm=2.172739e-03, alpha=0.107029\n",
      "Iter 160: f=2.137352e-11, grad_norm=1.972737e-08, step_norm=4.710536e-03, alpha=0.651738\n",
      "Iter 161: f=2.137240e-11, grad_norm=5.595741e-08, step_norm=8.754112e-04, alpha=0.073838\n",
      "Iter 162: f=2.137041e-11, grad_norm=8.417247e-08, step_norm=1.893458e-03, alpha=-0.989789\n",
      "Iter 163: f=2.136665e-11, grad_norm=8.981479e-08, step_norm=8.231177e-04, alpha=0.408342\n",
      "Iter 164: f=2.136058e-11, grad_norm=2.953785e-08, step_norm=4.700077e-03, alpha=2.000000\n",
      "Iter 165: f=2.135826e-11, grad_norm=4.534232e-08, step_norm=2.180531e-03, alpha=0.286429\n",
      "Iter 166: f=2.135435e-11, grad_norm=7.901306e-08, step_norm=1.421640e-03, alpha=1.622316\n",
      "Iter 167: f=2.135264e-11, grad_norm=4.393941e-08, step_norm=4.460891e-03, alpha=0.830744\n",
      "Iter 168: f=2.135042e-11, grad_norm=2.400852e-08, step_norm=1.056732e-03, alpha=0.126942\n",
      "Iter 169: f=2.134941e-11, grad_norm=3.003913e-08, step_norm=3.849769e-03, alpha=-0.035701\n",
      "Iter 170: f=2.134666e-11, grad_norm=3.147642e-08, step_norm=1.477589e-03, alpha=0.654747\n",
      "Iter 171: f=2.134418e-11, grad_norm=4.470832e-08, step_norm=4.078084e-03, alpha=1.167366\n",
      "Iter 172: f=2.134210e-11, grad_norm=2.924222e-08, step_norm=1.308101e-03, alpha=-0.152730\n",
      "Iter 173: f=2.134041e-11, grad_norm=2.426951e-08, step_norm=4.183674e-03, alpha=0.065990\n",
      "Iter 174: f=2.133810e-11, grad_norm=4.530935e-08, step_norm=9.139046e-04, alpha=0.232016\n",
      "Iter 175: f=2.133647e-11, grad_norm=2.591057e-08, step_norm=3.717357e-03, alpha=0.300946\n",
      "Iter 176: f=2.133401e-11, grad_norm=4.483556e-08, step_norm=7.938492e-04, alpha=0.315470\n",
      "Iter 177: f=2.132846e-11, grad_norm=2.870464e-08, step_norm=6.233050e-03, alpha=2.000000\n",
      "Iter 178: f=2.132630e-11, grad_norm=2.078177e-08, step_norm=4.941617e-03, alpha=0.445036\n",
      "Iter 179: f=2.132462e-11, grad_norm=4.635921e-08, step_norm=9.965590e-04, alpha=0.072806\n",
      "Iter 180: f=2.132183e-11, grad_norm=8.410834e-08, step_norm=1.687693e-03, alpha=-1.334873\n",
      "Iter 181: f=2.131806e-11, grad_norm=8.898635e-08, step_norm=9.216804e-04, alpha=0.438571\n",
      "Iter 182: f=2.131569e-11, grad_norm=3.037431e-08, step_norm=3.573064e-03, alpha=0.425916\n",
      "Iter 183: f=2.131295e-11, grad_norm=5.370535e-08, step_norm=7.530652e-04, alpha=0.273160\n",
      "Iter 184: f=2.130830e-11, grad_norm=2.648957e-08, step_norm=1.134336e-03, alpha=1.604216\n",
      "Iter 185: f=2.130477e-11, grad_norm=7.907282e-08, step_norm=4.365398e-03, alpha=1.414172\n",
      "Iter 186: f=2.130453e-11, grad_norm=3.726333e-08, step_norm=2.225115e-03, alpha=0.212125\n",
      "Iter 187: f=2.130235e-11, grad_norm=1.754170e-08, step_norm=4.579775e-03, alpha=0.278008\n",
      "Iter 188: f=2.130109e-11, grad_norm=5.585870e-08, step_norm=9.000149e-04, alpha=0.069848\n",
      "Iter 189: f=2.129885e-11, grad_norm=8.633180e-08, step_norm=2.246420e-03, alpha=-1.042839\n",
      "Iter 190: f=2.129503e-11, grad_norm=9.309684e-08, step_norm=9.888298e-04, alpha=0.408235\n",
      "Iter 191: f=2.129230e-11, grad_norm=3.158793e-08, step_norm=4.051128e-03, alpha=0.461609\n",
      "Iter 192: f=2.128932e-11, grad_norm=6.115862e-08, step_norm=7.822193e-04, alpha=0.225817\n",
      "Iter 193: f=2.128403e-11, grad_norm=2.651736e-08, step_norm=3.857914e-03, alpha=1.678174\n",
      "Iter 194: f=2.127989e-11, grad_norm=8.157252e-08, step_norm=3.022992e-03, alpha=1.557007\n",
      "Iter 195: f=2.127750e-11, grad_norm=4.552457e-08, step_norm=4.172377e-03, alpha=1.118867\n",
      "Iter 196: f=2.127309e-11, grad_norm=2.912260e-08, step_norm=4.417654e-03, alpha=-0.177163\n",
      "Iter 197: f=2.127110e-11, grad_norm=7.936420e-09, step_norm=1.232112e-03, alpha=-0.253930\n",
      "Iter 198: f=2.127097e-11, grad_norm=5.339680e-08, step_norm=2.091075e-03, alpha=0.202732\n",
      "Iter 199: f=2.126864e-11, grad_norm=3.327211e-08, step_norm=5.283859e-03, alpha=-0.205762\n",
      "Iter 200: f=2.126658e-11, grad_norm=5.071502e-08, step_norm=7.940335e-04, alpha=-0.191289\n",
      "Iter 201: f=2.126378e-11, grad_norm=3.479808e-08, step_norm=3.672658e-03, alpha=0.122547\n",
      "Iter 202: f=2.126098e-11, grad_norm=8.355056e-08, step_norm=8.674031e-04, alpha=-0.024246\n",
      "Iter 203: f=2.125353e-11, grad_norm=5.217454e-08, step_norm=5.849596e-03, alpha=1.580199\n",
      "Iter 204: f=2.125040e-11, grad_norm=6.335647e-09, step_norm=8.749465e-04, alpha=0.384954\n",
      "Iter 205: f=2.124515e-11, grad_norm=5.320093e-08, step_norm=3.474447e-03, alpha=1.791548\n",
      "Iter 206: f=2.124114e-11, grad_norm=6.020891e-08, step_norm=6.026985e-03, alpha=1.468400\n",
      "Iter 207: f=2.124093e-11, grad_norm=3.714259e-08, step_norm=2.219196e-03, alpha=0.193039\n",
      "Iter 208: f=2.123880e-11, grad_norm=1.874489e-08, step_norm=4.414500e-03, alpha=0.226958\n",
      "Iter 209: f=2.123251e-11, grad_norm=5.501419e-08, step_norm=2.244559e-03, alpha=0.278807\n",
      "Iter 210: f=2.123124e-11, grad_norm=1.256171e-07, step_norm=9.283841e-04, alpha=0.582919\n",
      "Iter 211: f=2.122797e-11, grad_norm=4.149274e-08, step_norm=2.204621e-03, alpha=-0.368314\n",
      "Iter 212: f=2.122663e-11, grad_norm=2.552315e-08, step_norm=3.917706e-03, alpha=0.156324\n",
      "Iter 213: f=2.122379e-11, grad_norm=3.867365e-08, step_norm=9.224484e-04, alpha=0.389106\n",
      "Iter 214: f=2.122185e-11, grad_norm=3.555289e-08, step_norm=3.639092e-03, alpha=0.612088\n",
      "Iter 215: f=2.121920e-11, grad_norm=3.069946e-08, step_norm=1.101207e-03, alpha=0.192807\n",
      "Iter 216: f=2.121837e-11, grad_norm=3.082520e-08, step_norm=3.589956e-03, alpha=-0.038855\n",
      "Iter 217: f=2.121474e-11, grad_norm=3.002491e-08, step_norm=1.677472e-03, alpha=0.809221\n",
      "Iter 218: f=2.120941e-11, grad_norm=5.673136e-08, step_norm=3.907378e-03, alpha=2.000000\n",
      "Iter 219: f=2.120530e-11, grad_norm=1.200835e-08, step_norm=6.290948e-03, alpha=0.263617\n",
      "Iter 220: f=2.120216e-11, grad_norm=4.583959e-08, step_norm=1.094289e-03, alpha=0.488814\n",
      "Iter 221: f=2.120002e-11, grad_norm=3.902251e-08, step_norm=4.020251e-03, alpha=0.706639\n",
      "Iter 222: f=2.119729e-11, grad_norm=3.096680e-08, step_norm=1.074239e-03, alpha=0.146281\n",
      "Iter 223: f=2.119631e-11, grad_norm=3.064190e-08, step_norm=3.718010e-03, alpha=-0.044810\n",
      "Iter 224: f=2.119265e-11, grad_norm=3.363507e-08, step_norm=1.664219e-03, alpha=0.753260\n",
      "Iter 225: f=2.118763e-11, grad_norm=5.386695e-08, step_norm=4.928727e-03, alpha=2.000000\n",
      "Iter 226: f=2.118474e-11, grad_norm=1.424668e-08, step_norm=1.001337e-03, alpha=-0.024842\n",
      "Iter 227: f=2.118418e-11, grad_norm=4.122665e-08, step_norm=4.171618e-03, alpha=-0.272535\n",
      "Iter 228: f=2.118144e-11, grad_norm=2.460184e-08, step_norm=4.096203e-03, alpha=0.473034\n",
      "Iter 229: f=2.117579e-11, grad_norm=6.198831e-08, step_norm=1.959760e-03, alpha=0.230383\n",
      "Iter 230: f=2.117488e-11, grad_norm=1.301394e-07, step_norm=7.482915e-04, alpha=0.526220\n",
      "Iter 231: f=2.117125e-11, grad_norm=5.237946e-08, step_norm=1.884267e-03, alpha=-0.513967\n",
      "Iter 232: f=2.116664e-11, grad_norm=2.061842e-08, step_norm=3.523871e-03, alpha=1.176200\n",
      "Iter 233: f=2.116018e-11, grad_norm=6.392020e-08, step_norm=1.514466e-03, alpha=2.000000\n",
      "Iter 234: f=2.115723e-11, grad_norm=5.363824e-08, step_norm=4.743752e-03, alpha=0.958285\n",
      "Iter 235: f=2.115638e-11, grad_norm=4.749613e-08, step_norm=3.194270e-03, alpha=0.468253\n",
      "Iter 236: f=2.115171e-11, grad_norm=1.238148e-08, step_norm=5.882999e-03, alpha=0.466674\n",
      "Iter 237: f=2.114780e-11, grad_norm=5.534082e-08, step_norm=1.337800e-03, alpha=0.564040\n",
      "Iter 238: f=2.114642e-11, grad_norm=4.203942e-08, step_norm=3.245560e-03, alpha=0.460872\n",
      "Iter 239: f=2.114234e-11, grad_norm=2.179988e-08, step_norm=3.266835e-03, alpha=0.539370\n",
      "Iter 240: f=2.114226e-11, grad_norm=3.736006e-08, step_norm=1.688103e-03, alpha=0.099983\n",
      "Iter 241: f=2.114053e-11, grad_norm=2.686405e-08, step_norm=3.972711e-03, alpha=0.100304\n",
      "Iter 242: f=2.113710e-11, grad_norm=4.821235e-08, step_norm=1.158835e-03, alpha=0.448497\n",
      "Iter 243: f=2.113621e-11, grad_norm=3.758689e-08, step_norm=3.043474e-03, alpha=0.301574\n",
      "Iter 244: f=2.113108e-11, grad_norm=2.155044e-08, step_norm=4.980712e-03, alpha=0.865395\n",
      "Iter 245: f=2.112348e-11, grad_norm=2.651713e-08, step_norm=8.974228e-03, alpha=2.000000\n",
      "Iter 246: f=2.112344e-11, grad_norm=7.408091e-08, step_norm=5.598014e-04, alpha=-0.042649\n",
      "Iter 247: f=2.111594e-11, grad_norm=8.261582e-08, step_norm=5.847018e-03, alpha=-0.759005\n",
      "Iter 248: f=2.110843e-11, grad_norm=4.067571e-08, step_norm=3.231345e-03, alpha=1.438032\n",
      "Iter 249: f=2.110819e-11, grad_norm=3.483562e-08, step_norm=3.703777e-03, alpha=-0.074471\n",
      "Iter 250: f=2.110412e-11, grad_norm=1.388615e-08, step_norm=2.478097e-03, alpha=0.660294\n",
      "Iter 251: f=2.109771e-11, grad_norm=5.237122e-08, step_norm=5.812740e-03, alpha=2.000000\n",
      "Iter 252: f=2.109435e-11, grad_norm=5.931099e-08, step_norm=4.543410e-03, alpha=1.391563\n",
      "Iter 253: f=2.108911e-11, grad_norm=1.475075e-08, step_norm=5.889634e-03, alpha=0.569626\n",
      "Iter 254: f=2.108492e-11, grad_norm=6.525559e-08, step_norm=1.168916e-03, alpha=0.464536\n",
      "Iter 255: f=2.108490e-11, grad_norm=3.611713e-08, step_norm=1.195290e-03, alpha=0.054766\n",
      "Iter 256: f=2.108366e-11, grad_norm=3.075125e-08, step_norm=3.619675e-03, alpha=0.026978\n",
      "Iter 257: f=2.107914e-11, grad_norm=4.170755e-08, step_norm=1.644156e-03, alpha=0.759690\n",
      "Iter 258: f=2.107386e-11, grad_norm=5.848249e-08, step_norm=4.942413e-03, alpha=2.000000\n",
      "Iter 259: f=2.107117e-11, grad_norm=2.515134e-08, step_norm=1.276425e-03, alpha=-0.198307\n",
      "Iter 260: f=2.106934e-11, grad_norm=3.216424e-08, step_norm=4.787963e-03, alpha=-0.162531\n",
      "Iter 261: f=2.106588e-11, grad_norm=4.519460e-08, step_norm=1.728628e-03, alpha=0.302768\n",
      "Iter 262: f=2.106572e-11, grad_norm=3.686984e-08, step_norm=1.917248e-03, alpha=0.145729\n",
      "Iter 263: f=2.106347e-11, grad_norm=2.517709e-08, step_norm=3.847682e-03, alpha=0.142219\n",
      "Iter 264: f=2.106000e-11, grad_norm=6.200843e-08, step_norm=1.010380e-03, alpha=0.226989\n",
      "Iter 265: f=2.105814e-11, grad_norm=3.032406e-08, step_norm=3.615260e-03, alpha=0.219195\n",
      "Iter 266: f=2.105431e-11, grad_norm=5.410346e-08, step_norm=1.017235e-03, alpha=0.420230\n",
      "Iter 267: f=2.104801e-11, grad_norm=3.903398e-08, step_norm=4.541529e-03, alpha=2.000000\n",
      "Iter 268: f=2.103775e-11, grad_norm=2.602804e-09, step_norm=3.237784e-03, alpha=-0.129575\n",
      "Iter 269: f=2.103674e-11, grad_norm=9.399276e-08, step_norm=1.264027e-03, alpha=0.504985\n",
      "Iter 270: f=2.103256e-11, grad_norm=4.250725e-08, step_norm=3.023270e-03, alpha=-0.495528\n",
      "Iter 271: f=2.103122e-11, grad_norm=3.417465e-08, step_norm=4.573466e-03, alpha=-0.129215\n",
      "Iter 272: f=2.102650e-11, grad_norm=3.483346e-08, step_norm=2.332082e-03, alpha=0.701643\n",
      "Iter 273: f=2.102326e-11, grad_norm=5.156108e-08, step_norm=4.522811e-03, alpha=0.980050\n",
      "Iter 274: f=2.102158e-11, grad_norm=3.210938e-08, step_norm=2.270341e-03, alpha=-0.069575\n",
      "Iter 275: f=2.101874e-11, grad_norm=5.873174e-08, step_norm=2.580436e-03, alpha=-1.003280\n",
      "Iter 276: f=2.101144e-11, grad_norm=3.629683e-08, step_norm=5.244525e-03, alpha=0.361213\n",
      "Iter 277: f=2.101005e-11, grad_norm=3.833292e-08, step_norm=3.239670e-03, alpha=-0.064447\n",
      "Iter 278: f=2.100215e-11, grad_norm=6.180179e-08, step_norm=3.859902e-03, alpha=1.226927\n",
      "Iter 279: f=2.099607e-11, grad_norm=8.129163e-08, step_norm=2.612110e-03, alpha=2.000000\n",
      "Iter 280: f=2.099089e-11, grad_norm=1.221517e-08, step_norm=5.412509e-03, alpha=0.128093\n",
      "Iter 281: f=2.098995e-11, grad_norm=2.223991e-08, step_norm=1.811757e-03, alpha=-0.599106\n",
      "Iter 282: f=2.098527e-11, grad_norm=1.848653e-08, step_norm=2.306985e-03, alpha=-0.797601\n",
      "Iter 283: f=2.097658e-11, grad_norm=9.320185e-08, step_norm=4.653747e-03, alpha=0.839801\n",
      "Iter 284: f=2.096834e-11, grad_norm=2.537986e-08, step_norm=2.020294e-03, alpha=-0.075942\n",
      "Iter 285: f=2.096417e-11, grad_norm=3.148587e-08, step_norm=4.263134e-03, alpha=0.082852\n",
      "Iter 286: f=2.095844e-11, grad_norm=8.816608e-08, step_norm=1.414114e-03, alpha=0.474009\n",
      "Iter 287: f=2.095651e-11, grad_norm=3.543803e-08, step_norm=3.845488e-03, alpha=-0.030445\n",
      "Iter 288: f=2.094862e-11, grad_norm=5.759277e-08, step_norm=3.322939e-03, alpha=1.116798\n",
      "Iter 289: f=2.094461e-11, grad_norm=7.878655e-08, step_norm=3.509120e-03, alpha=1.362669\n",
      "Iter 290: f=2.094165e-11, grad_norm=3.102523e-08, step_norm=1.797364e-03, alpha=-0.247721\n",
      "Iter 291: f=2.093916e-11, grad_norm=2.886849e-08, step_norm=4.855673e-03, alpha=-0.105475\n",
      "Iter 292: f=2.093538e-11, grad_norm=5.605409e-08, step_norm=1.289470e-03, alpha=0.172885\n",
      "Iter 293: f=2.093393e-11, grad_norm=3.222790e-08, step_norm=3.462673e-03, alpha=-0.019032\n",
      "Iter 294: f=2.092862e-11, grad_norm=5.396765e-08, step_norm=2.120112e-03, alpha=0.715233\n",
      "Iter 295: f=2.092392e-11, grad_norm=5.819994e-08, step_norm=3.979838e-03, alpha=1.512787\n",
      "Iter 296: f=2.092081e-11, grad_norm=3.443603e-08, step_norm=1.861125e-03, alpha=-0.383258\n",
      "Iter 297: f=2.091806e-11, grad_norm=2.532821e-08, step_norm=4.929035e-03, alpha=-0.061555\n",
      "Iter 298: f=2.091146e-11, grad_norm=6.147991e-08, step_norm=1.851235e-03, alpha=0.141642\n",
      "Iter 299: f=2.091002e-11, grad_norm=1.422403e-07, step_norm=6.587592e-04, alpha=0.593351\n",
      "Iter 300: f=2.090613e-11, grad_norm=4.757200e-08, step_norm=2.331515e-03, alpha=-0.387191\n",
      "Iter 301: f=2.090402e-11, grad_norm=2.526860e-08, step_norm=4.148151e-03, alpha=0.011655\n",
      "Iter 302: f=2.090000e-11, grad_norm=5.650092e-08, step_norm=1.283339e-03, alpha=0.272333\n",
      "Iter 303: f=2.089982e-11, grad_norm=3.435352e-08, step_norm=2.247064e-03, alpha=-0.082167\n",
      "Iter 304: f=2.089363e-11, grad_norm=3.211129e-08, step_norm=6.212127e-03, alpha=1.053921\n",
      "Iter 305: f=2.089184e-11, grad_norm=8.366620e-08, step_norm=2.815803e-03, alpha=-0.319854\n",
      "Iter 306: f=2.089165e-11, grad_norm=1.362148e-07, step_norm=9.076618e-04, alpha=0.291803\n",
      "Iter 307: f=2.088668e-11, grad_norm=9.149904e-08, step_norm=3.292558e-03, alpha=-0.858719\n",
      "Iter 308: f=2.088040e-11, grad_norm=6.671228e-08, step_norm=1.670786e-03, alpha=0.668453\n",
      "Iter 309: f=2.087213e-11, grad_norm=6.035657e-08, step_norm=1.940463e-03, alpha=2.000000\n",
      "Iter 310: f=2.087009e-11, grad_norm=5.051881e-08, step_norm=7.481699e-03, alpha=0.438561\n",
      "Iter 311: f=2.086700e-11, grad_norm=1.803616e-08, step_norm=4.696544e-03, alpha=0.013903\n",
      "Iter 312: f=2.086323e-11, grad_norm=7.373652e-08, step_norm=2.304819e-03, alpha=-0.284578\n",
      "Iter 313: f=2.085991e-11, grad_norm=4.020188e-08, step_norm=3.877623e-03, alpha=0.238753\n",
      "Iter 314: f=2.085191e-11, grad_norm=8.955952e-08, step_norm=1.587087e-03, alpha=0.226657\n",
      "Iter 315: f=2.084398e-11, grad_norm=4.485735e-08, step_norm=6.061443e-03, alpha=1.040079\n",
      "Iter 316: f=2.083899e-11, grad_norm=9.220367e-08, step_norm=4.495425e-03, alpha=-0.819841\n",
      "Iter 317: f=2.083153e-11, grad_norm=6.222879e-08, step_norm=3.134583e-03, alpha=0.875093\n",
      "Iter 318: f=2.082467e-11, grad_norm=7.478895e-08, step_norm=3.056774e-03, alpha=2.000000\n",
      "Iter 319: f=2.082103e-11, grad_norm=1.215877e-08, step_norm=1.458311e-03, alpha=-0.156248\n",
      "Iter 320: f=2.082058e-11, grad_norm=5.170860e-08, step_norm=5.305949e-03, alpha=-0.363430\n",
      "Iter 321: f=2.081962e-11, grad_norm=2.996384e-08, step_norm=3.834320e-03, alpha=-0.137680\n",
      "Iter 322: f=2.081382e-11, grad_norm=3.979780e-08, step_norm=3.234814e-03, alpha=0.629838\n",
      "Iter 323: f=2.080593e-11, grad_norm=4.982495e-08, step_norm=6.181039e-03, alpha=2.000000\n",
      "Iter 324: f=2.080050e-11, grad_norm=4.498501e-09, step_norm=1.745395e-03, alpha=-0.118910\n",
      "Iter 325: f=2.079943e-11, grad_norm=6.502058e-08, step_norm=4.195881e-03, alpha=0.501813\n",
      "Iter 326: f=2.079519e-11, grad_norm=1.531041e-08, step_norm=5.003359e-03, alpha=0.114797\n",
      "Iter 327: f=2.079029e-11, grad_norm=8.639183e-08, step_norm=4.733128e-03, alpha=-0.700438\n",
      "Iter 328: f=2.077980e-11, grad_norm=6.178137e-08, step_norm=8.903448e-03, alpha=0.557382\n",
      "Iter 329: f=2.077857e-11, grad_norm=1.074572e-07, step_norm=1.394279e-03, alpha=0.462156\n",
      "Iter 330: f=2.077433e-11, grad_norm=4.358358e-08, step_norm=1.746434e-03, alpha=-0.280635\n",
      "Iter 331: f=2.077416e-11, grad_norm=3.209609e-08, step_norm=1.564614e-03, alpha=0.143542\n",
      "Iter 332: f=2.077138e-11, grad_norm=2.365904e-08, step_norm=4.101896e-03, alpha=0.023165\n",
      "Iter 333: f=2.076750e-11, grad_norm=7.746907e-08, step_norm=1.832073e-03, alpha=-0.023301\n",
      "Iter 334: f=2.076495e-11, grad_norm=3.917480e-08, step_norm=3.597029e-03, alpha=0.139709\n",
      "Iter 335: f=2.075970e-11, grad_norm=8.176850e-08, step_norm=1.647704e-03, alpha=0.280323\n",
      "Iter 336: f=2.075589e-11, grad_norm=4.006187e-08, step_norm=2.358917e-03, alpha=0.629739\n",
      "Iter 337: f=2.075116e-11, grad_norm=4.398605e-08, step_norm=1.389985e-03, alpha=0.114522\n",
      "Iter 338: f=2.075033e-11, grad_norm=3.483704e-08, step_norm=3.392903e-03, alpha=-0.138898\n",
      "Iter 339: f=2.073956e-11, grad_norm=3.941726e-08, step_norm=7.530013e-03, alpha=1.555584\n",
      "Iter 340: f=2.073184e-11, grad_norm=2.767920e-08, step_norm=2.182348e-03, alpha=1.070611\n",
      "Iter 341: f=2.072357e-11, grad_norm=8.926318e-08, step_norm=6.115572e-03, alpha=2.000000\n",
      "Iter 342: f=2.072234e-11, grad_norm=2.359661e-08, step_norm=3.759914e-03, alpha=-0.108173\n",
      "Iter 343: f=2.071803e-11, grad_norm=4.213167e-08, step_norm=2.124334e-03, alpha=0.168507\n",
      "Iter 344: f=2.071460e-11, grad_norm=4.269718e-08, step_norm=3.364487e-03, alpha=0.870348\n",
      "Iter 345: f=2.071431e-11, grad_norm=3.326023e-08, step_norm=9.263036e-04, alpha=0.018176\n",
      "Iter 346: f=2.071050e-11, grad_norm=4.655641e-08, step_norm=2.221215e-03, alpha=-0.323725\n",
      "Iter 347: f=2.070798e-11, grad_norm=2.585433e-08, step_norm=4.450209e-03, alpha=-0.097478\n",
      "Iter 348: f=2.070333e-11, grad_norm=6.491693e-08, step_norm=1.671973e-03, alpha=0.168179\n",
      "Iter 349: f=2.070276e-11, grad_norm=3.636242e-08, step_norm=2.766459e-03, alpha=-0.123109\n",
      "Iter 350: f=2.069285e-11, grad_norm=4.738694e-08, step_norm=6.970355e-03, alpha=1.334219\n",
      "Iter 351: f=2.068016e-11, grad_norm=7.695041e-08, step_norm=2.503757e-03, alpha=0.524854\n",
      "Iter 352: f=2.067656e-11, grad_norm=1.596263e-07, step_norm=1.664979e-03, alpha=0.705479\n",
      "Iter 353: f=2.066992e-11, grad_norm=3.650446e-08, step_norm=4.476154e-03, alpha=-0.430326\n",
      "Iter 354: f=2.066584e-11, grad_norm=5.794513e-08, step_norm=6.683849e-03, alpha=1.311832\n",
      "Iter 355: f=2.065851e-11, grad_norm=3.112465e-08, step_norm=5.231493e-03, alpha=0.309746\n",
      "Iter 356: f=2.065837e-11, grad_norm=3.927627e-08, step_norm=1.929792e-03, alpha=-0.124533\n",
      "Iter 357: f=2.065639e-11, grad_norm=4.215858e-08, step_norm=3.205245e-03, alpha=0.084005\n",
      "Iter 358: f=2.064916e-11, grad_norm=7.470968e-08, step_norm=2.178265e-03, alpha=0.624605\n",
      "Iter 359: f=2.064274e-11, grad_norm=6.078064e-08, step_norm=2.770744e-03, alpha=1.580140\n",
      "Iter 360: f=2.063896e-11, grad_norm=1.761548e-08, step_norm=2.258847e-03, alpha=-0.349206\n",
      "Iter 361: f=2.063794e-11, grad_norm=5.080924e-08, step_norm=6.531217e-03, alpha=-0.388012\n",
      "Iter 362: f=2.063478e-11, grad_norm=1.974692e-08, step_norm=4.396365e-03, alpha=-0.016086\n",
      "Iter 363: f=2.063071e-11, grad_norm=8.084552e-08, step_norm=2.385839e-03, alpha=-0.197823\n",
      "Iter 364: f=2.062766e-11, grad_norm=4.218826e-08, step_norm=3.417373e-03, alpha=0.064755\n",
      "Iter 365: f=2.062212e-11, grad_norm=9.615738e-08, step_norm=1.623434e-03, alpha=0.146632\n",
      "Iter 366: f=2.062148e-11, grad_norm=4.664876e-08, step_norm=2.332544e-03, alpha=-0.033879\n",
      "Iter 367: f=2.061221e-11, grad_norm=6.578932e-08, step_norm=2.922493e-03, alpha=1.029762\n",
      "Iter 368: f=2.060202e-11, grad_norm=9.195264e-08, step_norm=3.867452e-03, alpha=2.000000\n",
      "Iter 369: f=2.060008e-11, grad_norm=4.622929e-08, step_norm=5.876835e-03, alpha=0.616018\n",
      "Iter 370: f=2.059947e-11, grad_norm=2.287334e-08, step_norm=1.160631e-03, alpha=-0.029674\n",
      "Iter 371: f=2.059513e-11, grad_norm=4.241546e-08, step_norm=1.994728e-03, alpha=-0.299614\n",
      "Iter 372: f=2.059358e-11, grad_norm=3.365523e-08, step_norm=4.102020e-03, alpha=-0.319239\n",
      "Iter 373: f=2.058368e-11, grad_norm=5.603401e-08, step_norm=8.676477e-03, alpha=0.603135\n",
      "Iter 374: f=2.057659e-11, grad_norm=1.041279e-07, step_norm=1.635979e-03, alpha=0.341387\n",
      "Iter 375: f=2.057281e-11, grad_norm=4.371310e-08, step_norm=2.544744e-03, alpha=0.542453\n",
      "Iter 376: f=2.056757e-11, grad_norm=4.979175e-08, step_norm=2.003669e-03, alpha=0.232874\n",
      "Iter 377: f=2.056757e-11, grad_norm=3.971825e-08, step_norm=3.516662e-04, alpha=0.026148\n",
      "Iter 378: f=2.056728e-11, grad_norm=3.837439e-08, step_norm=2.796622e-03, alpha=-0.183424\n",
      "Iter 379: f=2.056483e-11, grad_norm=3.581200e-08, step_norm=3.477361e-03, alpha=0.102703\n",
      "Iter 380: f=2.055817e-11, grad_norm=7.390791e-08, step_norm=1.691275e-03, alpha=0.468386\n",
      "Iter 381: f=2.055335e-11, grad_norm=4.961667e-08, step_norm=3.645376e-03, alpha=0.928144\n",
      "Iter 382: f=2.055260e-11, grad_norm=3.454983e-08, step_norm=1.972948e-03, alpha=-0.050961\n",
      "Iter 383: f=2.054886e-11, grad_norm=4.914726e-08, step_norm=2.415388e-03, alpha=-0.429473\n",
      "Iter 384: f=2.054525e-11, grad_norm=2.100397e-08, step_norm=4.724256e-03, alpha=-0.098437\n",
      "Iter 385: f=2.054172e-11, grad_norm=8.260986e-08, step_norm=2.546377e-03, alpha=-0.148441\n",
      "Iter 386: f=2.053762e-11, grad_norm=4.256395e-08, step_norm=3.741282e-03, alpha=0.070281\n",
      "Iter 387: f=2.053619e-11, grad_norm=1.094176e-07, step_norm=1.298434e-03, alpha=0.048800\n",
      "Iter 388: f=2.053531e-11, grad_norm=1.340618e-07, step_norm=2.146281e-03, alpha=-0.528484\n",
      "Iter 389: f=2.053336e-11, grad_norm=1.537467e-07, step_norm=1.860881e-03, alpha=-0.140255\n",
      "Iter 390: f=2.051613e-11, grad_norm=1.317739e-07, step_norm=7.188896e-03, alpha=0.980929\n",
      "Iter 391: f=2.051100e-11, grad_norm=9.179340e-08, step_norm=1.787036e-03, alpha=1.003738\n",
      "Iter 392: f=2.050569e-11, grad_norm=7.422161e-09, step_norm=2.239155e-03, alpha=-0.027696\n",
      "Iter 393: f=2.050556e-11, grad_norm=6.203559e-08, step_norm=2.074086e-03, alpha=0.230734\n",
      "Iter 394: f=2.050315e-11, grad_norm=3.958335e-08, step_norm=5.576307e-03, alpha=-0.314851\n",
      "Iter 395: f=2.049705e-11, grad_norm=4.607811e-08, step_norm=3.179125e-03, alpha=0.379027\n",
      "Iter 396: f=2.049588e-11, grad_norm=4.658039e-08, step_norm=2.723851e-03, alpha=0.375733\n",
      "Iter 397: f=2.049066e-11, grad_norm=2.364941e-08, step_norm=4.782087e-03, alpha=0.271053\n",
      "Iter 398: f=2.048840e-11, grad_norm=9.928182e-08, step_norm=1.387600e-03, alpha=0.182630\n",
      "Iter 399: f=2.048812e-11, grad_norm=1.202284e-07, step_norm=1.240912e-03, alpha=0.342094\n",
      "Iter 400: f=2.048324e-11, grad_norm=7.395192e-08, step_norm=4.881437e-03, alpha=-0.779929\n",
      "Iter 401: f=2.047860e-11, grad_norm=3.265133e-08, step_norm=5.377141e-03, alpha=0.195062\n",
      "Iter 402: f=2.047608e-11, grad_norm=1.054854e-07, step_norm=3.014198e-03, alpha=-0.540509\n",
      "Iter 403: f=2.047531e-11, grad_norm=9.502180e-08, step_norm=1.207539e-03, alpha=-0.056384\n",
      "Iter 404: f=2.047269e-11, grad_norm=1.214590e-07, step_norm=1.739965e-03, alpha=-0.204216\n",
      "Iter 405: f=2.046262e-11, grad_norm=1.007407e-07, step_norm=5.942774e-03, alpha=0.583914\n",
      "Iter 406: f=2.046186e-11, grad_norm=1.277716e-07, step_norm=1.911418e-03, alpha=0.380101\n",
      "Iter 407: f=2.045758e-11, grad_norm=6.665433e-08, step_norm=2.898071e-03, alpha=-0.330416\n",
      "Iter 408: f=2.045595e-11, grad_norm=3.230891e-08, step_norm=3.017394e-03, alpha=-0.069406\n",
      "Iter 409: f=2.045483e-11, grad_norm=7.602207e-08, step_norm=2.154841e-03, alpha=0.112809\n",
      "Iter 410: f=2.045032e-11, grad_norm=8.234871e-08, step_norm=2.401907e-03, alpha=-0.215409\n",
      "Iter 411: f=2.044915e-11, grad_norm=5.054339e-08, step_norm=2.597685e-03, alpha=-0.012805\n",
      "Iter 412: f=2.044032e-11, grad_norm=8.533037e-08, step_norm=5.916921e-03, alpha=0.605346\n",
      "Iter 413: f=2.044016e-11, grad_norm=3.216600e-08, step_norm=8.449081e-04, alpha=0.154651\n",
      "Iter 414: f=2.043841e-11, grad_norm=1.888772e-08, step_norm=2.712882e-03, alpha=-0.076160\n",
      "Iter 415: f=2.043376e-11, grad_norm=3.435936e-08, step_norm=3.261857e-03, alpha=-0.517461\n",
      "Iter 416: f=2.042969e-11, grad_norm=3.938204e-08, step_norm=2.023274e-03, alpha=0.531396\n",
      "Iter 417: f=2.042298e-11, grad_norm=2.592960e-08, step_norm=2.654126e-03, alpha=-0.332287\n",
      "Iter 418: f=2.042073e-11, grad_norm=5.365648e-08, step_norm=1.290392e-03, alpha=0.560476\n",
      "Iter 419: f=2.041897e-11, grad_norm=1.155531e-08, step_norm=3.638248e-03, alpha=0.053041\n",
      "Iter 420: f=2.041561e-11, grad_norm=5.293314e-08, step_norm=2.869716e-03, alpha=-0.337553\n",
      "Iter 421: f=2.041232e-11, grad_norm=2.201194e-08, step_norm=3.950993e-03, alpha=-0.125959\n",
      "Iter 422: f=2.040828e-11, grad_norm=8.877358e-08, step_norm=2.178353e-03, alpha=-0.133850\n",
      "Iter 423: f=2.040524e-11, grad_norm=5.175555e-08, step_norm=3.408607e-03, alpha=0.041724\n",
      "Iter 424: f=2.040021e-11, grad_norm=1.088851e-07, step_norm=3.130847e-03, alpha=0.151113\n",
      "Iter 425: f=2.039958e-11, grad_norm=4.530120e-08, step_norm=1.516786e-03, alpha=0.186347\n",
      "Iter 426: f=2.039443e-11, grad_norm=4.985441e-08, step_norm=4.949778e-03, alpha=0.283365\n",
      "Iter 427: f=2.039407e-11, grad_norm=1.172342e-07, step_norm=2.011681e-03, alpha=0.186885\n",
      "Iter 428: f=2.039164e-11, grad_norm=9.381513e-08, step_norm=3.314450e-03, alpha=-0.310418\n",
      "Iter 429: f=2.039043e-11, grad_norm=6.710358e-08, step_norm=2.485590e-03, alpha=0.171513\n",
      "Iter 430: f=2.038511e-11, grad_norm=7.249374e-08, step_norm=2.304504e-03, alpha=-0.169017\n",
      "Iter 431: f=2.038479e-11, grad_norm=4.297533e-08, step_norm=1.282663e-03, alpha=0.150334\n",
      "Iter 432: f=2.038114e-11, grad_norm=4.477665e-08, step_norm=3.996677e-03, alpha=0.115636\n",
      "Iter 433: f=2.037939e-11, grad_norm=1.069214e-07, step_norm=3.132438e-03, alpha=-0.101948\n",
      "Iter 434: f=2.037860e-11, grad_norm=7.934261e-08, step_norm=1.393048e-03, alpha=-0.089678\n",
      "Iter 435: f=2.037438e-11, grad_norm=1.064133e-07, step_norm=2.063497e-03, alpha=-0.153358\n",
      "Iter 436: f=2.036942e-11, grad_norm=7.588579e-08, step_norm=4.044601e-03, alpha=0.193650\n",
      "Iter 437: f=2.036636e-11, grad_norm=1.362345e-07, step_norm=2.324778e-03, alpha=-0.072825\n",
      "Iter 438: f=2.035262e-11, grad_norm=9.608693e-08, step_norm=9.037318e-03, alpha=1.082679\n",
      "Iter 439: f=2.035081e-11, grad_norm=4.361034e-08, step_norm=4.868198e-03, alpha=-0.543179\n",
      "Iter 440: f=2.034860e-11, grad_norm=1.837371e-08, step_norm=3.527229e-03, alpha=-0.428249\n",
      "Iter 441: f=2.034569e-11, grad_norm=9.927596e-08, step_norm=4.202620e-03, alpha=0.470591\n",
      "Iter 442: f=2.033599e-11, grad_norm=3.368728e-08, step_norm=3.867940e-03, alpha=-0.280347\n",
      "Iter 443: f=2.033291e-11, grad_norm=1.480482e-07, step_norm=3.690196e-03, alpha=0.328215\n",
      "Iter 444: f=2.033253e-11, grad_norm=1.203948e-07, step_norm=2.764989e-03, alpha=0.301379\n",
      "Iter 445: f=2.032903e-11, grad_norm=7.473033e-08, step_norm=4.252091e-03, alpha=-0.356561\n",
      "Iter 446: f=2.032872e-11, grad_norm=4.055412e-08, step_norm=2.013096e-03, alpha=0.116537\n",
      "Iter 447: f=2.032241e-11, grad_norm=4.466617e-08, step_norm=1.611057e-03, alpha=-0.136813\n",
      "Iter 448: f=2.032065e-11, grad_norm=1.410907e-07, step_norm=4.375188e-03, alpha=0.488220\n",
      "Iter 449: f=2.031548e-11, grad_norm=5.665138e-08, step_norm=3.194596e-03, alpha=-0.179157\n",
      "Iter 450: f=2.031181e-11, grad_norm=3.312138e-08, step_norm=3.292970e-03, alpha=0.685522\n",
      "Iter 451: f=2.030000e-11, grad_norm=1.122961e-08, step_norm=4.860167e-03, alpha=-0.222264\n",
      "Iter 452: f=2.029184e-11, grad_norm=5.479645e-08, step_norm=4.493677e-03, alpha=0.759493\n",
      "Iter 453: f=2.028815e-11, grad_norm=7.261145e-08, step_norm=2.096575e-03, alpha=0.916259\n",
      "Iter 454: f=2.028584e-11, grad_norm=4.525617e-09, step_norm=1.861369e-03, alpha=-0.046126\n",
      "Iter 455: f=2.028345e-11, grad_norm=3.706339e-08, step_norm=3.640346e-03, alpha=-0.496625\n",
      "Iter 456: f=2.027984e-11, grad_norm=1.213018e-08, step_norm=5.183067e-03, alpha=-0.199411\n",
      "Iter 457: f=2.027910e-11, grad_norm=8.093976e-08, step_norm=2.184807e-03, alpha=-0.486090\n",
      "Iter 458: f=2.027800e-11, grad_norm=7.736501e-08, step_norm=1.570341e-03, alpha=-0.330619\n",
      "Iter 459: f=2.026991e-11, grad_norm=6.605728e-08, step_norm=3.996949e-03, alpha=-0.624293\n",
      "Iter 460: f=2.026645e-11, grad_norm=1.233860e-07, step_norm=3.063040e-03, alpha=-0.007101\n",
      "Iter 461: f=2.026641e-11, grad_norm=1.872801e-07, step_norm=5.935557e-04, alpha=-0.042412\n",
      "Iter 462: f=2.026356e-11, grad_norm=1.998044e-07, step_norm=1.299629e-03, alpha=-0.063593\n",
      "Iter 463: f=2.025466e-11, grad_norm=2.584759e-07, step_norm=2.296948e-03, alpha=0.432854\n",
      "Iter 464: f=2.025136e-11, grad_norm=8.645273e-08, step_norm=3.694912e-03, alpha=0.219081\n",
      "Iter 465: f=2.024672e-11, grad_norm=1.197868e-07, step_norm=2.399846e-03, alpha=-0.136277\n",
      "Iter 466: f=2.024298e-11, grad_norm=8.430915e-08, step_norm=3.806634e-03, alpha=0.187484\n",
      "Iter 467: f=2.023590e-11, grad_norm=1.313792e-07, step_norm=2.206519e-03, alpha=0.020989\n",
      "Iter 468: f=2.023522e-11, grad_norm=7.642974e-08, step_norm=1.953332e-03, alpha=-0.019652\n",
      "Iter 469: f=2.022242e-11, grad_norm=1.025464e-07, step_norm=6.196857e-03, alpha=0.962799\n",
      "Iter 470: f=2.022232e-11, grad_norm=7.352691e-08, step_norm=6.813793e-04, alpha=0.213884\n",
      "Iter 471: f=2.022011e-11, grad_norm=5.403731e-08, step_norm=1.980650e-03, alpha=-0.484169\n",
      "Iter 472: f=2.021345e-11, grad_norm=3.373003e-08, step_norm=6.267186e-03, alpha=-0.042715\n",
      "Iter 473: f=2.021270e-11, grad_norm=8.048019e-08, step_norm=2.518427e-03, alpha=0.354816\n",
      "Iter 474: f=2.020751e-11, grad_norm=3.895010e-08, step_norm=3.490831e-03, alpha=-0.352019\n",
      "Iter 475: f=2.020608e-11, grad_norm=3.784323e-08, step_norm=1.679246e-03, alpha=-0.460092\n",
      "Iter 476: f=2.020318e-11, grad_norm=3.093550e-08, step_norm=3.136221e-03, alpha=0.045855\n",
      "Iter 477: f=2.020256e-11, grad_norm=7.781790e-08, step_norm=2.259023e-03, alpha=-0.328037\n",
      "Iter 478: f=2.020233e-11, grad_norm=7.788428e-08, step_norm=1.198050e-03, alpha=0.190641\n",
      "Iter 479: f=2.019343e-11, grad_norm=6.678360e-08, step_norm=4.667363e-03, alpha=-0.622858\n",
      "Iter 480: f=2.018468e-11, grad_norm=1.409299e-07, step_norm=4.612742e-03, alpha=0.427707\n",
      "Iter 481: f=2.018351e-11, grad_norm=1.519875e-07, step_norm=2.465733e-03, alpha=0.288193\n",
      "Iter 482: f=2.017468e-11, grad_norm=8.486302e-08, step_norm=2.858407e-03, alpha=-0.097375\n",
      "Iter 483: f=2.017369e-11, grad_norm=6.153811e-08, step_norm=1.485248e-03, alpha=0.211615\n",
      "Iter 484: f=2.016802e-11, grad_norm=6.676075e-08, step_norm=5.230732e-03, alpha=0.341820\n",
      "Iter 485: f=2.016344e-11, grad_norm=1.291531e-07, step_norm=2.504233e-03, alpha=0.257778\n",
      "Iter 486: f=2.015944e-11, grad_norm=1.415788e-07, step_norm=5.700968e-03, alpha=0.825889\n",
      "Iter 487: f=2.014909e-11, grad_norm=1.997417e-08, step_norm=4.022938e-03, alpha=-0.197484\n",
      "Iter 488: f=2.014817e-11, grad_norm=4.462369e-08, step_norm=1.194676e-03, alpha=0.383520\n",
      "Iter 489: f=2.014475e-11, grad_norm=2.035267e-08, step_norm=3.849563e-03, alpha=-0.038092\n",
      "Iter 490: f=2.012999e-11, grad_norm=8.285527e-08, step_norm=1.988980e-03, alpha=0.332267\n",
      "Iter 491: f=2.011927e-11, grad_norm=1.824472e-07, step_norm=3.197830e-03, alpha=1.008795\n",
      "Iter 492: f=2.011810e-11, grad_norm=6.327290e-08, step_norm=4.052541e-03, alpha=0.520375\n",
      "Iter 493: f=2.011341e-11, grad_norm=2.387334e-08, step_norm=3.397306e-03, alpha=-0.243716\n",
      "Iter 494: f=2.010797e-11, grad_norm=4.786506e-08, step_norm=3.453026e-03, alpha=-0.588325\n",
      "Iter 495: f=2.009745e-11, grad_norm=3.037152e-08, step_norm=4.612288e-03, alpha=-0.640916\n",
      "Iter 496: f=2.009504e-11, grad_norm=5.640165e-08, step_norm=1.054114e-03, alpha=0.499794\n",
      "Iter 497: f=2.008851e-11, grad_norm=1.864296e-08, step_norm=1.685840e-03, alpha=-0.232565\n",
      "Iter 498: f=2.008828e-11, grad_norm=8.972931e-08, step_norm=1.341503e-03, alpha=0.233681\n",
      "Iter 499: f=2.008470e-11, grad_norm=6.042153e-08, step_norm=3.314836e-03, alpha=-0.387351\n",
      "Iter 500: f=2.008449e-11, grad_norm=2.761650e-08, step_norm=1.733198e-03, alpha=0.039022\n",
      "Iter 501: f=2.007295e-11, grad_norm=3.721856e-08, step_norm=4.601351e-03, alpha=-0.401277\n",
      "Iter 502: f=2.006802e-11, grad_norm=1.877594e-07, step_norm=3.147942e-03, alpha=0.552863\n",
      "Iter 503: f=2.005541e-11, grad_norm=5.169683e-08, step_norm=3.759106e-03, alpha=-0.194462\n",
      "Iter 504: f=2.005421e-11, grad_norm=5.558721e-08, step_norm=2.155148e-03, alpha=-0.121804\n",
      "Iter 505: f=2.002730e-11, grad_norm=9.350643e-08, step_norm=1.560245e-02, alpha=2.000000\n",
      "Iter 506: f=2.001450e-11, grad_norm=4.361336e-08, step_norm=1.090392e-02, alpha=1.003192\n",
      "Iter 507: f=1.999974e-11, grad_norm=6.308721e-08, step_norm=1.028960e-02, alpha=1.300806\n",
      "Iter 508: f=1.999480e-11, grad_norm=1.360177e-07, step_norm=3.399996e-03, alpha=0.579317\n",
      "Iter 509: f=1.999432e-11, grad_norm=7.971829e-08, step_norm=3.876878e-03, alpha=-0.490686\n",
      "Iter 510: f=1.999409e-11, grad_norm=8.522164e-08, step_norm=1.685704e-03, alpha=0.231825\n",
      "Iter 511: f=1.999006e-11, grad_norm=5.743757e-08, step_norm=4.176420e-03, alpha=-0.421258\n",
      "Iter 512: f=1.998913e-11, grad_norm=1.835246e-08, step_norm=2.615802e-03, alpha=0.094366\n",
      "Iter 513: f=1.997735e-11, grad_norm=4.613395e-08, step_norm=4.486431e-03, alpha=-0.408559\n",
      "Iter 514: f=1.997733e-11, grad_norm=5.533962e-08, step_norm=5.049156e-04, alpha=0.081913\n",
      "Iter 515: f=1.997550e-11, grad_norm=5.238628e-08, step_norm=2.342697e-03, alpha=-0.233654\n",
      "Iter 516: f=1.997059e-11, grad_norm=1.103508e-07, step_norm=4.089846e-03, alpha=0.400620\n",
      "Iter 517: f=1.996446e-11, grad_norm=9.479130e-08, step_norm=2.919869e-03, alpha=-0.116390\n",
      "Iter 518: f=1.996236e-11, grad_norm=5.514228e-08, step_norm=2.370532e-03, alpha=-0.173151\n",
      "Iter 519: f=1.996227e-11, grad_norm=1.154598e-07, step_norm=1.138569e-03, alpha=-0.083602\n",
      "Iter 520: f=1.995998e-11, grad_norm=1.294866e-07, step_norm=1.937205e-03, alpha=-0.108326\n",
      "Iter 521: f=1.995482e-11, grad_norm=1.742228e-07, step_norm=2.405520e-03, alpha=-0.080630\n",
      "Iter 522: f=1.995023e-11, grad_norm=1.355904e-07, step_norm=2.793498e-03, alpha=0.298611\n",
      "Iter 523: f=1.994275e-11, grad_norm=1.286168e-07, step_norm=2.763079e-03, alpha=-0.091187\n",
      "Iter 524: f=1.993867e-11, grad_norm=9.393977e-08, step_norm=3.311891e-03, alpha=0.094268\n",
      "Iter 525: f=1.993444e-11, grad_norm=1.518568e-07, step_norm=2.279931e-03, alpha=-0.028002\n",
      "Iter 526: f=1.992960e-11, grad_norm=1.946120e-07, step_norm=3.126573e-03, alpha=0.357907\n",
      "Iter 527: f=1.991944e-11, grad_norm=7.658183e-08, step_norm=1.792841e-03, alpha=-0.125586\n",
      "Iter 528: f=1.991074e-11, grad_norm=1.985890e-07, step_norm=5.344818e-03, alpha=0.708044\n",
      "Iter 529: f=1.990499e-11, grad_norm=2.808641e-08, step_norm=3.388338e-03, alpha=-0.151425\n",
      "Iter 530: f=1.990486e-11, grad_norm=1.267163e-07, step_norm=1.922505e-03, alpha=0.137895\n",
      "Iter 531: f=1.990197e-11, grad_norm=1.020393e-07, step_norm=4.024036e-03, alpha=-0.206245\n",
      "Iter 532: f=1.989123e-11, grad_norm=7.220219e-08, step_norm=1.812105e-03, alpha=-0.099710\n",
      "Iter 533: f=1.988474e-11, grad_norm=1.936413e-07, step_norm=5.995172e-03, alpha=0.664737\n",
      "Iter 534: f=1.987149e-11, grad_norm=4.338779e-08, step_norm=4.423391e-03, alpha=-0.170738\n",
      "Iter 535: f=1.985754e-11, grad_norm=1.545214e-07, step_norm=2.602846e-03, alpha=0.325128\n",
      "Iter 536: f=1.984649e-11, grad_norm=2.101413e-07, step_norm=7.805009e-03, alpha=1.053238\n",
      "Iter 537: f=1.983014e-11, grad_norm=2.061498e-08, step_norm=6.712550e-03, alpha=-0.425013\n",
      "Iter 538: f=1.981989e-11, grad_norm=1.123214e-07, step_norm=2.580956e-03, alpha=0.317629\n",
      "Iter 539: f=1.981985e-11, grad_norm=4.598901e-08, step_norm=1.284511e-03, alpha=-0.200161\n",
      "Iter 540: f=1.981925e-11, grad_norm=5.566916e-08, step_norm=1.355838e-03, alpha=0.176424\n",
      "Iter 541: f=1.981434e-11, grad_norm=5.569514e-08, step_norm=3.442363e-03, alpha=0.149998\n",
      "Iter 542: f=1.980291e-11, grad_norm=1.192983e-07, step_norm=2.398681e-03, alpha=0.403110\n",
      "Iter 543: f=1.980144e-11, grad_norm=4.450056e-08, step_norm=2.694267e-03, alpha=-0.262006\n",
      "Iter 544: f=1.979083e-11, grad_norm=7.585757e-08, step_norm=4.701937e-03, alpha=0.530645\n",
      "Iter 545: f=1.977017e-11, grad_norm=1.407741e-07, step_norm=2.157654e-03, alpha=0.624027\n",
      "Iter 546: f=1.975290e-11, grad_norm=2.067165e-07, step_norm=3.225306e-03, alpha=1.294550\n",
      "Iter 547: f=1.975288e-11, grad_norm=2.801372e-08, step_norm=1.708037e-03, alpha=0.086119\n",
      "Iter 548: f=1.975093e-11, grad_norm=2.466889e-08, step_norm=6.255087e-03, alpha=-0.330833\n",
      "Iter 549: f=1.974861e-11, grad_norm=1.314462e-08, step_norm=3.614196e-03, alpha=-0.217129\n",
      "Iter 550: f=1.974858e-11, grad_norm=6.716325e-08, step_norm=9.366969e-04, alpha=0.097020\n",
      "Iter 551: f=1.974647e-11, grad_norm=5.871841e-08, step_norm=4.252805e-03, alpha=-0.399217\n",
      "Iter 552: f=1.974245e-11, grad_norm=4.032222e-08, step_norm=4.274775e-03, alpha=-0.539943\n",
      "Iter 553: f=1.973649e-11, grad_norm=1.629783e-07, step_norm=2.800887e-03, alpha=0.507666\n",
      "Iter 554: f=1.973000e-11, grad_norm=4.921357e-08, step_norm=3.637088e-03, alpha=-0.216440\n",
      "Iter 555: f=1.972943e-11, grad_norm=1.610284e-07, step_norm=2.565764e-03, alpha=-0.219242\n",
      "Iter 556: f=1.972594e-11, grad_norm=1.709717e-07, step_norm=2.844542e-03, alpha=-0.186077\n",
      "Iter 557: f=1.971265e-11, grad_norm=1.542193e-07, step_norm=1.931840e-03, alpha=-0.087722\n",
      "Iter 558: f=1.969456e-11, grad_norm=2.933458e-07, step_norm=4.006042e-03, alpha=0.832191\n",
      "Iter 559: f=1.969362e-11, grad_norm=1.622469e-08, step_norm=2.304947e-03, alpha=-0.112526\n",
      "Iter 560: f=1.968052e-11, grad_norm=5.278706e-08, step_norm=6.491138e-03, alpha=-0.350282\n",
      "Iter 561: f=1.967770e-11, grad_norm=7.389736e-08, step_norm=2.270541e-03, alpha=-0.120641\n",
      "Iter 562: f=1.967332e-11, grad_norm=1.396743e-07, step_norm=4.170950e-03, alpha=0.284034\n",
      "Iter 563: f=1.965436e-11, grad_norm=9.845968e-08, step_norm=3.124683e-03, alpha=0.007688\n",
      "Iter 564: f=1.963141e-11, grad_norm=2.214515e-07, step_norm=3.843158e-03, alpha=0.684727\n",
      "Iter 565: f=1.960599e-11, grad_norm=2.137688e-07, step_norm=1.203302e-02, alpha=2.000000\n",
      "Iter 566: f=1.960345e-11, grad_norm=2.775350e-08, step_norm=4.067435e-03, alpha=-0.338801\n",
      "Iter 567: f=1.959775e-11, grad_norm=3.534088e-08, step_norm=4.822629e-03, alpha=-0.079298\n",
      "Iter 568: f=1.959225e-11, grad_norm=9.923243e-08, step_norm=2.830865e-03, alpha=0.390313\n",
      "Iter 569: f=1.958787e-11, grad_norm=4.022692e-08, step_norm=2.771459e-03, alpha=-0.030575\n",
      "Iter 570: f=1.958727e-11, grad_norm=9.232571e-08, step_norm=1.688434e-03, alpha=0.174160\n",
      "Iter 571: f=1.957536e-11, grad_norm=6.338544e-08, step_norm=4.952862e-03, alpha=-0.365910\n",
      "Iter 572: f=1.956632e-11, grad_norm=9.100072e-08, step_norm=3.008366e-03, alpha=-0.244075\n",
      "Iter 573: f=1.956474e-11, grad_norm=2.206978e-07, step_norm=3.108770e-03, alpha=0.176275\n",
      "Iter 574: f=1.955289e-11, grad_norm=1.758685e-07, step_norm=2.091570e-03, alpha=0.144846\n",
      "Iter 575: f=1.952719e-11, grad_norm=2.374153e-07, step_norm=5.331365e-03, alpha=1.658205\n",
      "Iter 576: f=1.951790e-11, grad_norm=4.317963e-08, step_norm=1.230540e-02, alpha=0.135079\n",
      "Iter 577: f=1.950238e-11, grad_norm=1.397774e-07, step_norm=8.320380e-03, alpha=1.526072\n",
      "Iter 578: f=1.949112e-11, grad_norm=4.466904e-08, step_norm=3.813274e-03, alpha=-0.175501\n",
      "Iter 579: f=1.949088e-11, grad_norm=1.813799e-07, step_norm=1.712304e-03, alpha=0.128229\n",
      "Iter 580: f=1.948485e-11, grad_norm=1.491510e-07, step_norm=3.929238e-03, alpha=-0.135749\n",
      "Iter 581: f=1.947936e-11, grad_norm=1.071800e-07, step_norm=2.622275e-03, alpha=-0.096639\n",
      "Iter 582: f=1.947121e-11, grad_norm=1.673115e-07, step_norm=3.206467e-03, alpha=-0.115125\n",
      "Iter 583: f=1.947114e-11, grad_norm=1.333853e-07, step_norm=1.074825e-03, alpha=-0.055233\n",
      "Iter 584: f=1.946779e-11, grad_norm=1.471107e-07, step_norm=2.233104e-03, alpha=-0.120186\n",
      "Iter 585: f=1.944634e-11, grad_norm=2.061885e-07, step_norm=3.100893e-03, alpha=0.139092\n",
      "Iter 586: f=1.943879e-11, grad_norm=2.502648e-07, step_norm=3.994730e-03, alpha=0.407442\n",
      "Iter 587: f=1.941927e-11, grad_norm=1.697646e-07, step_norm=2.849236e-03, alpha=0.506754\n",
      "Iter 588: f=1.938494e-11, grad_norm=2.296111e-07, step_norm=6.804996e-03, alpha=1.976944\n",
      "Iter 589: f=1.937421e-11, grad_norm=9.798085e-08, step_norm=5.101018e-03, alpha=0.727740\n",
      "Iter 590: f=1.936345e-11, grad_norm=3.149814e-08, step_norm=4.131489e-03, alpha=0.130638\n",
      "Iter 591: f=1.936087e-11, grad_norm=1.443185e-07, step_norm=3.150452e-03, alpha=0.375057\n",
      "Iter 592: f=1.935987e-11, grad_norm=7.048901e-08, step_norm=2.202661e-03, alpha=0.160174\n",
      "Iter 593: f=1.935116e-11, grad_norm=8.748516e-08, step_norm=6.755352e-03, alpha=-0.216745\n",
      "Iter 594: f=1.933620e-11, grad_norm=7.861619e-08, step_norm=2.437811e-03, alpha=0.091381\n",
      "Iter 595: f=1.932084e-11, grad_norm=2.018137e-07, step_norm=3.012830e-03, alpha=0.824003\n",
      "Iter 596: f=1.931618e-11, grad_norm=7.997269e-08, step_norm=2.942952e-03, alpha=0.291934\n",
      "Iter 597: f=1.931463e-11, grad_norm=1.229282e-07, step_norm=3.704502e-03, alpha=-0.234868\n",
      "Iter 598: f=1.931305e-11, grad_norm=1.227557e-07, step_norm=3.762477e-03, alpha=-0.301620\n",
      "Iter 599: f=1.931147e-11, grad_norm=1.815348e-07, step_norm=2.187305e-03, alpha=0.188208\n",
      "Iter 600: f=1.930573e-11, grad_norm=1.238798e-07, step_norm=3.869288e-03, alpha=-0.174293\n",
      "Iter 601: f=1.928958e-11, grad_norm=1.830452e-07, step_norm=2.912272e-03, alpha=0.675701\n",
      "Iter 602: f=1.928938e-11, grad_norm=1.237584e-07, step_norm=1.510136e-03, alpha=0.144531\n",
      "Iter 603: f=1.928500e-11, grad_norm=1.147234e-07, step_norm=4.520981e-03, alpha=-0.294459\n",
      "Iter 604: f=1.927954e-11, grad_norm=1.876899e-07, step_norm=2.735053e-03, alpha=0.371021\n",
      "Iter 605: f=1.927807e-11, grad_norm=9.880510e-08, step_norm=2.389530e-03, alpha=-0.146944\n",
      "Iter 606: f=1.926395e-11, grad_norm=1.442344e-07, step_norm=6.333046e-03, alpha=-0.109442\n",
      "Iter 607: f=1.924941e-11, grad_norm=1.006838e-07, step_norm=2.037668e-03, alpha=0.012837\n",
      "Iter 608: f=1.922006e-11, grad_norm=2.245166e-07, step_norm=3.040403e-03, alpha=1.143455\n",
      "Iter 609: f=1.919363e-11, grad_norm=1.811678e-07, step_norm=4.045061e-03, alpha=1.565088\n",
      "Iter 610: f=1.919014e-11, grad_norm=9.749662e-08, step_norm=2.655318e-03, alpha=0.279117\n",
      "Iter 611: f=1.918101e-11, grad_norm=1.225484e-07, step_norm=6.830942e-03, alpha=-0.283966\n",
      "Iter 612: f=1.917761e-11, grad_norm=1.022807e-07, step_norm=2.874482e-03, alpha=0.230411\n",
      "Iter 613: f=1.916448e-11, grad_norm=1.336048e-07, step_norm=4.641244e-03, alpha=0.153177\n",
      "Iter 614: f=1.914655e-11, grad_norm=1.568844e-07, step_norm=9.838594e-03, alpha=0.896390\n",
      "Iter 615: f=1.912727e-11, grad_norm=9.490487e-08, step_norm=2.563648e-03, alpha=-0.123516\n",
      "Iter 616: f=1.912344e-11, grad_norm=2.620728e-07, step_norm=3.153265e-03, alpha=0.270218\n",
      "Iter 617: f=1.909825e-11, grad_norm=1.645841e-07, step_norm=3.048088e-03, alpha=0.299177\n",
      "Iter 618: f=1.908478e-11, grad_norm=1.688189e-07, step_norm=5.511961e-03, alpha=0.731383\n",
      "Iter 619: f=1.907021e-11, grad_norm=1.380232e-07, step_norm=7.499683e-03, alpha=0.535229\n",
      "Iter 620: f=1.906433e-11, grad_norm=1.241436e-07, step_norm=4.844627e-03, alpha=-0.218211\n",
      "Iter 621: f=1.904424e-11, grad_norm=1.256024e-07, step_norm=2.748944e-03, alpha=-0.008519\n",
      "Iter 622: f=1.901339e-11, grad_norm=2.753277e-07, step_norm=3.453326e-03, alpha=1.018630\n",
      "Iter 623: f=1.898018e-11, grad_norm=1.847035e-07, step_norm=1.357821e-02, alpha=1.483000\n",
      "Iter 624: f=1.896830e-11, grad_norm=2.543922e-07, step_norm=4.090060e-03, alpha=0.459390\n",
      "Iter 625: f=1.894430e-11, grad_norm=1.093957e-07, step_norm=2.826902e-03, alpha=0.075172\n",
      "Iter 626: f=1.894333e-11, grad_norm=2.579195e-07, step_norm=2.364617e-03, alpha=-0.033023\n",
      "Iter 627: f=1.892398e-11, grad_norm=2.455125e-07, step_norm=3.655972e-03, alpha=0.498084\n",
      "Iter 628: f=1.891275e-11, grad_norm=2.140574e-07, step_norm=6.699920e-03, alpha=0.651807\n",
      "Iter 629: f=1.888708e-11, grad_norm=9.599538e-08, step_norm=2.852944e-03, alpha=0.104655\n",
      "Iter 630: f=1.888213e-11, grad_norm=2.457712e-07, step_norm=3.226151e-03, alpha=0.032217\n",
      "Iter 631: f=1.887123e-11, grad_norm=1.971689e-07, step_norm=3.285359e-03, alpha=0.070896\n",
      "Iter 632: f=1.887062e-11, grad_norm=2.482451e-07, step_norm=1.812365e-03, alpha=0.090194\n",
      "Iter 633: f=1.885662e-11, grad_norm=2.151037e-07, step_norm=2.902197e-03, alpha=0.172490\n",
      "Iter 634: f=1.885483e-11, grad_norm=1.418375e-07, step_norm=2.278958e-03, alpha=0.180688\n",
      "Iter 635: f=1.884133e-11, grad_norm=1.514256e-07, step_norm=2.939844e-03, alpha=0.133036\n",
      "Iter 636: f=1.883070e-11, grad_norm=2.170627e-07, step_norm=5.194916e-03, alpha=0.496265\n",
      "Iter 637: f=1.880616e-11, grad_norm=1.012523e-07, step_norm=2.537573e-03, alpha=0.141200\n",
      "Iter 638: f=1.880524e-11, grad_norm=2.522389e-07, step_norm=2.345560e-03, alpha=0.105734\n",
      "Iter 639: f=1.878910e-11, grad_norm=2.146452e-07, step_norm=2.921477e-03, alpha=0.218097\n",
      "Iter 640: f=1.878282e-11, grad_norm=1.230517e-07, step_norm=3.276464e-03, alpha=0.305246\n",
      "Iter 641: f=1.876509e-11, grad_norm=1.627184e-07, step_norm=3.437251e-03, alpha=0.367275\n",
      "Iter 642: f=1.874794e-11, grad_norm=2.172064e-07, step_norm=8.550993e-03, alpha=0.815446\n",
      "Iter 643: f=1.872073e-11, grad_norm=1.098408e-07, step_norm=2.763735e-03, alpha=0.109634\n",
      "Iter 644: f=1.870763e-11, grad_norm=2.682080e-07, step_norm=2.859290e-03, alpha=0.193731\n",
      "Iter 645: f=1.870441e-11, grad_norm=1.709608e-07, step_norm=2.559052e-03, alpha=-0.130115\n",
      "Iter 646: f=1.868238e-11, grad_norm=2.380796e-07, step_norm=3.258794e-03, alpha=0.457901\n",
      "Iter 647: f=1.866891e-11, grad_norm=2.449702e-07, step_norm=6.070094e-03, alpha=0.619762\n",
      "Iter 648: f=1.864047e-11, grad_norm=1.113485e-07, step_norm=2.767907e-03, alpha=0.201284\n",
      "Iter 649: f=1.862248e-11, grad_norm=2.580261e-07, step_norm=2.642249e-03, alpha=0.375929\n",
      "Iter 650: f=1.861862e-11, grad_norm=1.242427e-07, step_norm=2.629601e-03, alpha=0.243950\n",
      "Iter 651: f=1.860134e-11, grad_norm=1.505203e-07, step_norm=3.649962e-03, alpha=0.227259\n",
      "Iter 652: f=1.858887e-11, grad_norm=2.176209e-07, step_norm=6.267090e-03, alpha=0.519370\n",
      "Iter 653: f=1.856052e-11, grad_norm=1.225791e-07, step_norm=2.757692e-03, alpha=0.253625\n",
      "Iter 654: f=1.853946e-11, grad_norm=2.621012e-07, step_norm=2.557975e-03, alpha=0.492523\n",
      "Iter 655: f=1.853486e-11, grad_norm=1.125620e-07, step_norm=2.684063e-03, alpha=0.243822\n",
      "Iter 656: f=1.851573e-11, grad_norm=1.490492e-07, step_norm=3.917238e-03, alpha=0.245471\n",
      "Iter 657: f=1.850196e-11, grad_norm=2.212875e-07, step_norm=6.443974e-03, alpha=0.527786\n",
      "Iter 658: f=1.847280e-11, grad_norm=1.289483e-07, step_norm=2.812375e-03, alpha=0.271950\n",
      "Iter 659: f=1.844733e-11, grad_norm=2.679079e-07, step_norm=2.613042e-03, alpha=0.612163\n",
      "Iter 660: f=1.844037e-11, grad_norm=1.017978e-07, step_norm=2.942786e-03, alpha=0.270390\n",
      "Iter 661: f=1.841853e-11, grad_norm=1.568976e-07, step_norm=4.262227e-03, alpha=0.314562\n",
      "Iter 662: f=1.840803e-11, grad_norm=2.291268e-07, step_norm=5.819824e-03, alpha=0.398867\n",
      "Iter 663: f=1.837646e-11, grad_norm=1.501388e-07, step_norm=2.972052e-03, alpha=0.404657\n",
      "Iter 664: f=1.833963e-11, grad_norm=2.504216e-07, step_norm=5.050431e-03, alpha=0.947782\n",
      "Iter 665: f=1.832446e-11, grad_norm=2.284866e-07, step_norm=8.712328e-03, alpha=0.572310\n",
      "Iter 666: f=1.829015e-11, grad_norm=1.670513e-07, step_norm=3.088359e-03, alpha=0.348890\n",
      "Iter 667: f=1.826900e-11, grad_norm=2.314579e-07, step_norm=3.495905e-03, alpha=0.299412\n",
      "Iter 668: f=1.825311e-11, grad_norm=2.773018e-07, step_norm=3.353516e-03, alpha=0.154489\n",
      "Iter 669: f=1.822928e-11, grad_norm=2.073453e-07, step_norm=3.466070e-03, alpha=0.321902\n",
      "Iter 670: f=1.822097e-11, grad_norm=2.671853e-07, step_norm=3.234624e-03, alpha=0.089754\n",
      "Iter 671: f=1.818485e-11, grad_norm=2.285082e-07, step_norm=4.371459e-03, alpha=0.727430\n",
      "Iter 672: f=1.816740e-11, grad_norm=2.520521e-07, step_norm=7.151380e-03, alpha=0.549565\n",
      "Iter 673: f=1.813277e-11, grad_norm=1.622753e-07, step_norm=3.216382e-03, alpha=0.354342\n",
      "Iter 674: f=1.807402e-11, grad_norm=2.813562e-07, step_norm=7.516036e-03, alpha=1.547431\n",
      "Iter 675: f=1.801453e-11, grad_norm=2.525901e-07, step_norm=1.269127e-02, alpha=1.663882\n",
      "Iter 676: f=1.797709e-11, grad_norm=3.147450e-07, step_norm=3.034408e-03, alpha=0.612265\n",
      "Iter 677: f=1.795210e-11, grad_norm=2.182694e-07, step_norm=3.684866e-03, alpha=0.205330\n",
      "Iter 678: f=1.788655e-11, grad_norm=3.012513e-07, step_norm=5.471898e-03, alpha=1.728262\n",
      "Iter 679: f=1.784544e-11, grad_norm=2.314518e-07, step_norm=1.065095e-02, alpha=0.708793\n",
      "Iter 680: f=1.780475e-11, grad_norm=2.676488e-07, step_norm=3.241592e-03, alpha=0.557227\n",
      "Iter 681: f=1.777154e-11, grad_norm=2.421147e-07, step_norm=4.081918e-03, alpha=0.434013\n",
      "Iter 682: f=1.774132e-11, grad_norm=2.987795e-07, step_norm=2.648428e-03, alpha=0.499115\n",
      "Iter 683: f=1.771518e-11, grad_norm=1.676880e-07, step_norm=4.002622e-03, alpha=0.180043\n",
      "Iter 684: f=1.765024e-11, grad_norm=2.800607e-07, step_norm=8.440263e-03, alpha=1.519631\n",
      "Iter 685: f=1.762848e-11, grad_norm=3.330231e-07, step_norm=2.748243e-03, alpha=0.253902\n",
      "Iter 686: f=1.758181e-11, grad_norm=2.444946e-07, step_norm=5.051582e-03, alpha=0.692922\n",
      "Iter 687: f=1.757082e-11, grad_norm=3.078671e-07, step_norm=3.172600e-03, alpha=0.085188\n",
      "Iter 688: f=1.751033e-11, grad_norm=2.776270e-07, step_norm=6.992932e-03, alpha=1.114331\n",
      "Iter 689: f=1.749509e-11, grad_norm=3.210841e-07, step_norm=3.143534e-03, alpha=0.053802\n",
      "Iter 690: f=1.743323e-11, grad_norm=3.011412e-07, step_norm=6.698912e-03, alpha=1.129018\n",
      "Iter 691: f=1.742677e-11, grad_norm=3.333439e-07, step_norm=2.954264e-03, alpha=0.026580\n",
      "Iter 692: f=1.735520e-11, grad_norm=3.190622e-07, step_norm=1.071860e-02, alpha=1.367979\n",
      "Iter 693: f=1.730757e-11, grad_norm=3.184116e-07, step_norm=4.149923e-03, alpha=0.465840\n",
      "Iter 694: f=1.725411e-11, grad_norm=2.960202e-07, step_norm=6.566978e-03, alpha=0.750982\n",
      "Iter 695: f=1.722073e-11, grad_norm=3.384624e-07, step_norm=3.289418e-03, alpha=0.409030\n",
      "Iter 696: f=1.716947e-11, grad_norm=2.191105e-07, step_norm=7.276202e-03, alpha=0.574635\n",
      "Iter 697: f=1.710216e-11, grad_norm=3.593725e-07, step_norm=8.475562e-03, alpha=1.157651\n",
      "Iter 698: f=1.704840e-11, grad_norm=2.255493e-07, step_norm=6.955349e-03, alpha=0.470883\n",
      "Iter 699: f=1.695919e-11, grad_norm=3.674854e-07, step_norm=1.255765e-02, alpha=1.582188\n",
      "Iter 700: f=1.689282e-11, grad_norm=2.990956e-07, step_norm=6.956220e-03, alpha=0.759089\n",
      "Iter 701: f=1.686025e-11, grad_norm=3.624953e-07, step_norm=3.013762e-03, alpha=0.292013\n",
      "Iter 702: f=1.679609e-11, grad_norm=2.633039e-07, step_norm=7.857816e-03, alpha=0.776319\n",
      "Iter 703: f=1.675202e-11, grad_norm=3.895210e-07, step_norm=3.991684e-03, alpha=0.488424\n",
      "Iter 704: f=1.669037e-11, grad_norm=2.404908e-07, step_norm=7.368275e-03, alpha=0.596587\n",
      "Iter 705: f=1.662457e-11, grad_norm=3.911079e-07, step_norm=6.998304e-03, alpha=0.880673\n",
      "Iter 706: f=1.656902e-11, grad_norm=2.133275e-07, step_norm=6.983522e-03, alpha=0.392615\n",
      "Iter 707: f=1.646011e-11, grad_norm=3.856816e-07, step_norm=1.494537e-02, alpha=1.634391\n",
      "Iter 708: f=1.635634e-11, grad_norm=4.056813e-07, step_norm=9.741467e-03, alpha=1.419766\n",
      "Iter 709: f=1.634640e-11, grad_norm=3.638562e-07, step_norm=6.568896e-03, alpha=0.241566\n",
      "Iter 710: f=1.629465e-11, grad_norm=2.918371e-07, step_norm=4.535449e-03, alpha=0.325149\n",
      "Iter 711: f=1.624088e-11, grad_norm=2.201422e-07, step_norm=8.379381e-03, alpha=0.493620\n",
      "Iter 712: f=1.617657e-11, grad_norm=3.848634e-07, step_norm=6.695339e-03, alpha=0.650492\n",
      "Iter 713: f=1.614312e-11, grad_norm=1.908410e-07, step_norm=5.329795e-03, alpha=0.162445\n",
      "Iter 714: f=1.603554e-11, grad_norm=3.418979e-07, step_norm=1.645682e-02, alpha=1.404782\n",
      "Iter 715: f=1.591304e-11, grad_norm=4.651317e-07, step_norm=1.035262e-02, alpha=1.546650\n",
      "Iter 716: f=1.584030e-11, grad_norm=1.970348e-07, step_norm=1.769219e-02, alpha=0.551299\n",
      "Iter 717: f=1.575291e-11, grad_norm=4.519286e-07, step_norm=6.630859e-03, alpha=0.918024\n",
      "Iter 718: f=1.572089e-11, grad_norm=1.524579e-07, step_norm=5.320423e-03, alpha=0.317816\n",
      "Iter 719: f=1.562230e-11, grad_norm=2.975787e-07, step_norm=1.370782e-02, alpha=1.097698\n",
      "Iter 720: f=1.555984e-11, grad_norm=4.831161e-07, step_norm=4.921898e-03, alpha=0.463467\n",
      "Iter 721: f=1.550388e-11, grad_norm=2.506887e-07, step_norm=6.833499e-03, alpha=0.405932\n",
      "Iter 722: f=1.544144e-11, grad_norm=3.976483e-07, step_norm=6.088334e-03, alpha=0.511971\n",
      "Iter 723: f=1.539174e-11, grad_norm=2.003821e-07, step_norm=7.560429e-03, alpha=0.319877\n",
      "Iter 724: f=1.531456e-11, grad_norm=3.791631e-07, step_norm=8.838494e-03, alpha=0.711748\n",
      "Iter 725: f=1.529573e-11, grad_norm=1.812210e-07, step_norm=4.044127e-03, alpha=0.188607\n",
      "Iter 726: f=1.519513e-11, grad_norm=2.849076e-07, step_norm=1.443625e-02, alpha=1.006567\n",
      "Iter 727: f=1.514437e-11, grad_norm=4.842023e-07, step_norm=4.629354e-03, alpha=0.291778\n",
      "Iter 728: f=1.505223e-11, grad_norm=2.964064e-07, step_norm=1.047472e-02, alpha=0.893103\n",
      "Iter 729: f=1.504628e-11, grad_norm=4.289876e-07, step_norm=3.768795e-03, alpha=-0.144219\n",
      "Iter 730: f=1.498935e-11, grad_norm=4.560496e-07, step_norm=5.495499e-03, alpha=0.405828\n",
      "Iter 731: f=1.491882e-11, grad_norm=2.282584e-07, step_norm=1.037840e-02, alpha=0.566786\n",
      "Iter 732: f=1.489279e-11, grad_norm=4.188307e-07, step_norm=3.642317e-03, alpha=0.051299\n",
      "Iter 733: f=1.476835e-11, grad_norm=3.287598e-07, step_norm=1.779737e-02, alpha=1.398308\n",
      "Iter 734: f=1.469483e-11, grad_norm=4.556277e-07, step_norm=6.999411e-03, alpha=0.609541\n",
      "Iter 735: f=1.459952e-11, grad_norm=1.655524e-07, step_norm=1.581137e-02, alpha=0.893227\n",
      "Iter 736: f=1.453478e-11, grad_norm=4.302768e-07, step_norm=6.549350e-03, alpha=0.509605\n",
      "Iter 737: f=1.444168e-11, grad_norm=1.743112e-07, step_norm=1.729193e-02, alpha=0.979405\n",
      "Iter 738: f=1.439213e-11, grad_norm=4.203081e-07, step_norm=4.986297e-03, alpha=0.290760\n",
      "Iter 739: f=1.432977e-11, grad_norm=2.301017e-07, step_norm=1.252448e-02, alpha=0.764675\n",
      "Iter 740: f=1.430719e-11, grad_norm=3.522202e-07, step_norm=4.154392e-03, alpha=-0.067452\n",
      "Iter 741: f=1.419490e-11, grad_norm=2.969908e-07, step_norm=2.017498e-02, alpha=1.326057\n",
      "Iter 742: f=1.415291e-11, grad_norm=3.115827e-07, step_norm=5.242712e-03, alpha=0.347967\n",
      "Iter 743: f=1.411551e-11, grad_norm=4.118809e-07, step_norm=4.582290e-03, alpha=0.210599\n",
      "Iter 744: f=1.401062e-11, grad_norm=2.478498e-07, step_norm=1.667916e-02, alpha=1.137187\n",
      "Iter 745: f=1.400296e-11, grad_norm=4.623757e-07, step_norm=3.162580e-03, alpha=-0.088452\n",
      "Iter 746: f=1.391581e-11, grad_norm=4.541943e-07, step_norm=8.630094e-03, alpha=0.856387\n",
      "Iter 747: f=1.384187e-11, grad_norm=1.018135e-07, step_norm=1.728759e-02, alpha=0.636153\n",
      "Iter 748: f=1.373831e-11, grad_norm=3.493708e-07, step_norm=1.198593e-02, alpha=1.170876\n",
      "Iter 749: f=1.368599e-11, grad_norm=1.988788e-07, step_norm=8.900601e-03, alpha=0.422220\n",
      "Iter 750: f=1.365662e-11, grad_norm=3.768288e-07, step_norm=4.091439e-03, alpha=0.139913\n",
      "Iter 751: f=1.355648e-11, grad_norm=2.390837e-07, step_norm=2.004074e-02, alpha=1.054129\n",
      "Iter 752: f=1.352801e-11, grad_norm=4.406452e-07, step_norm=3.927891e-03, alpha=0.125978\n",
      "Iter 753: f=1.348051e-11, grad_norm=3.083133e-07, step_norm=5.241655e-03, alpha=0.411016\n",
      "Iter 754: f=1.348050e-11, grad_norm=4.145546e-07, step_norm=6.016663e-04, alpha=0.022095\n",
      "Iter 755: f=1.347723e-11, grad_norm=4.036532e-07, step_norm=2.566106e-03, alpha=-0.069844\n",
      "Iter 756: f=1.340951e-11, grad_norm=3.968342e-07, step_norm=8.118484e-03, alpha=0.650430\n",
      "Iter 757: f=1.334561e-11, grad_norm=1.373921e-07, step_norm=1.510831e-02, alpha=0.685966\n",
      "Iter 758: f=1.331661e-11, grad_norm=3.386400e-07, step_norm=3.988214e-03, alpha=0.153377\n",
      "Iter 759: f=1.329700e-11, grad_norm=2.108186e-07, step_norm=3.919415e-03, alpha=0.076193\n",
      "Iter 760: f=1.321504e-11, grad_norm=3.366940e-07, step_norm=1.338879e-02, alpha=0.961980\n",
      "Iter 761: f=1.316329e-11, grad_norm=1.464298e-07, step_norm=1.411596e-02, alpha=0.706529\n",
      "Iter 762: f=1.313882e-11, grad_norm=2.853766e-07, step_norm=3.715401e-03, alpha=0.092311\n",
      "Iter 763: f=1.311565e-11, grad_norm=1.834502e-07, step_norm=4.100009e-03, alpha=0.084620\n",
      "Iter 764: f=1.306221e-11, grad_norm=3.254241e-07, step_norm=8.897712e-03, alpha=0.539932\n",
      "Iter 765: f=1.304474e-11, grad_norm=1.320695e-07, step_norm=8.327024e-03, alpha=0.318985\n",
      "Iter 766: f=1.296968e-11, grad_norm=2.133419e-07, step_norm=1.690622e-02, alpha=0.873835\n",
      "Iter 767: f=1.291767e-11, grad_norm=4.122517e-07, step_norm=6.219189e-03, alpha=0.498768\n",
      "Iter 768: f=1.291208e-11, grad_norm=1.434339e-07, step_norm=4.757501e-03, alpha=0.176929\n",
      "Iter 769: f=1.286754e-11, grad_norm=1.797276e-07, step_norm=7.758246e-03, alpha=0.407540\n",
      "Iter 770: f=1.286141e-11, grad_norm=3.594513e-07, step_norm=4.525296e-03, alpha=0.209269\n",
      "Iter 771: f=1.280929e-11, grad_norm=2.477589e-07, step_norm=1.039092e-02, alpha=0.561174\n",
      "Iter 772: f=1.280798e-11, grad_norm=1.271301e-07, step_norm=3.171145e-03, alpha=0.114187\n",
      "Iter 773: f=1.278524e-11, grad_norm=1.382163e-07, step_norm=4.084539e-03, alpha=0.052208\n",
      "Iter 774: f=1.270956e-11, grad_norm=2.880376e-07, step_norm=1.499939e-02, alpha=0.970881\n",
      "Iter 775: f=1.269983e-11, grad_norm=1.552630e-07, step_norm=5.246067e-03, alpha=0.277598\n",
      "Iter 776: f=1.265638e-11, grad_norm=1.869853e-07, step_norm=6.870655e-03, alpha=0.424117\n",
      "Iter 777: f=1.264381e-11, grad_norm=3.564908e-07, step_norm=6.360236e-03, alpha=0.361195\n",
      "Iter 778: f=1.257687e-11, grad_norm=1.908198e-07, step_norm=1.420044e-02, alpha=0.806826\n",
      "Iter 779: f=1.246571e-11, grad_norm=3.143567e-07, step_norm=1.633652e-02, alpha=1.725289\n",
      "Iter 780: f=1.241263e-11, grad_norm=4.080574e-07, step_norm=1.640018e-02, alpha=1.168781\n",
      "Iter 781: f=1.239502e-11, grad_norm=1.062781e-07, step_norm=4.771743e-03, alpha=-0.112644\n",
      "Iter 782: f=1.236514e-11, grad_norm=2.575866e-07, step_norm=4.373489e-03, alpha=0.271401\n",
      "Iter 783: f=1.235029e-11, grad_norm=1.332398e-07, step_norm=4.084597e-03, alpha=-0.050599\n",
      "Iter 784: f=1.226636e-11, grad_norm=2.629388e-07, step_norm=1.548600e-02, alpha=1.326586\n",
      "Iter 785: f=1.225931e-11, grad_norm=3.965722e-07, step_norm=3.757129e-03, alpha=0.211528\n",
      "Iter 786: f=1.218629e-11, grad_norm=2.727882e-07, step_norm=1.509738e-02, alpha=1.183269\n",
      "Iter 787: f=1.216035e-11, grad_norm=1.728496e-07, step_norm=4.361156e-03, alpha=0.166747\n",
      "Iter 788: f=1.207248e-11, grad_norm=3.109403e-07, step_norm=2.007600e-02, alpha=1.513843\n",
      "Iter 789: f=1.206545e-11, grad_norm=2.155297e-07, step_norm=3.631440e-03, alpha=-0.085189\n",
      "Iter 790: f=1.200008e-11, grad_norm=3.175139e-07, step_norm=8.894577e-03, alpha=1.068002\n",
      "Iter 791: f=1.195292e-11, grad_norm=3.293617e-07, step_norm=2.231539e-02, alpha=0.958978\n",
      "Iter 792: f=1.192332e-11, grad_norm=2.598081e-07, step_norm=4.658361e-03, alpha=0.377642\n",
      "Iter 793: f=1.191921e-11, grad_norm=1.388066e-07, step_norm=3.763205e-03, alpha=-0.154507\n",
      "Iter 794: f=1.188349e-11, grad_norm=2.213648e-07, step_norm=6.059821e-03, alpha=0.471951\n",
      "Iter 795: f=1.186647e-11, grad_norm=3.326736e-07, step_norm=6.551557e-03, alpha=0.400295\n",
      "Iter 796: f=1.182960e-11, grad_norm=1.575029e-07, step_norm=1.102248e-02, alpha=0.586222\n",
      "Iter 797: f=1.180813e-11, grad_norm=3.274462e-07, step_norm=9.453862e-03, alpha=0.605500\n",
      "Iter 798: f=1.178141e-11, grad_norm=1.168283e-07, step_norm=6.726105e-03, alpha=0.298227\n",
      "Iter 799: f=1.178037e-11, grad_norm=2.777386e-07, step_norm=2.442092e-03, alpha=0.100798\n",
      "Iter 800: f=1.175881e-11, grad_norm=2.327309e-07, step_norm=5.751877e-03, alpha=0.299173\n",
      "Iter 801: f=1.173328e-11, grad_norm=1.169634e-07, step_norm=6.085924e-03, alpha=0.267493\n",
      "Iter 802: f=1.170174e-11, grad_norm=2.680160e-07, step_norm=9.476726e-03, alpha=0.563759\n",
      "Iter 803: f=1.168413e-11, grad_norm=1.117185e-07, step_norm=4.241711e-03, alpha=0.118338\n",
      "Iter 804: f=1.163007e-11, grad_norm=2.382043e-07, step_norm=2.018036e-02, alpha=1.271246\n",
      "Iter 805: f=1.161937e-11, grad_norm=2.917093e-07, step_norm=4.709566e-03, alpha=0.315662\n",
      "Iter 806: f=1.159586e-11, grad_norm=1.778929e-07, step_norm=5.491053e-03, alpha=0.395209\n",
      "Iter 807: f=1.157823e-11, grad_norm=2.818125e-07, step_norm=9.001235e-03, alpha=0.634849\n",
      "Iter 808: f=1.155696e-11, grad_norm=9.919070e-08, step_norm=6.160069e-03, alpha=0.271289\n",
      "Iter 809: f=1.155496e-11, grad_norm=2.447703e-07, step_norm=2.247071e-03, alpha=-0.040104\n",
      "Iter 810: f=1.151272e-11, grad_norm=2.274334e-07, step_norm=1.836510e-02, alpha=1.228737\n",
      "Iter 811: f=1.149373e-11, grad_norm=1.816279e-07, step_norm=4.473431e-03, alpha=0.286558\n",
      "Iter 812: f=1.147030e-11, grad_norm=2.748939e-07, step_norm=7.452726e-03, alpha=0.680402\n",
      "Iter 813: f=1.145999e-11, grad_norm=8.872304e-08, step_norm=3.672513e-03, alpha=0.030979\n",
      "Iter 814: f=1.142371e-11, grad_norm=1.894982e-07, step_norm=1.853956e-02, alpha=1.171063\n",
      "Iter 815: f=1.138602e-11, grad_norm=2.648994e-07, step_norm=1.578830e-02, alpha=1.819745\n",
      "Iter 816: f=1.136510e-11, grad_norm=2.024125e-07, step_norm=2.212994e-02, alpha=0.903741\n",
      "Iter 817: f=1.136046e-11, grad_norm=1.896623e-07, step_norm=2.595738e-03, alpha=0.113421\n",
      "Iter 818: f=1.134589e-11, grad_norm=1.388716e-07, step_norm=4.279430e-03, alpha=0.339527\n",
      "Iter 819: f=1.131603e-11, grad_norm=2.252094e-07, step_norm=1.523847e-02, alpha=1.977902\n",
      "Iter 820: f=1.129704e-11, grad_norm=1.116787e-07, step_norm=1.031072e-02, alpha=0.757175\n",
      "Iter 821: f=1.126721e-11, grad_norm=2.097560e-07, step_norm=1.429212e-02, alpha=2.000000\n",
      "Iter 822: f=1.125423e-11, grad_norm=1.289693e-07, step_norm=6.196794e-03, alpha=0.796562\n",
      "Iter 823: f=1.122879e-11, grad_norm=1.829538e-07, step_norm=1.365076e-02, alpha=2.000000\n",
      "Iter 824: f=1.120929e-11, grad_norm=1.762578e-07, step_norm=6.897368e-03, alpha=1.612537\n",
      "Iter 825: f=1.119162e-11, grad_norm=1.589356e-07, step_norm=1.527454e-02, alpha=1.561722\n",
      "Iter 826: f=1.119034e-11, grad_norm=1.651720e-07, step_norm=2.153430e-03, alpha=-0.026489\n",
      "Iter 827: f=1.117722e-11, grad_norm=1.534077e-07, step_norm=4.728198e-03, alpha=0.626628\n",
      "Iter 828: f=1.116756e-11, grad_norm=1.840682e-07, step_norm=3.906581e-03, alpha=0.864400\n",
      "Iter 829: f=1.115882e-11, grad_norm=4.039382e-08, step_norm=4.647939e-03, alpha=0.183274\n",
      "Iter 830: f=1.115014e-11, grad_norm=1.399755e-07, step_norm=4.322937e-03, alpha=0.436090\n",
      "Iter 831: f=1.113975e-11, grad_norm=5.589660e-08, step_norm=4.866877e-03, alpha=0.367279\n",
      "Iter 832: f=1.112775e-11, grad_norm=1.520211e-07, step_norm=5.504492e-03, alpha=0.747781\n",
      "Iter 833: f=1.111779e-11, grad_norm=3.303183e-08, step_norm=4.521095e-03, alpha=0.179999\n",
      "Iter 834: f=1.109287e-11, grad_norm=1.393236e-07, step_norm=1.118665e-02, alpha=2.000000\n",
      "Iter 835: f=1.108782e-11, grad_norm=1.402483e-07, step_norm=3.312315e-03, alpha=0.266678\n",
      "Iter 836: f=1.107556e-11, grad_norm=7.978979e-08, step_norm=4.041447e-03, alpha=0.644180\n",
      "Iter 837: f=1.107160e-11, grad_norm=1.568053e-07, step_norm=3.299413e-03, alpha=0.388328\n",
      "Iter 838: f=1.105910e-11, grad_norm=8.528732e-08, step_norm=6.527431e-03, alpha=0.795774\n",
      "Iter 839: f=1.104284e-11, grad_norm=1.476510e-07, step_norm=6.574145e-03, alpha=1.215206\n",
      "Iter 840: f=1.103271e-11, grad_norm=2.010312e-08, step_norm=4.724542e-03, alpha=0.068818\n",
      "Iter 841: f=1.101355e-11, grad_norm=1.295487e-07, step_norm=5.813865e-03, alpha=1.425927\n",
      "Iter 842: f=1.100415e-11, grad_norm=1.421537e-07, step_norm=7.545387e-03, alpha=1.063616\n",
      "Iter 843: f=1.099299e-11, grad_norm=8.005612e-08, step_norm=5.211332e-03, alpha=0.384180\n",
      "Iter 844: f=1.097444e-11, grad_norm=1.477550e-07, step_norm=7.258840e-03, alpha=1.347249\n",
      "Iter 845: f=1.096332e-11, grad_norm=3.186617e-08, step_norm=4.562159e-03, alpha=0.304003\n",
      "Iter 846: f=1.093708e-11, grad_norm=1.412665e-07, step_norm=7.950001e-03, alpha=2.000000\n",
      "Iter 847: f=1.092704e-11, grad_norm=1.073083e-07, step_norm=5.480959e-03, alpha=0.719721\n",
      "Iter 848: f=1.091462e-11, grad_norm=6.920652e-08, step_norm=3.868933e-03, alpha=0.579595\n",
      "Iter 849: f=1.089398e-11, grad_norm=1.555543e-07, step_norm=7.684115e-03, alpha=1.856266\n",
      "Iter 850: f=1.088023e-11, grad_norm=7.144362e-08, step_norm=4.308128e-03, alpha=0.656824\n",
      "Iter 851: f=1.085626e-11, grad_norm=1.610188e-07, step_norm=7.071268e-03, alpha=2.000000\n",
      "Iter 852: f=1.084295e-11, grad_norm=1.053603e-07, step_norm=6.076771e-03, alpha=1.072019\n",
      "Iter 853: f=1.083791e-11, grad_norm=1.670069e-07, step_norm=3.509308e-03, alpha=0.256736\n",
      "Iter 854: f=1.082371e-11, grad_norm=1.183872e-07, step_norm=3.809572e-03, alpha=0.939017\n",
      "Iter 855: f=1.081319e-11, grad_norm=1.592178e-07, step_norm=4.521655e-03, alpha=0.781870\n",
      "Iter 856: f=1.080106e-11, grad_norm=6.473488e-08, step_norm=4.088945e-03, alpha=0.503408\n",
      "Iter 857: f=1.077879e-11, grad_norm=1.599264e-07, step_norm=7.175289e-03, alpha=1.975054\n",
      "Iter 858: f=1.077426e-11, grad_norm=1.537760e-07, step_norm=3.593193e-03, alpha=0.095819\n",
      "Iter 859: f=1.076037e-11, grad_norm=1.361891e-07, step_norm=4.347297e-03, alpha=1.007135\n",
      "Iter 860: f=1.073720e-11, grad_norm=1.592079e-07, step_norm=1.015016e-02, alpha=2.000000\n",
      "Iter 861: f=1.072005e-11, grad_norm=1.563543e-07, step_norm=5.102996e-03, alpha=1.434971\n",
      "Iter 862: f=1.069978e-11, grad_norm=1.460720e-07, step_norm=9.483384e-03, alpha=1.812908\n",
      "Iter 863: f=1.068334e-11, grad_norm=1.248495e-07, step_norm=3.843713e-03, alpha=1.030604\n",
      "Iter 864: f=1.066182e-11, grad_norm=1.593901e-07, step_norm=7.804524e-03, alpha=1.919009\n",
      "Iter 865: f=1.064398e-11, grad_norm=1.243639e-07, step_norm=5.379495e-03, alpha=1.277628\n",
      "Iter 866: f=1.062727e-11, grad_norm=1.620745e-07, step_norm=5.955251e-03, alpha=1.304193\n",
      "Iter 867: f=1.061192e-11, grad_norm=9.480546e-08, step_norm=5.202942e-03, alpha=0.917230\n",
      "Iter 868: f=1.058827e-11, grad_norm=1.692547e-07, step_norm=6.643502e-03, alpha=2.000000\n",
      "Iter 869: f=1.057140e-11, grad_norm=1.224198e-07, step_norm=8.589893e-03, alpha=1.369915\n",
      "Iter 870: f=1.055215e-11, grad_norm=1.778412e-07, step_norm=5.523430e-03, alpha=1.523603\n",
      "Iter 871: f=1.053537e-11, grad_norm=1.133198e-07, step_norm=9.301824e-03, alpha=1.214918\n",
      "Iter 872: f=1.051831e-11, grad_norm=1.734457e-07, step_norm=4.972021e-03, alpha=1.218480\n",
      "Iter 873: f=1.049752e-11, grad_norm=1.330891e-07, step_norm=7.097647e-03, alpha=1.701126\n",
      "Iter 874: f=1.047969e-11, grad_norm=1.704276e-07, step_norm=5.127981e-03, alpha=1.291184\n",
      "Iter 875: f=1.046120e-11, grad_norm=1.192306e-07, step_norm=6.479833e-03, alpha=1.352031\n",
      "Iter 876: f=1.044173e-11, grad_norm=1.703217e-07, step_norm=5.326054e-03, alpha=1.493609\n",
      "Iter 877: f=1.042344e-11, grad_norm=1.183745e-07, step_norm=7.932352e-03, alpha=1.311995\n",
      "Iter 878: f=1.040459e-11, grad_norm=1.747139e-07, step_norm=4.935305e-03, alpha=1.377703\n",
      "Iter 879: f=1.038499e-11, grad_norm=1.265787e-07, step_norm=8.622701e-03, alpha=1.483328\n",
      "Iter 880: f=1.036657e-11, grad_norm=1.774842e-07, step_norm=4.725169e-03, alpha=1.278607\n",
      "Iter 881: f=1.034558e-11, grad_norm=1.317332e-07, step_norm=8.103564e-03, alpha=1.620052\n",
      "Iter 882: f=1.032636e-11, grad_norm=1.784467e-07, step_norm=4.765729e-03, alpha=1.348921\n",
      "Iter 883: f=1.030596e-11, grad_norm=1.285579e-07, step_norm=8.621781e-03, alpha=1.523216\n",
      "Iter 884: f=1.028675e-11, grad_norm=1.804301e-07, step_norm=4.648428e-03, alpha=1.316211\n",
      "Iter 885: f=1.026541e-11, grad_norm=1.320702e-07, step_norm=8.670338e-03, alpha=1.604029\n",
      "Iter 886: f=1.024579e-11, grad_norm=1.826284e-07, step_norm=4.608836e-03, alpha=1.331367\n",
      "Iter 887: f=1.022430e-11, grad_norm=1.322381e-07, step_norm=8.990114e-03, alpha=1.590774\n",
      "Iter 888: f=1.020454e-11, grad_norm=1.847466e-07, step_norm=4.542225e-03, alpha=1.315011\n",
      "Iter 889: f=1.018238e-11, grad_norm=1.342853e-07, step_norm=9.025032e-03, alpha=1.630103\n",
      "Iter 890: f=1.016221e-11, grad_norm=1.867504e-07, step_norm=4.515799e-03, alpha=1.329219\n",
      "Iter 891: f=1.013992e-11, grad_norm=1.345202e-07, step_norm=9.330646e-03, alpha=1.611197\n",
      "Iter 892: f=1.011961e-11, grad_norm=1.887161e-07, step_norm=4.464301e-03, alpha=1.309062\n",
      "Iter 893: f=1.009661e-11, grad_norm=1.367006e-07, step_norm=9.261199e-03, alpha=1.649381\n",
      "Iter 894: f=1.007580e-11, grad_norm=1.904902e-07, step_norm=4.452195e-03, alpha=1.329559\n",
      "Iter 895: f=1.005285e-11, grad_norm=1.365438e-07, step_norm=9.646205e-03, alpha=1.610096\n",
      "Iter 896: f=1.003200e-11, grad_norm=1.922927e-07, step_norm=4.402643e-03, alpha=1.296443\n",
      "Iter 897: f=1.000810e-11, grad_norm=1.394967e-07, step_norm=9.383329e-03, alpha=1.666960\n",
      "Iter 898: f=9.986561e-12, grad_norm=1.939126e-07, step_norm=4.408796e-03, alpha=1.335013\n",
      "Iter 899: f=9.963124e-12, grad_norm=1.383004e-07, step_norm=1.002619e-02, alpha=1.586822\n",
      "Iter 900: f=9.941801e-12, grad_norm=1.956348e-07, step_norm=4.347698e-03, alpha=1.271914\n",
      "Iter 901: f=9.916762e-12, grad_norm=1.431486e-07, step_norm=9.333715e-03, alpha=1.698436\n",
      "Iter 902: f=9.894337e-12, grad_norm=1.970417e-07, step_norm=4.386964e-03, alpha=1.354156\n",
      "Iter 903: f=9.870745e-12, grad_norm=1.394386e-07, step_norm=1.059239e-02, alpha=1.529395\n",
      "Iter 904: f=9.849131e-12, grad_norm=1.986076e-07, step_norm=4.293059e-03, alpha=1.226712\n",
      "Iter 905: f=9.822457e-12, grad_norm=1.484966e-07, step_norm=8.950246e-03, alpha=1.766768\n",
      "Iter 906: f=9.798782e-12, grad_norm=1.994960e-07, step_norm=4.417787e-03, alpha=1.408240\n",
      "Iter 907: f=9.775726e-12, grad_norm=1.392583e-07, step_norm=1.148707e-02, alpha=1.410027\n",
      "Iter 908: f=9.754127e-12, grad_norm=2.004845e-07, step_norm=4.240101e-03, alpha=1.154540\n",
      "Iter 909: f=9.725100e-12, grad_norm=1.565355e-07, step_norm=8.077013e-03, alpha=1.885520\n",
      "Iter 910: f=9.699572e-12, grad_norm=1.996533e-07, step_norm=4.644137e-03, alpha=1.522112\n",
      "Iter 911: f=9.677370e-12, grad_norm=1.389860e-07, step_norm=1.233072e-02, alpha=1.266470\n",
      "Iter 912: f=9.655753e-12, grad_norm=2.011670e-07, step_norm=4.186587e-03, alpha=1.094729\n",
      "Iter 913: f=9.624604e-12, grad_norm=1.643453e-07, step_norm=7.465153e-03, alpha=1.972222\n",
      "Iter 914: f=9.597299e-12, grad_norm=1.989217e-07, step_norm=5.066055e-03, alpha=1.616112\n",
      "Iter 915: f=9.574360e-12, grad_norm=1.426130e-07, step_norm=1.235506e-02, alpha=1.258865\n",
      "Iter 916: f=9.552005e-12, grad_norm=2.042997e-07, step_norm=4.120756e-03, alpha=1.092634\n",
      "Iter 917: f=9.520065e-12, grad_norm=1.671619e-07, step_norm=7.935846e-03, alpha=1.947729\n",
      "Iter 918: f=9.492184e-12, grad_norm=2.039628e-07, step_norm=4.935683e-03, alpha=1.576608\n",
      "Iter 919: f=9.469028e-12, grad_norm=1.448018e-07, step_norm=1.289628e-02, alpha=1.211821\n",
      "Iter 920: f=9.446141e-12, grad_norm=2.069468e-07, step_norm=4.130214e-03, alpha=1.073024\n",
      "Iter 921: f=9.412799e-12, grad_norm=1.719437e-07, step_norm=7.878303e-03, alpha=1.966824\n",
      "Iter 922: f=9.383778e-12, grad_norm=2.064378e-07, step_norm=5.108015e-03, alpha=1.592554\n",
      "Iter 923: f=9.359950e-12, grad_norm=1.481432e-07, step_norm=1.308178e-02, alpha=1.198971\n",
      "Iter 924: f=9.336322e-12, grad_norm=2.101396e-07, step_norm=4.110851e-03, alpha=1.068212\n",
      "Iter 925: f=9.301988e-12, grad_norm=1.753333e-07, step_norm=8.204990e-03, alpha=1.953748\n",
      "Iter 926: f=9.272228e-12, grad_norm=2.108507e-07, step_norm=5.064304e-03, alpha=1.567351\n",
      "Iter 927: f=9.248063e-12, grad_norm=1.507893e-07, step_norm=1.351957e-02, alpha=1.162707\n",
      "Iter 928: f=9.223799e-12, grad_norm=2.129112e-07, step_norm=4.124168e-03, alpha=1.056483\n",
      "Iter 929: f=9.188204e-12, grad_norm=1.795941e-07, step_norm=8.320439e-03, alpha=1.958831\n",
      "Iter 930: f=9.157461e-12, grad_norm=2.141943e-07, step_norm=5.162755e-03, alpha=1.565323\n",
      "Iter 931: f=9.132791e-12, grad_norm=1.538882e-07, step_norm=1.381754e-02, alpha=1.139589\n",
      "Iter 932: f=9.107814e-12, grad_norm=2.158547e-07, step_norm=4.127113e-03, alpha=1.050416\n",
      "Iter 933: f=9.071143e-12, grad_norm=1.833036e-07, step_norm=8.607946e-03, alpha=1.951337\n",
      "Iter 934: f=9.039616e-12, grad_norm=2.182361e-07, step_norm=5.166044e-03, alpha=1.546092\n",
      "Iter 935: f=9.014576e-12, grad_norm=1.567777e-07, step_norm=1.421608e-02, alpha=1.109891\n",
      "Iter 936: f=8.988924e-12, grad_norm=2.185974e-07, step_norm=4.146391e-03, alpha=1.043187\n",
      "Iter 937: f=8.951127e-12, grad_norm=1.871951e-07, step_norm=8.844700e-03, alpha=1.948498\n",
      "Iter 938: f=8.918784e-12, grad_norm=2.219202e-07, step_norm=5.211285e-03, alpha=1.532629\n",
      "Iter 939: f=8.893360e-12, grad_norm=1.597470e-07, step_norm=1.457844e-02, alpha=1.083803\n",
      "Iter 940: f=8.867033e-12, grad_norm=2.212350e-07, step_norm=4.165719e-03, alpha=1.038008\n",
      "Iter 941: f=8.828215e-12, grad_norm=1.908344e-07, step_norm=9.151449e-03, alpha=1.941667\n",
      "Iter 942: f=8.795173e-12, grad_norm=2.256914e-07, step_norm=5.223058e-03, alpha=1.513221\n",
      "Iter 943: f=8.769426e-12, grad_norm=1.626276e-07, step_norm=1.496596e-02, alpha=1.057891\n",
      "Iter 944: f=8.742457e-12, grad_norm=2.236792e-07, step_norm=4.192427e-03, alpha=1.033599\n",
      "Iter 945: f=8.702698e-12, grad_norm=1.943135e-07, step_norm=9.475973e-03, alpha=1.934121\n",
      "Iter 946: f=8.669045e-12, grad_norm=2.292488e-07, step_norm=5.231165e-03, alpha=1.492515\n",
      "Iter 947: f=8.643003e-12, grad_norm=1.654537e-07, step_norm=1.534390e-02, alpha=1.034912\n",
      "Iter 948: f=8.615434e-12, grad_norm=2.259039e-07, step_norm=4.221607e-03, alpha=1.030330\n",
      "Iter 949: f=8.574872e-12, grad_norm=1.975034e-07, step_norm=9.844089e-03, alpha=1.924152\n",
      "Iter 950: f=8.540754e-12, grad_norm=2.325720e-07, step_norm=5.219865e-03, alpha=1.468076\n",
      "Iter 951: f=8.514435e-12, grad_norm=1.682206e-07, step_norm=1.571188e-02, alpha=1.016342\n",
      "Iter 952: f=8.486334e-12, grad_norm=2.278487e-07, step_norm=4.251776e-03, alpha=1.027529\n",
      "Iter 953: f=8.445140e-12, grad_norm=2.003670e-07, step_norm=1.024801e-02, alpha=1.912226\n",
      "Iter 954: f=8.410716e-12, grad_norm=2.355243e-07, step_norm=5.195752e-03, alpha=1.440996\n",
      "Iter 955: f=8.384138e-12, grad_norm=1.708812e-07, step_norm=1.605620e-02, alpha=1.003019\n",
      "Iter 956: f=8.355602e-12, grad_norm=2.294383e-07, step_norm=4.279795e-03, alpha=1.024996\n",
      "Iter 957: f=8.313997e-12, grad_norm=2.027986e-07, step_norm=1.069108e-02, alpha=1.897519\n",
      "Iter 958: f=8.279463e-12, grad_norm=2.379551e-07, step_norm=5.157213e-03, alpha=1.410941\n",
      "Iter 959: f=8.252630e-12, grad_norm=1.734169e-07, step_norm=1.636416e-02, alpha=0.996958\n",
      "Iter 960: f=8.223785e-12, grad_norm=2.305996e-07, step_norm=4.302309e-03, alpha=1.022261\n",
      "Iter 961: f=8.182041e-12, grad_norm=2.047092e-07, step_norm=1.117254e-02, alpha=1.879820\n",
      "Iter 962: f=8.147626e-12, grad_norm=2.397041e-07, step_norm=5.106288e-03, alpha=1.378116\n",
      "Iter 963: f=8.120544e-12, grad_norm=1.757901e-07, step_norm=1.662390e-02, alpha=0.999315\n",
      "Iter 964: f=8.091558e-12, grad_norm=2.312038e-07, step_norm=4.315940e-03, alpha=1.018763\n",
      "Iter 965: f=8.049991e-12, grad_norm=2.060053e-07, step_norm=1.168550e-02, alpha=1.859132\n",
      "Iter 966: f=8.015954e-12, grad_norm=2.405517e-07, step_norm=5.046822e-03, alpha=1.342960\n",
      "Iter 967: f=7.988643e-12, grad_norm=1.779200e-07, step_norm=1.682366e-02, alpha=1.011187\n",
      "Iter 968: f=7.959718e-12, grad_norm=2.311293e-07, step_norm=4.318592e-03, alpha=1.014488\n",
      "Iter 969: f=7.918704e-12, grad_norm=2.065451e-07, step_norm=1.223000e-02, alpha=1.834883\n",
      "Iter 970: f=7.885350e-12, grad_norm=2.402735e-07, step_norm=4.976921e-03, alpha=1.304938\n",
      "Iter 971: f=7.857842e-12, grad_norm=1.797567e-07, step_norm=1.695236e-02, alpha=1.034408\n",
      "Iter 972: f=7.829225e-12, grad_norm=2.301991e-07, step_norm=4.307867e-03, alpha=1.008923\n",
      "Iter 973: f=7.789190e-12, grad_norm=2.061577e-07, step_norm=1.279152e-02, alpha=1.806913\n",
      "Iter 974: f=7.756825e-12, grad_norm=2.387000e-07, step_norm=4.904581e-03, alpha=1.266435\n",
      "Iter 975: f=7.729274e-12, grad_norm=1.809919e-07, step_norm=1.701878e-02, alpha=1.065142\n",
      "Iter 976: f=7.701247e-12, grad_norm=2.281526e-07, step_norm=4.288870e-03, alpha=1.003247\n",
      "Iter 977: f=7.662651e-12, grad_norm=2.046582e-07, step_norm=1.336568e-02, alpha=1.775479\n",
      "Iter 978: f=7.631631e-12, grad_norm=2.355613e-07, step_norm=4.823141e-03, alpha=1.226369\n",
      "Iter 979: f=7.604218e-12, grad_norm=1.815369e-07, step_norm=1.701557e-02, alpha=1.106073\n",
      "Iter 980: f=7.577100e-12, grad_norm=2.247967e-07, step_norm=4.261556e-03, alpha=0.997261\n",
      "Iter 981: f=7.540438e-12, grad_norm=2.018343e-07, step_norm=1.392759e-02, alpha=1.740672\n",
      "Iter 982: f=7.511122e-12, grad_norm=2.306153e-07, step_norm=4.737244e-03, alpha=1.186385\n",
      "Iter 983: f=7.484156e-12, grad_norm=1.810260e-07, step_norm=1.694918e-02, alpha=1.154033\n",
      "Iter 984: f=7.458298e-12, grad_norm=2.198332e-07, step_norm=4.230768e-03, alpha=0.992098\n",
      "Iter 985: f=7.424083e-12, grad_norm=1.974148e-07, step_norm=1.446046e-02, alpha=1.702321\n",
      "Iter 986: f=7.396843e-12, grad_norm=2.236419e-07, step_norm=4.643285e-03, alpha=1.146988\n",
      "Iter 987: f=7.370724e-12, grad_norm=1.791235e-07, step_norm=1.682037e-02, alpha=1.207919\n",
      "Iter 988: f=7.346504e-12, grad_norm=2.129781e-07, step_norm=4.199343e-03, alpha=0.988441\n",
      "Iter 989: f=7.315242e-12, grad_norm=1.911381e-07, step_norm=1.493117e-02, alpha=1.660867\n",
      "Iter 990: f=7.290440e-12, grad_norm=2.144075e-07, step_norm=4.540773e-03, alpha=1.109357\n",
      "Iter 991: f=7.265667e-12, grad_norm=1.754180e-07, step_norm=1.663139e-02, alpha=1.265502\n",
      "Iter 992: f=7.243478e-12, grad_norm=2.039484e-07, step_norm=4.168219e-03, alpha=0.986862\n",
      "Iter 993: f=7.215629e-12, grad_norm=1.827512e-07, step_norm=1.529571e-02, alpha=1.617445\n",
      "Iter 994: f=7.193599e-12, grad_norm=2.026975e-07, step_norm=4.428486e-03, alpha=1.074824\n",
      "Iter 995: f=7.170751e-12, grad_norm=1.694264e-07, step_norm=1.637382e-02, alpha=1.323639\n",
      "Iter 996: f=7.150985e-12, grad_norm=1.924171e-07, step_norm=4.135111e-03, alpha=0.987750\n",
      "Iter 997: f=7.126931e-12, grad_norm=1.720094e-07, step_norm=1.550231e-02, alpha=1.574121\n",
      "Iter 998: f=7.107951e-12, grad_norm=1.883799e-07, step_norm=4.305469e-03, alpha=1.045548\n",
      "Iter 999: f=7.087654e-12, grad_norm=1.606205e-07, step_norm=1.603904e-02, alpha=1.377794\n",
      "Iter 1000: f=7.070674e-12, grad_norm=1.781328e-07, step_norm=4.090438e-03, alpha=0.990888\n",
      "Iter 1001: f=7.050664e-12, grad_norm=1.587310e-07, step_norm=1.548654e-02, alpha=1.535140\n",
      "Iter 1002: f=7.034934e-12, grad_norm=1.713700e-07, step_norm=4.168701e-03, alpha=1.023723\n",
      "Iter 1003: f=7.017785e-12, grad_norm=1.485024e-07, step_norm=1.559243e-02, alpha=1.423002\n",
      "Iter 1004: f=7.003883e-12, grad_norm=1.608280e-07, step_norm=4.017355e-03, alpha=0.995715\n",
      "Iter 1005: f=6.988012e-12, grad_norm=1.427825e-07, step_norm=1.518796e-02, alpha=1.506221\n",
      "Iter 1006: f=6.975637e-12, grad_norm=1.516136e-07, step_norm=4.010210e-03, alpha=1.010985\n",
      "Iter 1007: f=6.962080e-12, grad_norm=1.327780e-07, step_norm=1.497545e-02, alpha=1.457564\n",
      "Iter 1008: f=6.951426e-12, grad_norm=1.403454e-07, step_norm=3.891025e-03, alpha=1.001293\n",
      "Iter 1009: f=6.939629e-12, grad_norm=1.240423e-07, step_norm=1.454609e-02, alpha=1.493750\n",
      "Iter 1010: f=6.930587e-12, grad_norm=1.290677e-07, step_norm=3.814891e-03, alpha=1.008359\n",
      "Iter 1011: f=6.920809e-12, grad_norm=1.133051e-07, step_norm=1.408509e-02, alpha=1.482743\n",
      "Iter 1012: f=6.913385e-12, grad_norm=1.166165e-07, step_norm=3.688092e-03, alpha=1.008923\n",
      "Iter 1013: f=6.905437e-12, grad_norm=1.023163e-07, step_norm=1.349133e-02, alpha=1.500722\n",
      "Iter 1014: f=6.899543e-12, grad_norm=1.036088e-07, step_norm=3.553134e-03, alpha=1.015609\n",
      "Iter 1015: f=6.893383e-12, grad_norm=9.021228e-08, step_norm=1.277145e-02, alpha=1.508727\n",
      "Iter 1016: f=6.888929e-12, grad_norm=8.972537e-08, step_norm=3.381555e-03, alpha=1.022618\n",
      "Iter 1017: f=6.884397e-12, grad_norm=7.741520e-08, step_norm=1.189234e-02, alpha=1.528959\n",
      "Iter 1018: f=6.881246e-12, grad_norm=7.514262e-08, step_norm=3.172963e-03, alpha=1.033388\n",
      "Iter 1019: f=6.878170e-12, grad_norm=6.377351e-08, step_norm=1.081005e-02, alpha=1.554278\n",
      "Iter 1020: f=6.876151e-12, grad_norm=5.980450e-08, step_norm=2.906532e-03, alpha=1.047074\n",
      "Iter 1021: f=6.874302e-12, grad_norm=4.943097e-08, step_norm=9.461964e-03, alpha=1.594458\n",
      "Iter 1022: f=6.873203e-12, grad_norm=4.375954e-08, step_norm=2.548107e-03, alpha=1.062776\n",
      "Iter 1023: f=6.872307e-12, grad_norm=3.452386e-08, step_norm=7.728838e-03, alpha=1.659584\n",
      "Iter 1024: f=6.871872e-12, grad_norm=2.705679e-08, step_norm=2.014328e-03, alpha=1.070174\n",
      "Iter 1025: f=6.871603e-12, grad_norm=1.946975e-08, step_norm=5.295796e-03, alpha=1.757392\n",
      "Iter 1026: f=6.871531e-12, grad_norm=1.002703e-08, step_norm=1.099300e-03, alpha=1.003799\n",
      "Iter 1027: f=6.871516e-12, grad_norm=5.861427e-09, step_norm=1.195843e-03, alpha=1.307110\n",
      "Optimised parameters: [ 4.91338747e-01  2.51088950e-01  1.04900181e-01  5.75164250e-02\n",
      "  3.62410836e-02  2.35194657e-02  1.50321074e-02  9.30787769e-03\n",
      "  5.52076150e-03  3.08920566e-03  1.58869889e-03  7.20752024e-04\n",
      " -1.01050672e+00  8.11681930e-01  2.06340935e+00  3.06281699e+00\n",
      "  4.01344539e+00  4.99925196e+00  6.05645585e+00  7.21193261e+00\n",
      "  8.50109028e+00  9.98428086e+00  1.17867177e+01  1.42838166e+01]\n",
      "Final gradient norm: 5.453738556856839e-10\n",
      "Final objective value: 2.6213576350715283e-06\n"
     ]
    }
   ],
   "source": [
    "# --- synthetic data ---------------------------------------------------\n",
    "n_terms = 12               # number of (a_i, b_i) pairs\n",
    "N       = 500               # data points\n",
    "\n",
    "d, w = gauss_legendre_rule_multilevel(0.0, 1.0, 100, L=5, ratio=0.2)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "\n",
    "b = np.linspace(-1., 11.5, n_terms)\n",
    "a = optimal_a(d, w, target, np.exp(b))\n",
    "print(\"Optimal a:\", a)\n",
    "\n",
    "means = np.concatenate((a, b))  # initial guess in log-space\n",
    "\n",
    "# --- optimiser call ---------------------------------------------------\n",
    "x, grad_norm, f_history, grad_norm_history = newton(\n",
    "    f=obj,\n",
    "    grad=grad,\n",
    "    hess=hess,\n",
    "    x0=means,\n",
    "    tol_grad=1e-9,\n",
    "    tol_step=1e-8,\n",
    "    stall_iter=50,\n",
    "    max_iter=3000,\n",
    "    ls_bounds=(-2.0, 2.0),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Optimised parameters:\", x)\n",
    "print(\"Final gradient norm:\", grad_norm)\n",
    "print(\"Final objective value:\", np.sqrt(f_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddd19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: best_score=0.0013950703793328882, max_std=1.1356440572972426, info={'normg': 0.13175763560330805}\n",
      "Iter 1: best_score=0.0009889552061308979, max_std=1.2447310414460087, info={'normg': 0.13790330909674978}\n",
      "Iter 2: best_score=0.0009889552061308979, max_std=1.3869532696309697, info={'normg': 0.07160743968188284}\n",
      "Iter 3: best_score=0.0009889552061308979, max_std=1.4030271446417653, info={'normg': 0.1294265030283102}\n",
      "Iter 4: best_score=0.0009889552061308979, max_std=1.3402764583545088, info={'normg': 0.08211164220393306}\n",
      "Iter 5: best_score=0.0009889552061308979, max_std=1.3223914089623938, info={'normg': 0.08433113742867276}\n",
      "Iter 6: best_score=0.0009889552061308979, max_std=1.2070336778372304, info={'normg': 0.08518818383235606}\n",
      "Iter 7: best_score=0.0009889552061308979, max_std=1.1772958148914703, info={'normg': 0.09000993917449461}\n",
      "Iter 8: best_score=0.0006586318020339291, max_std=1.024109070485738, info={'normg': 0.06882914264805107}\n",
      "Iter 9: best_score=0.0006586318020339291, max_std=1.0974909127010353, info={'normg': 0.0922613830345869}\n",
      "Iter 10: best_score=0.0006586318020339291, max_std=0.9518810190479945, info={'normg': 0.08999283289956882}\n",
      "Iter 11: best_score=0.0006586318020339291, max_std=0.8945710810052168, info={'normg': 0.10144263225661267}\n",
      "Iter 12: best_score=0.0006586318020339291, max_std=0.7288917997108284, info={'normg': 0.07536397248296436}\n",
      "Iter 13: best_score=0.0003649344987902314, max_std=0.8316380091520996, info={'normg': 0.08090243376291358}\n",
      "Iter 14: best_score=0.0003649344987902314, max_std=0.7590712849856411, info={'normg': 0.0663452028119802}\n",
      "Iter 15: best_score=0.0003649344987902314, max_std=0.6438844169211918, info={'normg': 0.02195853472836657}\n",
      "Iter 16: best_score=0.00027520291622068347, max_std=0.5772602881151687, info={'normg': 0.020240989776893377}\n",
      "Iter 17: best_score=0.00027520291622068347, max_std=0.5198112840635483, info={'normg': 0.055626152767178784}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     82\u001b[39m init_std = np.full(n_terms, \u001b[32m1.0\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m#  Run Cross-Entropy\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m best_b_log, best_score, ce_info = \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mce_user_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43melite_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_std\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1234\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_g\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_std\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# prints per-iteration progress\u001b[39;49;00m\n\u001b[32m    100\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m#  Report results\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m    105\u001b[39m best_b = np.exp(best_b_log)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(user_fn, initial_mean, initial_std, pop_size, elite_frac, alpha_mean, alpha_std, n_iters, random_state, verbose, tol_std, tol_g)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iters):\n\u001b[32m     53\u001b[39m     samples = rng.randn(pop_size, mean.size) * std + mean\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     samples, scores, info_dict = \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     idx = \u001b[38;5;28mint\u001b[39m(np.argmin(scores))\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# update best if improved\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mce_user_fn\u001b[39m\u001b[34m(b_log_samples)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# ---- (1) build starting point --------------------------------------\u001b[39;00m\n\u001b[32m     39\u001b[39m     b_pos = np.exp(b_log_samples[i])            \u001b[38;5;66;03m# (K,)  positive\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     a_opt = \u001b[43moptimal_a\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_pos\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# (K,)\u001b[39;00m\n\u001b[32m     41\u001b[39m     p0 = np.concatenate([a_opt, b_pos])      \u001b[38;5;66;03m# shape (2K,)\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# ---- (2) limited Newton polishing ----------------------------------\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36moptimal_a\u001b[39m\u001b[34m(d, w, target, b)\u001b[39m\n\u001b[32m     36\u001b[39m Phi_w = Phi * W_sqrt[:, \u001b[38;5;28;01mNone\u001b[39;00m]        \u001b[38;5;66;03m# shape (J, K)\u001b[39;00m\n\u001b[32m     37\u001b[39m y_w = target * W_sqrt              \u001b[38;5;66;03m# shape (J,)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m a, _ = \u001b[43mnnls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPhi_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/scipy/optimize/_nnls.py:93\u001b[39m, in \u001b[36mnnls\u001b[39m\u001b[34m(A, b, maxiter, atol)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m maxiter:\n\u001b[32m     92\u001b[39m     maxiter = \u001b[32m3\u001b[39m*n\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m x, rnorm, info = \u001b[43m_nnls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info == -\u001b[32m1\u001b[39m:\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMaximum number of iterations reached.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mscipy/optimize/_cython_nnls.pyx:216\u001b[39m, in \u001b[36mscipy.optimize._cython_nnls._nnls\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/numpy/linalg/_linalg.py:2618\u001b[39m, in \u001b[36m_norm_dispatcher\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2614\u001b[39m     result = op(svd(y, compute_uv=\u001b[38;5;28;01mFalse\u001b[39;00m), axis=-\u001b[32m1\u001b[39m, initial=initial)\n\u001b[32m   2615\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_norm_dispatcher\u001b[39m(x, \u001b[38;5;28mord\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, axis=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (x,)\n\u001b[32m   2622\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_norm_dispatcher)\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnorm\u001b[39m(x, \u001b[38;5;28mord\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, axis=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#  Cross-Entropy search over log-b with internal Newton polishing\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "\n",
    "# --- problem size -----------------------------------------------------------\n",
    "n_terms = 8          # K  (= length of b and a)\n",
    "newton_steps = 1     # how many Newton iterations per sample\n",
    "pop_size = 100       # CE population\n",
    "np.random.seed(0)    # reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  helper: CE user-defined update function operating on log-b only\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def ce_user_fn(b_log_samples: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b_log_samples : (N, K) ndarray\n",
    "        Current population in *log* space.\n",
    "    Returns\n",
    "    -------\n",
    "    updated_log_b : (N, K) ndarray\n",
    "        Polished & sorted log-b that CE will keep using.\n",
    "    scores        : (N,)  ndarray\n",
    "        Objective values after polishing (smaller is better).\n",
    "    info          : dict\n",
    "        Contains minimum gradient norm among all samples (for CE stopping).\n",
    "    \"\"\"\n",
    "    N, K = b_log_samples.shape\n",
    "    updated = np.empty_like(b_log_samples)\n",
    "    scores = np.empty(N)\n",
    "    grad_norm = np.empty(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        # ---- (1) build starting point --------------------------------------\n",
    "        b_pos = np.exp(b_log_samples[i])            # (K,)  positive\n",
    "        a_opt = optimal_a(d, w, target, b_pos)      # (K,)\n",
    "        p0 = np.concatenate([a_opt, b_pos])      # shape (2K,)\n",
    "\n",
    "        # ---- (2) limited Newton polishing ----------------------------------\n",
    "        p_new, gnorm, *_ = newton(\n",
    "            f=obj,\n",
    "            grad=grad,\n",
    "            hess=hess,\n",
    "            x0=p0,\n",
    "            max_iter=newton_steps,\n",
    "            tol_grad=1e-6,\n",
    "            tol_step=1e-6,\n",
    "            stall_iter=newton_steps,\n",
    "            ls_bounds=(-2., 2.),\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        if not np.any(np.isnan(p_new)) and not np.any(np.isinf(p_new)) and not np.any(p_new < 0):\n",
    "            # ---- (3) extract & sort b (log) ------------------------------------\n",
    "            b_new_pos = np.clip(p_new[K:], 1e-12, None)  # keep positivity\n",
    "            b_new_log = np.log(b_new_pos)\n",
    "            order = np.argsort(b_new_log)\n",
    "            a_opt = optimal_a(d, w, target, b_new_pos)      # (K,)\n",
    "            p0 = np.concatenate([a_opt, b_new_pos])      # shape (2K,)\n",
    "\n",
    "            updated[i] = b_new_log[order]\n",
    "            scores[i] = obj(p0)       # score *after* polishing\n",
    "            grad_norm[i] = np.linalg.norm(grad(p0))\n",
    "        else:\n",
    "            # if polishing failed, keep the original sample\n",
    "            updated[i] = b_log_samples[i]\n",
    "            scores[i] = np.inf\n",
    "            grad_norm[i] = np.inf\n",
    "\n",
    "    info = {'normg': float(grad_norm.min())}\n",
    "    return updated, scores, info\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  CE initial distribution over log-b\n",
    "# --------------------------------------------------------------------------- #\n",
    "init_mean = np.linspace(-1.5, 9.3, n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 1.0)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Run Cross-Entropy\n",
    "# --------------------------------------------------------------------------- #\n",
    "best_b_log, best_score, ce_info = cross_entropy(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    elite_frac=0.2,\n",
    "    alpha_mean=0.5,\n",
    "    alpha_std=0.5,\n",
    "    n_iters=2000,\n",
    "    random_state=1234,\n",
    "    tol_g=1e-6,\n",
    "    tol_std=1e-6,\n",
    "    verbose=True      # prints per-iteration progress\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Report results\n",
    "# --------------------------------------------------------------------------- #\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = obj(np.concatenate([best_a, best_b]))\n",
    "\n",
    "print(\"\\n====================  CE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"CE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"stopped at iteration\", ce_info['iter'],\n",
    "      \"| max Ïƒ =\", ce_info['max_std_history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "830c3d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.48353019,  0.04141208,  1.34315267,  2.62557281,  4.04747186,\n",
       "        5.72257207,  7.78431983, 10.25991933])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_b_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f43c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Differential-Evolution optimiser that works with the same `user_fn`\n",
    "###############################################################################\n",
    "from typing import Callable, Tuple, Dict, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def differential_evolution(\n",
    "    user_fn: Callable[[np.ndarray],\n",
    "                      Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],\n",
    "    initial_mean: np.ndarray,\n",
    "    initial_std: np.ndarray,\n",
    "    pop_size: int = 100,\n",
    "    F: float = 0.8,                 # differential weight\n",
    "    CR: float = 0.9,                # crossover probability\n",
    "    n_iters: int = 500,\n",
    "    random_state: Optional[int] = None,\n",
    "    tol_g: float = 1e-6,            # stop if minâ€–gâ€– drops below this\n",
    "    stall_generations: int = 50,    # stop if no improvement for so many gens\n",
    "    verbose: bool = False\n",
    ") -> Tuple[np.ndarray, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Differential Evolution (DE/rand/1/bin) with the CE-style `user_fn`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_sample : ndarray\n",
    "        The best parameter vector found (d,).\n",
    "    best_score  : float\n",
    "        Objective value of `best_sample`.\n",
    "    info        : dict\n",
    "        Diagnostics (`iter`, `best_score_history`).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    d = initial_mean.size\n",
    "\n",
    "    # --- initial population --------------------------------------------------\n",
    "    pop = rng.randn(pop_size, d) * initial_std + initial_mean       # (N, d)\n",
    "    pop, scores, info = user_fn(pop)                               # polish & score\n",
    "    best_idx = int(np.argmin(scores))\n",
    "    best_sample = pop[best_idx].copy()\n",
    "    best_score = float(scores[best_idx])\n",
    "\n",
    "    best_score_history = [best_score]\n",
    "    no_improve = 0\n",
    "\n",
    "    for it in range(1, n_iters + 1):\n",
    "        # ------------ mutation & crossover ----------------------------------\n",
    "        trial = np.empty_like(pop)\n",
    "        for i in range(pop_size):\n",
    "            # choose three *distinct* indices â‰  i\n",
    "            r1, r2, r3 = rng.choice([j for j in range(pop_size) if j != i],\n",
    "                                    size=3, replace=False)\n",
    "            mutant = pop[r1] + F * (pop[r2] - pop[r3])\n",
    "\n",
    "            # binomial crossover\n",
    "            cross_pts = rng.rand(d) < CR\n",
    "            cross_pts[rng.randint(0, d)] = True      # ensure at least one gene\n",
    "            trial[i] = np.where(cross_pts, mutant, pop[i])\n",
    "\n",
    "        # ------------ evaluate trial population ------------------------------\n",
    "        trial, trial_scores, info_trial = user_fn(trial)\n",
    "\n",
    "        # ------------ selection ----------------------------------------------\n",
    "        improved = trial_scores < scores\n",
    "        pop[improved] = trial[improved]\n",
    "        scores[improved] = trial_scores[improved]\n",
    "\n",
    "        # ------------ keep track of best -------------------------------------\n",
    "        gen_best_idx = int(np.argmin(scores))\n",
    "        gen_best_score = float(scores[gen_best_idx])\n",
    "        if gen_best_score + 1e-12 < best_score:      # strict improvement\n",
    "            best_score = gen_best_score\n",
    "            best_sample = pop[gen_best_idx].copy()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        best_score_history.append(best_score)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Gen {it:4d}  best={best_score:.4e} \"\n",
    "                  f\"minâ€–gâ€–={info_trial.get('normg', np.inf):.2e}\")\n",
    "\n",
    "        # ------------ stopping criteria --------------------------------------\n",
    "        if info_trial.get('normg', np.inf) <= tol_g:\n",
    "            if verbose:\n",
    "                print(\"Stopping: gradient norm below tol_g\")\n",
    "            break\n",
    "        if no_improve >= stall_generations:\n",
    "            if verbose:\n",
    "                print(\"Stopping: no improvement for\",\n",
    "                      stall_generations, \"generations\")\n",
    "            break\n",
    "\n",
    "    info = {\n",
    "        'iter': it,\n",
    "        'best_score_history': best_score_history\n",
    "    }\n",
    "    return best_sample, best_score, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f1ddd9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen    1  best=9.7662e-03 minâ€–gâ€–=9.96e-06\n",
      "Gen    2  best=9.7662e-03 minâ€–gâ€–=2.81e-05\n",
      "Gen    3  best=9.7662e-03 minâ€–gâ€–=4.36e-05\n",
      "Gen    4  best=9.7657e-03 minâ€–gâ€–=3.94e-06\n",
      "Gen    5  best=9.7656e-03 minâ€–gâ€–=1.36e-06\n",
      "Gen    6  best=9.7656e-03 minâ€–gâ€–=6.33e-08\n",
      "Gen    7  best=9.7656e-03 minâ€–gâ€–=3.62e-10\n",
      "Stopping: gradient norm below tol_g\n",
      "\n",
      "====================  DE RESULT  ====================\n",
      "best log-b : [-0.34875468  2.6915483 ]\n",
      "best b     : [ 0.7055662 14.7545027]\n",
      "best a     : [0.71376705 0.24218186]\n",
      "DE score   : 0.00976555851710301\n",
      "check obj  : 0.2119444539178491\n",
      "generations: 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "#  TEST SCRIPT  â€”  optimise log-b with the same `ce_user_fn`\n",
    "###############################################################################\n",
    "\n",
    "# --- problem & helper already defined earlier ---------------------------\n",
    "# n_terms, ce_user_fn, d, w, target, obj â€¦ are assumed to be in scope\n",
    "\n",
    "pop_size = 20\n",
    "rng_seed = 1234\n",
    "\n",
    "# --- problem size -----------------------------------------------------------\n",
    "n_terms = 2          # K  (= length of b and a)\n",
    "newton_steps = 1     # how many Newton iterations per sample\n",
    "np.random.seed(1)    # reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  helper: CE user-defined update function operating on log-b only\n",
    "# --------------------------------------------------------------------------- #\n",
    "d, w = gauss_legendre_rule_multilevel(0.0, 1.0, 100, L=5, ratio=0.2)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "\n",
    "def ce_user_fn(b_log_samples: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b_log_samples : (N, K) ndarray\n",
    "        Current population in *log* space.\n",
    "    Returns\n",
    "    -------\n",
    "    updated_log_b : (N, K) ndarray\n",
    "        Polished & sorted log-b that CE will keep using.\n",
    "    scores        : (N,)  ndarray\n",
    "        Objective values after polishing (smaller is better).\n",
    "    info          : dict\n",
    "        Contains minimum gradient norm among all samples (for CE stopping).\n",
    "    \"\"\"\n",
    "    N, K = b_log_samples.shape\n",
    "    updated = np.empty_like(b_log_samples)\n",
    "    scores = np.empty(N)\n",
    "    grad_norm = np.empty(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        # ---- (1) build starting point --------------------------------------\n",
    "        b_pos = (b_log_samples[i])            # (K,)  positive\n",
    "        a_opt = optimal_a(d, w, target, np.exp(b_pos))      # (K,)\n",
    "        p0 = np.concatenate([a_opt, b_pos])      # shape (2K,)\n",
    "\n",
    "        # ---- (2) limited Newton polishing ----------------------------------\n",
    "        p_new, gnorm, *_ = newton(\n",
    "            f=obj,\n",
    "            grad=grad,\n",
    "            hess=hess,\n",
    "            x0=p0,\n",
    "            max_iter=newton_steps,\n",
    "            tol_grad=1e-16,\n",
    "            tol_step=1e-16,\n",
    "            stall_iter=newton_steps,\n",
    "            ls_bounds=(-2.0, 2.),\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        if not np.any(np.isnan(p_new)) and not np.any(np.isinf(p_new)):\n",
    "            # ---- (3) extract & sort b (log) ------------------------------------\n",
    "            # b_new_pos = np.clip(p_new[K:], 1e-12, None)  # keep positivity\n",
    "            b_new_pos = p_new[K:]\n",
    "            # b_new_log = np.log(b_new_pos)\n",
    "            b_new_log = b_new_pos\n",
    "            order = np.argsort(b_new_log)\n",
    "            a_opt = optimal_a(d, w, target, np.exp(b_new_pos))      # (K,)\n",
    "            p0 = np.concatenate([a_opt, b_new_pos])      # shape (2K,)\n",
    "\n",
    "            updated[i] = b_new_log[order]\n",
    "            scores[i] = np.sqrt(obj(p0))       # score *after* polishing\n",
    "            grad_norm[i] = np.linalg.norm(grad(p0))\n",
    "        else:\n",
    "            # if polishing failed, keep the original sample\n",
    "            updated[i] = b_log_samples[i]\n",
    "            scores[i] = np.inf\n",
    "            grad_norm[i] = np.inf\n",
    "\n",
    "    min_idx = np.argmin(scores)\n",
    "    info = {'normg': float(grad_norm[min_idx])}\n",
    "    return updated, scores, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init_mean = np.linspace(0,  1, n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 1.0)\n",
    "\n",
    "best_b_log, best_score, de_info = differential_evolution(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    F=0.8,\n",
    "    CR=0.9,\n",
    "    n_iters=10,\n",
    "    random_state=rng_seed,\n",
    "    tol_g=1e-9,\n",
    "    stall_generations=75,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- final reconstruction of a and objective ----------------------------\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = np.sqrt(obj(np.concatenate([best_a, best_b])))\n",
    "\n",
    "print(\"\\n====================  DE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"DE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"generations:\", de_info['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8e3be2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen    1  best=5.0741e-06 minâ€–gâ€–=8.78e-11\n",
      "Gen    2  best=3.1283e-06 minâ€–gâ€–=7.73e-11\n",
      "Gen    3  best=3.1283e-06 minâ€–gâ€–=5.79e-11\n",
      "Gen    4  best=1.7608e-06 minâ€–gâ€–=1.71e-11\n",
      "Gen    5  best=1.1591e-06 minâ€–gâ€–=5.37e-12\n",
      "Gen    6  best=1.0894e-06 minâ€–gâ€–=2.69e-11\n",
      "Gen    7  best=1.0894e-06 minâ€–gâ€–=6.22e-11\n",
      "Gen    8  best=1.0164e-06 minâ€–gâ€–=1.64e-11\n",
      "Gen    9  best=1.0164e-06 minâ€–gâ€–=2.19e-11\n",
      "Gen   10  best=1.0164e-06 minâ€–gâ€–=1.36e-11\n",
      "Gen   11  best=9.5676e-07 minâ€–gâ€–=5.48e-12\n",
      "Gen   12  best=9.4692e-07 minâ€–gâ€–=1.37e-12\n",
      "Gen   13  best=8.0547e-07 minâ€–gâ€–=2.60e-12\n",
      "Gen   14  best=7.5250e-07 minâ€–gâ€–=2.16e-12\n",
      "Gen   15  best=7.0118e-07 minâ€–gâ€–=1.14e-11\n",
      "Gen   16  best=6.2503e-07 minâ€–gâ€–=5.50e-12\n",
      "Gen   17  best=5.6474e-07 minâ€–gâ€–=1.36e-12\n",
      "Gen   18  best=5.3749e-07 minâ€–gâ€–=3.28e-12\n",
      "Gen   19  best=5.3749e-07 minâ€–gâ€–=1.61e-12\n",
      "Gen   20  best=4.9644e-07 minâ€–gâ€–=1.46e-12\n",
      "Gen   21  best=4.9644e-07 minâ€–gâ€–=6.78e-13\n",
      "Gen   22  best=4.8150e-07 minâ€–gâ€–=4.69e-13\n",
      "Gen   23  best=4.5814e-07 minâ€–gâ€–=8.72e-13\n",
      "Gen   24  best=4.3910e-07 minâ€–gâ€–=4.44e-13\n",
      "Gen   25  best=4.2218e-07 minâ€–gâ€–=4.30e-13\n",
      "Gen   26  best=4.1402e-07 minâ€–gâ€–=2.33e-13\n",
      "Gen   27  best=4.0993e-07 minâ€–gâ€–=1.63e-13\n",
      "Gen   28  best=4.0483e-07 minâ€–gâ€–=2.50e-13\n",
      "Gen   29  best=4.0083e-07 minâ€–gâ€–=1.57e-13\n",
      "Gen   30  best=3.9759e-07 minâ€–gâ€–=7.55e-14\n",
      "Gen   31  best=3.9612e-07 minâ€–gâ€–=4.41e-14\n",
      "Gen   32  best=3.9555e-07 minâ€–gâ€–=4.05e-14\n",
      "Gen   33  best=3.9555e-07 minâ€–gâ€–=8.32e-14\n",
      "Gen   34  best=3.9519e-07 minâ€–gâ€–=7.31e-14\n",
      "Gen   35  best=3.9515e-07 minâ€–gâ€–=2.48e-14\n",
      "Gen   36  best=3.9499e-07 minâ€–gâ€–=2.97e-14\n",
      "Gen   37  best=3.9464e-07 minâ€–gâ€–=2.64e-14\n",
      "Gen   38  best=3.9432e-07 minâ€–gâ€–=3.46e-14\n",
      "Gen   39  best=3.9421e-07 minâ€–gâ€–=2.38e-14\n",
      "Gen   40  best=3.9421e-07 minâ€–gâ€–=2.78e-14\n",
      "Gen   41  best=3.9408e-07 minâ€–gâ€–=3.06e-14\n",
      "Gen   42  best=3.9408e-07 minâ€–gâ€–=4.47e-14\n",
      "Gen   43  best=3.9356e-07 minâ€–gâ€–=4.92e-14\n",
      "Gen   44  best=3.9292e-07 minâ€–gâ€–=3.81e-14\n",
      "Gen   45  best=3.9292e-07 minâ€–gâ€–=3.76e-14\n",
      "Gen   46  best=3.9177e-07 minâ€–gâ€–=5.06e-14\n",
      "Gen   47  best=3.9140e-07 minâ€–gâ€–=1.57e-13\n",
      "Gen   48  best=3.9105e-07 minâ€–gâ€–=3.43e-14\n",
      "Gen   49  best=3.9105e-07 minâ€–gâ€–=3.35e-14\n",
      "Gen   50  best=3.9072e-07 minâ€–gâ€–=8.26e-14\n",
      "Gen   51  best=3.9060e-07 minâ€–gâ€–=4.34e-14\n",
      "Gen   52  best=3.9060e-07 minâ€–gâ€–=5.06e-14\n",
      "Gen   53  best=3.9044e-07 minâ€–gâ€–=7.87e-14\n",
      "Gen   54  best=3.9011e-07 minâ€–gâ€–=5.92e-14\n",
      "Gen   55  best=3.9005e-07 minâ€–gâ€–=2.73e-14\n",
      "Gen   56  best=3.8990e-07 minâ€–gâ€–=7.05e-14\n",
      "Gen   57  best=3.8967e-07 minâ€–gâ€–=5.18e-14\n",
      "Gen   58  best=3.8956e-07 minâ€–gâ€–=1.16e-13\n",
      "Gen   59  best=3.8951e-07 minâ€–gâ€–=6.39e-14\n",
      "Gen   60  best=3.8933e-07 minâ€–gâ€–=5.62e-14\n",
      "Gen   61  best=3.8896e-07 minâ€–gâ€–=1.05e-13\n",
      "Gen   62  best=3.8860e-07 minâ€–gâ€–=3.33e-14\n",
      "Gen   63  best=3.8856e-07 minâ€–gâ€–=8.48e-14\n",
      "Gen   64  best=3.8838e-07 minâ€–gâ€–=1.06e-13\n",
      "Gen   65  best=3.8802e-07 minâ€–gâ€–=9.89e-14\n",
      "Gen   66  best=3.8782e-07 minâ€–gâ€–=1.08e-13\n",
      "Gen   67  best=3.8775e-07 minâ€–gâ€–=8.21e-14\n",
      "Gen   68  best=3.8757e-07 minâ€–gâ€–=1.40e-13\n",
      "Gen   69  best=3.8729e-07 minâ€–gâ€–=5.13e-14\n",
      "Gen   70  best=3.8718e-07 minâ€–gâ€–=6.63e-14\n",
      "Gen   71  best=3.8705e-07 minâ€–gâ€–=1.86e-13\n",
      "Gen   72  best=3.8681e-07 minâ€–gâ€–=4.37e-14\n",
      "Gen   73  best=3.8646e-07 minâ€–gâ€–=1.40e-13\n",
      "Gen   74  best=3.8608e-07 minâ€–gâ€–=2.65e-13\n",
      "Gen   75  best=3.8566e-07 minâ€–gâ€–=1.15e-13\n",
      "Gen   76  best=3.8534e-07 minâ€–gâ€–=6.59e-14\n",
      "Gen   77  best=3.8507e-07 minâ€–gâ€–=3.21e-13\n",
      "Gen   78  best=3.8477e-07 minâ€–gâ€–=3.96e-13\n",
      "Gen   79  best=3.8423e-07 minâ€–gâ€–=1.27e-13\n",
      "Gen   80  best=3.8373e-07 minâ€–gâ€–=3.18e-13\n",
      "Gen   81  best=3.8354e-07 minâ€–gâ€–=4.09e-13\n",
      "Gen   82  best=3.8263e-07 minâ€–gâ€–=5.35e-14\n",
      "Gen   83  best=3.8243e-07 minâ€–gâ€–=1.37e-13\n",
      "Gen   84  best=3.8153e-07 minâ€–gâ€–=7.14e-14\n",
      "Gen   85  best=3.8080e-07 minâ€–gâ€–=7.09e-13\n",
      "Gen   86  best=3.8001e-07 minâ€–gâ€–=2.59e-13\n",
      "Gen   87  best=3.7933e-07 minâ€–gâ€–=1.57e-13\n",
      "Gen   88  best=3.7898e-07 minâ€–gâ€–=5.61e-14\n",
      "Gen   89  best=3.7818e-07 minâ€–gâ€–=1.99e-13\n",
      "Gen   90  best=3.7818e-07 minâ€–gâ€–=5.91e-14\n",
      "Gen   91  best=3.7772e-07 minâ€–gâ€–=2.49e-13\n",
      "Gen   92  best=3.7734e-07 minâ€–gâ€–=1.97e-13\n",
      "Gen   93  best=3.7691e-07 minâ€–gâ€–=6.02e-14\n",
      "Gen   94  best=3.7659e-07 minâ€–gâ€–=5.82e-14\n",
      "Gen   95  best=3.7616e-07 minâ€–gâ€–=7.14e-14\n",
      "Gen   96  best=3.7583e-07 minâ€–gâ€–=7.25e-14\n",
      "Gen   97  best=3.7537e-07 minâ€–gâ€–=1.07e-13\n",
      "Gen   98  best=3.7505e-07 minâ€–gâ€–=5.54e-14\n",
      "Gen   99  best=3.7423e-07 minâ€–gâ€–=1.13e-13\n",
      "Gen  100  best=3.7396e-07 minâ€–gâ€–=1.13e-13\n",
      "Gen  101  best=3.7291e-07 minâ€–gâ€–=1.05e-13\n",
      "Gen  102  best=3.7227e-07 minâ€–gâ€–=1.06e-13\n",
      "Gen  103  best=3.7144e-07 minâ€–gâ€–=7.41e-14\n",
      "Gen  104  best=3.7017e-07 minâ€–gâ€–=1.11e-13\n",
      "Gen  105  best=3.6958e-07 minâ€–gâ€–=6.65e-14\n",
      "Gen  106  best=3.6907e-07 minâ€–gâ€–=1.09e-13\n",
      "Gen  107  best=3.6802e-07 minâ€–gâ€–=1.09e-13\n",
      "Gen  108  best=3.6664e-07 minâ€–gâ€–=1.71e-13\n",
      "Gen  109  best=3.6474e-07 minâ€–gâ€–=1.21e-13\n",
      "Gen  110  best=3.6165e-07 minâ€–gâ€–=1.15e-13\n",
      "Gen  111  best=3.5931e-07 minâ€–gâ€–=6.50e-14\n",
      "Gen  112  best=3.5788e-07 minâ€–gâ€–=1.30e-13\n",
      "Gen  113  best=3.5663e-07 minâ€–gâ€–=3.86e-13\n",
      "Gen  114  best=3.5617e-07 minâ€–gâ€–=1.86e-13\n",
      "Gen  115  best=3.5431e-07 minâ€–gâ€–=2.32e-13\n",
      "Gen  116  best=3.5288e-07 minâ€–gâ€–=7.99e-14\n",
      "Gen  117  best=3.5199e-07 minâ€–gâ€–=4.20e-13\n",
      "Gen  118  best=3.4899e-07 minâ€–gâ€–=8.10e-14\n",
      "Gen  119  best=3.4872e-07 minâ€–gâ€–=9.83e-13\n",
      "Gen  120  best=3.4872e-07 minâ€–gâ€–=1.05e-13\n",
      "Gen  121  best=3.4792e-07 minâ€–gâ€–=8.58e-14\n",
      "Gen  122  best=3.4632e-07 minâ€–gâ€–=8.44e-14\n",
      "Gen  123  best=3.4587e-07 minâ€–gâ€–=4.06e-13\n",
      "Gen  124  best=3.4408e-07 minâ€–gâ€–=9.24e-14\n",
      "Gen  125  best=3.4359e-07 minâ€–gâ€–=2.16e-13\n",
      "Gen  126  best=3.4285e-07 minâ€–gâ€–=1.55e-12\n",
      "Gen  127  best=3.4128e-07 minâ€–gâ€–=9.35e-14\n",
      "Gen  128  best=3.3945e-07 minâ€–gâ€–=6.92e-13\n",
      "Gen  129  best=3.3832e-07 minâ€–gâ€–=8.25e-14\n",
      "Gen  130  best=3.3696e-07 minâ€–gâ€–=1.52e-13\n",
      "Gen  131  best=3.3589e-07 minâ€–gâ€–=8.98e-14\n",
      "Gen  132  best=3.3463e-07 minâ€–gâ€–=1.19e-13\n",
      "Gen  133  best=3.3340e-07 minâ€–gâ€–=9.49e-14\n",
      "Gen  134  best=3.3340e-07 minâ€–gâ€–=8.08e-14\n",
      "Gen  135  best=3.3253e-07 minâ€–gâ€–=9.91e-13\n",
      "Gen  136  best=3.3170e-07 minâ€–gâ€–=7.07e-14\n",
      "Gen  137  best=3.3078e-07 minâ€–gâ€–=7.71e-14\n",
      "Gen  138  best=3.2931e-07 minâ€–gâ€–=2.93e-13\n",
      "Gen  139  best=3.2826e-07 minâ€–gâ€–=5.73e-14\n",
      "Gen  140  best=3.2722e-07 minâ€–gâ€–=5.00e-13\n",
      "Gen  141  best=3.2616e-07 minâ€–gâ€–=9.25e-14\n",
      "Gen  142  best=3.2371e-07 minâ€–gâ€–=1.55e-13\n",
      "Gen  143  best=3.2299e-07 minâ€–gâ€–=7.21e-14\n",
      "Gen  144  best=3.2175e-07 minâ€–gâ€–=6.62e-14\n",
      "Gen  145  best=3.2036e-07 minâ€–gâ€–=3.02e-13\n",
      "Gen  146  best=3.1977e-07 minâ€–gâ€–=1.04e-13\n",
      "Gen  147  best=3.1947e-07 minâ€–gâ€–=6.29e-14\n",
      "Gen  148  best=3.1773e-07 minâ€–gâ€–=8.04e-14\n",
      "Gen  149  best=3.1645e-07 minâ€–gâ€–=4.13e-14\n",
      "Gen  150  best=3.1618e-07 minâ€–gâ€–=4.74e-14\n",
      "Gen  151  best=3.1457e-07 minâ€–gâ€–=1.27e-13\n",
      "Gen  152  best=3.1457e-07 minâ€–gâ€–=3.62e-14\n",
      "Gen  153  best=3.1438e-07 minâ€–gâ€–=3.13e-14\n",
      "Gen  154  best=3.1394e-07 minâ€–gâ€–=5.05e-14\n",
      "Gen  155  best=3.1371e-07 minâ€–gâ€–=8.36e-14\n",
      "Gen  156  best=3.1343e-07 minâ€–gâ€–=2.72e-14\n",
      "Gen  157  best=3.1323e-07 minâ€–gâ€–=5.04e-13\n",
      "Gen  158  best=3.1299e-07 minâ€–gâ€–=2.22e-14\n",
      "Gen  159  best=3.1273e-07 minâ€–gâ€–=2.04e-14\n",
      "Gen  160  best=3.1243e-07 minâ€–gâ€–=2.16e-13\n",
      "Gen  161  best=3.1226e-07 minâ€–gâ€–=1.40e-12\n",
      "Gen  162  best=3.1214e-07 minâ€–gâ€–=2.13e-14\n",
      "Gen  163  best=3.1202e-07 minâ€–gâ€–=4.38e-15\n",
      "Stopping: gradient norm below tol_g\n",
      "\n",
      "====================  DE RESULT  ====================\n",
      "best log-b : [-1.09740695  0.6458012   1.83620014  2.74226355  3.55098033  4.34788637\n",
      "  5.16886124  6.02996675  6.94194973  7.91618918  8.96773299 10.11847325\n",
      " 11.40286526 12.88117096 14.67866603 17.17097099]\n",
      "best b     : [3.33735356e-01 1.90751471e+00 6.27265772e+00 1.55220804e+01\n",
      " 3.48474629e+01 7.73148747e+01 1.75714627e+02 4.15701208e+02\n",
      " 1.03478580e+03 2.74130450e+03 7.84579496e+03 2.47968833e+04\n",
      " 8.95780205e+04 3.92845215e+05 2.37062936e+06 2.86587965e+07]\n",
      "best a     : [4.60134151e-01 2.56691914e-01 1.08780040e-01 5.89638242e-02\n",
      " 3.76712798e-02 2.57526477e-02 1.78313617e-02 1.22348546e-02\n",
      " 8.24998349e-03 5.44093067e-03 3.49264083e-03 2.16817240e-03\n",
      " 1.28936559e-03 7.23554572e-04 3.73258855e-04 1.69939434e-04]\n",
      "DE score   : 3.120215187484843e-07\n",
      "check obj  : 0.052671012106299916\n",
      "generations: 163\n"
     ]
    }
   ],
   "source": [
    "newton_steps = 20\n",
    "pop_size = 4\n",
    "rng_seed = 123\n",
    "n_terms = 16\n",
    "init_mean = np.linspace(min(best_b_log), max(best_b_log), n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 0.1)\n",
    "\n",
    "best_b_log, best_score, de_info = differential_evolution(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    F=0.8,\n",
    "    CR=0.9,\n",
    "    n_iters=50000,\n",
    "    random_state=rng_seed,\n",
    "    tol_g=1e-14,\n",
    "    stall_generations=750,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- final reconstruction of a and objective ----------------------------\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = obj(np.concatenate([best_a, best_b]))\n",
    "\n",
    "print(\"\\n====================  DE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"DE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"generations:\", de_info['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "46c7b368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2ff7dc6d50>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(best_b_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = grad(x)\n",
    "H = hess(x)\n",
    "d = np.linalg.solve(H, -g)  # Newton step\n",
    "dir_obj = lambda alpha: obj(x + alpha * d)\n",
    "alphas = np.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(alphas, [dir_obj(alpha) for alpha in alphas])\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Objective along Newton direction\")\n",
    "plt.title(\"Line search objective along Newton step\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae89cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 5.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "f_min_all = []\n",
    "min_b = -1.0\n",
    "max_b = 1.5\n",
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "for n_terms in range(2, 16):               # number of (a_i, b_i) pairs\n",
    "\n",
    "    b = np.linspace(min_b * 1.05, max_b * 1.1, n_terms)\n",
    "    a = optimal_a(d, w, target, np.exp(b))\n",
    "    # print(\"Optimal a:\", a)\n",
    "\n",
    "    means = np.concatenate((a, np.exp(b)))  # initial guess in log-space\n",
    "\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    x, grad_norm, f_history, grad_norm_history = newton(\n",
    "        f=obj,\n",
    "        grad=grad,\n",
    "        hess=hess,\n",
    "        x0=means,\n",
    "        tol_grad=1e-5,\n",
    "        tol_step=1e-8,\n",
    "        stall_iter=50,\n",
    "        max_iter=1000,\n",
    "        ls_bounds=(-2.0, 2.0)\n",
    "    )\n",
    "    min_b = min(np.log(x[n_terms:]))\n",
    "    max_b = max(np.log(x[n_terms:]))\n",
    "    print(f\"Min a log: {min_b}, Max b log: {max_b}\")\n",
    "    print(\"Final objective value:\", f_history[-1])\n",
    "    f_min_all.append(f_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f_min_all)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "\n",
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "best_init = np.array([0,0])\n",
    "\n",
    "for n_terms in range(1,10):                 # number of (a_i, b_i) pairs\n",
    "    means = best_init  # initial mean for (a_i, b_i)\n",
    "    cov = np.eye(2 * n_terms) * 4  # initial covariance matrix\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    best, stats = cross_entropy_numba(\n",
    "        d=d, target=target, w=w,\n",
    "        n_terms=n_terms,\n",
    "        max_iter=10000,\n",
    "        pop_size=10000,\n",
    "        elite_frac=0.1,\n",
    "        mean=means,\n",
    "        cov=cov\n",
    "    )\n",
    "\n",
    "    print(\"Best score :\", stats[\"best_score\"])\n",
    "    print(\"Best params:\", best)\n",
    "    print(\"Iterations :\", stats[\"iterations\"])\n",
    "    print(\"Runtime    :\", stats[\"runtime\"], \"s\")\n",
    "\n",
    "    first_half = best[:n_terms]\n",
    "    second_half = best[n_terms:]\n",
    "\n",
    "    # Original equidistant indices (0 to n_terms-1), new indices from 0 to n_terms\n",
    "    x_old = np.linspace(0, 1, n_terms)\n",
    "    x_new = np.linspace(0, 1, n_terms + 1)\n",
    "\n",
    "    interp_first = np.interp(x_new, x_old, first_half)\n",
    "    interp_second = np.interp(x_new, x_old, second_half)\n",
    "\n",
    "    best_init = np.hstack([interp_first, interp_second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d75dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(stats[\"history\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.title(\"Convergence of the Cross-Entropy Method\")\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best[n_terms:], 'o-')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"b_i (log scale)\")\n",
    "plt.title(\"Fitted b_i parameters\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f276bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import lsq_linear\n",
    "\n",
    "# compute using least squares the coefficients for approximation with squared exponential kernels\n",
    "# Construct the design matrix for squared exponential kernels\n",
    "A = np.zeros((len(d), n_terms))\n",
    "b_params = np.exp(best[n_terms:])\n",
    "\n",
    "for i in range(n_terms):\n",
    "    A[:, i] = np.exp(-b_params[i] * d**2)\n",
    "\n",
    "# Solve the weighted least squares problem: minimize ||W(Aa - target)||^2\n",
    "W = np.sqrt(w)\n",
    "Aw = A * W[:, None]\n",
    "tw = target * W\n",
    "\n",
    "res = lsq_linear(Aw, tw, bounds=(0, np.inf))\n",
    "squared_exponential_coeffs = res.x\n",
    "print(\"Squared exponential coefficients:\", np.log(squared_exponential_coeffs))\n",
    "\n",
    "# compute fit with this coefficients\n",
    "params = np.hstack([np.log(squared_exponential_coeffs), np.log(b_params)])\n",
    "\n",
    "res = objective_numba(np.array([params]), d, target, w)\n",
    "print(\"Objective value with least squares coefficients:\", res[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "f = lambda t: np.exp(-t)\n",
    "a, b, info = fit_exp_sum_ce(5, x, w, f, iterations=2000, pop_size=1000)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('best_score:', info.best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
