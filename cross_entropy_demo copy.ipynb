{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy kernel fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from typing import Callable, Tuple, Dict, Any, Optional\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from kl_decomposition import rectangle_rule, gauss_legendre_rule_multilevel\n",
    "\n",
    "# Enable 64-bit (double) precision in JAX\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8852ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(\n",
    "    user_fn: Callable[[np.ndarray], Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],\n",
    "    initial_mean: np.ndarray,\n",
    "    initial_std: np.ndarray,\n",
    "    pop_size: int = 100,\n",
    "    elite_frac: float = 0.2,\n",
    "    alpha_mean: float = 0.7,\n",
    "    alpha_std: float = 0.7,\n",
    "    n_iters: int = 100,\n",
    "    random_state: Optional[int] = None,\n",
    "    verbose: bool = False,\n",
    "    tol_std: float = 1e-6,\n",
    "    tol_g: float = 1e-6\n",
    ") -> Tuple[np.ndarray, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Method with independent Gaussian distribution and stopping criteria.\n",
    "\n",
    "    Args:\n",
    "        user_fn: function taking samples (shape Nxd) and returning\n",
    "            (updated_samples, scores, info_dict)\n",
    "        initial_mean: initial mean vector (d,)\n",
    "        initial_std: initial std deviation vector (d,)\n",
    "        pop_size: number of samples per iteration\n",
    "        elite_frac: fraction of samples selected as elites\n",
    "        alpha_mean: learning rate for mean update\n",
    "        alpha_std: learning rate for std update\n",
    "        n_iters: maximum number of iterations\n",
    "        random_state: RNG seed for reproducibility\n",
    "        verbose: print debug info each iteration\n",
    "        tol_std: stop if max(std) <= tol_std\n",
    "        tol_g: stop if info_dict['normg'] <= tol_g\n",
    "\n",
    "    Returns:\n",
    "        best_sample: best found sample (d,)\n",
    "        best_score: best objective value\n",
    "        info: dict with:\n",
    "            'iter': iteration index at stop\n",
    "            'best_score_history': list of best_score after each iteration\n",
    "            'max_std_history': list of max_std after each iteration\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    mean = np.array(initial_mean, dtype=float)\n",
    "    std = np.array(initial_std, dtype=float)\n",
    "    n_elite = max(1, int(np.ceil(pop_size * elite_frac)))\n",
    "    best_sample: Optional[np.ndarray] = None\n",
    "    best_score = np.inf\n",
    "\n",
    "    # history lists\n",
    "    best_score_history: list = []\n",
    "    max_std_history: list = []\n",
    "\n",
    "    for iteration in range(n_iters):\n",
    "        samples = rng.randn(pop_size, mean.size) * std + mean\n",
    "        samples, scores, info_dict = user_fn(samples)\n",
    "        idx = int(np.argmin(scores))\n",
    "        # update best if improved\n",
    "        if scores[idx] < best_score:\n",
    "            best_score = float(scores[idx])\n",
    "            best_sample = samples[idx]\n",
    "        # select elites and update distribution\n",
    "        elite = samples[np.argsort(scores)[:n_elite]]\n",
    "        elite_mean = elite.mean(axis=0)\n",
    "        elite_std = elite.std(axis=0, ddof=0)\n",
    "        mean = alpha_mean * elite_mean + (1 - alpha_mean) * mean\n",
    "        std = alpha_std * elite_std + (1 - alpha_std) * std\n",
    "        # record history\n",
    "        max_std = float(np.max(std))\n",
    "        best_score_history.append(best_score)\n",
    "        max_std_history.append(max_std)\n",
    "        # debug print\n",
    "        if verbose:\n",
    "            print(f\"Iter {iteration}: best_score={best_score}, max_std={max_std}, info={info_dict}\")\n",
    "        # stopping criteria\n",
    "        if max_std <= tol_std:\n",
    "            break\n",
    "        if info_dict.get('normg', np.inf) <= tol_g:\n",
    "            break\n",
    "\n",
    "    info = {\n",
    "        'iter': iteration,\n",
    "        'best_score_history': best_score_history,\n",
    "        'max_std_history': max_std_history\n",
    "    }\n",
    "    return best_sample, best_score, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84814de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-06-19 15:36:55,566:jax._src.xla_bridge:967: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "# setup squared exponential approximation\n",
    "# x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "x, w = gauss_legendre_rule_multilevel(0.0, 1.0, 100, L=5, ratio=0.2)\n",
    "\n",
    "target = np.exp(-x)\n",
    "\n",
    "# Convert to JAX arrays with float64 dtype\n",
    "x_j = jnp.array(x, dtype=jnp.float64)\n",
    "w_j = jnp.array(w, dtype=jnp.float64)\n",
    "target_j = jnp.array(target, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def obj_jax(p):\n",
    "    # p is already float64\n",
    "    n_param = p.shape[0] // 2\n",
    "    a = p[:n_param]\n",
    "    b = p[n_param:]\n",
    "    # compute prediction and weighted RMS error\n",
    "    pred = jnp.sum(a[:, None] * jnp.exp(-b[:, None] * x_j[None, :] ** 2), axis=0)\n",
    "    diff = pred - target_j\n",
    "    return jnp.sum(w_j * diff ** 2)\n",
    "\n",
    "\n",
    "# gradients and Hessians in double precision\n",
    "grad_jax = jax.jit(jax.grad(obj_jax))\n",
    "hess_jax = jax.jit(jax.hessian(obj_jax))\n",
    "obj_jax_jit = jax.jit(obj_jax)\n",
    "\n",
    "\n",
    "def obj(p):\n",
    "    # ensure NumPyâ†’JAX conversion to float64\n",
    "    return float(obj_jax_jit(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def grad(p):\n",
    "    return np.array(grad_jax(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def hess(p):\n",
    "    return np.array(hess_jax(jnp.array(p, dtype=jnp.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e6be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def golden_section_search(f, a=0.0, b=1.0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Golden-section search to find the minimum of a unimodal function f on [a, b].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        The objective function to minimize.\n",
    "    a : float\n",
    "        Left endpoint of the initial interval.\n",
    "    b : float\n",
    "        Right endpoint of the initial interval.\n",
    "    tol : float\n",
    "        Tolerance for the interval width (stopping criterion).\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_min : float\n",
    "        Estimated position of the minimum.\n",
    "    f_min : float\n",
    "        Value of f at x_min.\n",
    "    \"\"\"\n",
    "    # Golden ratio constant\n",
    "    phi = (np.sqrt(5.0) - 1) / 2\n",
    "\n",
    "    # Initialize interior points\n",
    "    c = b - phi * (b - a)\n",
    "    d = a + phi * (b - a)\n",
    "    f_c = f(c)\n",
    "    f_d = f(d)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        if abs(b - a) < tol:\n",
    "            break\n",
    "\n",
    "        if f_c < f_d:\n",
    "            b, d, f_d = d, c, f_c\n",
    "            c = b - phi * (b - a)\n",
    "            f_c = f(c)\n",
    "        else:\n",
    "            a, c, f_c = c, d, f_d\n",
    "            d = a + phi * (b - a)\n",
    "            f_d = f(d)\n",
    "\n",
    "    # Choose the best of the final points\n",
    "    if f_c < f_d:\n",
    "        x_min, f_min = c, f_c\n",
    "    else:\n",
    "        x_min, f_min = d, f_d\n",
    "    # print(f\"Iterations: {_ + 1}\")\n",
    "    return x_min, f_min\n",
    "\n",
    "\n",
    "def newton(f, grad, hess, x0,\n",
    "                                   tol_grad=1e-6,\n",
    "                                   tol_step=1e-8,\n",
    "                                   stall_iter=5,\n",
    "                                   max_iter=100,\n",
    "                                   ls_bounds=(-1.0, 1.0),\n",
    "                                   verbose=False):\n",
    "    \"\"\"\n",
    "    Newton's method with backtracking line search and fallback to gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Objective function f(x).\n",
    "    grad : callable\n",
    "        Gradient of f: grad(x).\n",
    "    hess : callable\n",
    "        Hessian of f: hess(x).\n",
    "    x0 : ndarray\n",
    "        Initial guess.\n",
    "    tol_grad : float, optional\n",
    "        Tolerance for gradient norm.\n",
    "    tol_step : float, optional\n",
    "        Tolerance for step size norm for stall criterion.\n",
    "    stall_iter : int, optional\n",
    "        Number of consecutive small steps to trigger stop.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations.\n",
    "    ls_bounds : (float, float), optional\n",
    "        Initial interval for line search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : ndarray\n",
    "        Estimated minimizer.\n",
    "    grad_norm : float\n",
    "        Norm of gradient at the solution.\n",
    "    f_history : list of float\n",
    "        History of f values.\n",
    "    grad_norm_history : list of float\n",
    "        History of gradient norms.\n",
    "    \"\"\"\n",
    "    x = x0.astype(float)\n",
    "    f_history = []\n",
    "    grad_norm_history = []\n",
    "    fx = f(x)\n",
    "\n",
    "    stall_count = 0\n",
    "    trust = 1e-6 * np.eye(len(x))  # trust region to ensure positive definiteness\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        gx = grad(x)\n",
    "        grad_norm = np.linalg.norm(gx)\n",
    "\n",
    "        f_history.append(fx)\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "        # Check gradient convergence\n",
    "        if grad_norm < tol_grad:\n",
    "            break\n",
    "\n",
    "        # Try Newton step\n",
    "        try:\n",
    "            Hx = hess(x)\n",
    "            p = -np.linalg.solve(Hx + trust, gx)\n",
    "            trust /= 2\n",
    "            # ensure descent direction\n",
    "            # if np.dot(p, gx) >= 0:\n",
    "            #     raise np.linalg.LinAlgError\n",
    "        except np.linalg.LinAlgError:\n",
    "            # fallback to steepest descent\n",
    "            print(\"Hessian not positive definite, using gradient descent\")\n",
    "            p = -gx\n",
    "            trust *= 2\n",
    "        # Backtracking line search\n",
    "        alpha, fx = golden_section_search(\n",
    "            lambda alpha: f(x + alpha * p),\n",
    "            a=ls_bounds[0], b=ls_bounds[1], tol=1e-12, max_iter=100\n",
    "        )\n",
    "\n",
    "        # Update\n",
    "        x_new = x + alpha * p\n",
    "        step_norm = np.linalg.norm(x_new - x)\n",
    "        x = x_new\n",
    "\n",
    "        # Stall criterion\n",
    "        if step_norm < tol_step:\n",
    "            stall_count += 1\n",
    "            if stall_count >= stall_iter:\n",
    "                break\n",
    "        else:\n",
    "            stall_count = 0\n",
    "\n",
    "        # print debug info\n",
    "        if verbose:\n",
    "            print(f\"Iter {k}: f={fx:.6e}, grad_norm={grad_norm:.6e}, step_norm={step_norm:.6e}, alpha={alpha:.6f}\")\n",
    "\n",
    "    return x, grad_norm, f_history, grad_norm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910c5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "\n",
    "def optimal_a(d: np.ndarray,\n",
    "              w: np.ndarray,\n",
    "              target: np.ndarray,\n",
    "              b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve for a >= 0 minimizing\n",
    "        sum_j w[j] * (sum_k a[k] * exp(-b[k] * d[j]**2) - target[j])**2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : (J,) array\n",
    "        Sample points x_j.\n",
    "    w : (J,) array\n",
    "        Quadrature weights.\n",
    "    target : (J,) array\n",
    "        Target values at each d[j].\n",
    "    b : (K,) array\n",
    "        Exponents b_k.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : (K,) array\n",
    "        Non-negative coefficient vector minimizing the weighted LS error.\n",
    "    \"\"\"\n",
    "    # Build design matrix Î¦[j,k] = exp(-b[k] * d[j]**2)\n",
    "    # Note: np.outer(d**2, b) yields shape (J, K)\n",
    "    Phi = np.exp(-np.outer(d**2, b))     # shape (J, K)\n",
    "\n",
    "    # Incorporate weights by scaling rows\n",
    "    W_sqrt = np.sqrt(w)                  # shape (J,)\n",
    "    Phi_w = Phi * W_sqrt[:, None]        # shape (J, K)\n",
    "    y_w = target * W_sqrt              # shape (J,)\n",
    "\n",
    "    a, _ = nnls(Phi_w, y_w)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270cb850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal a: [0.6979596  0.24840666]\n",
      "Optimised parameters: [-0.33719864 -1.41806631 -0.34875471  2.69154832]\n",
      "Final gradient norm: 4.758920864438474e-11\n",
      "Final objective value: 0.009765558517102698\n"
     ]
    }
   ],
   "source": [
    "# --- synthetic data ---------------------------------------------------\n",
    "n_terms = 2               # number of (a_i, b_i) pairs\n",
    "N       = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "\n",
    "b = np.linspace(-0.5, 2.5, n_terms)\n",
    "a = optimal_a(d, w, target, np.exp(b))\n",
    "print(\"Optimal a:\", a)\n",
    "\n",
    "means = np.concatenate((a, np.exp(b)))  # initial guess in log-space\n",
    "\n",
    "# --- optimiser call ---------------------------------------------------\n",
    "x, grad_norm, f_history, grad_norm_history = newton(\n",
    "    f=obj,\n",
    "    grad=grad,\n",
    "    hess=hess,\n",
    "    x0=means,\n",
    "    tol_grad=1e-9,\n",
    "    tol_step=1e-8,\n",
    "    stall_iter=50,\n",
    "    max_iter=3000,\n",
    "    ls_bounds=(-2.0, 2.0)\n",
    ")\n",
    "\n",
    "print(\"Optimised parameters:\", np.log(x))\n",
    "print(\"Final gradient norm:\", grad_norm)\n",
    "print(\"Final objective value:\", np.sqrt(f_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddd19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: best_score=0.0013950703793328882, max_std=1.1356440572972426, info={'normg': 0.13175763560330805}\n",
      "Iter 1: best_score=0.0009889552061308979, max_std=1.2447310414460087, info={'normg': 0.13790330909674978}\n",
      "Iter 2: best_score=0.0009889552061308979, max_std=1.3869532696309697, info={'normg': 0.07160743968188284}\n",
      "Iter 3: best_score=0.0009889552061308979, max_std=1.4030271446417653, info={'normg': 0.1294265030283102}\n",
      "Iter 4: best_score=0.0009889552061308979, max_std=1.3402764583545088, info={'normg': 0.08211164220393306}\n",
      "Iter 5: best_score=0.0009889552061308979, max_std=1.3223914089623938, info={'normg': 0.08433113742867276}\n",
      "Iter 6: best_score=0.0009889552061308979, max_std=1.2070336778372304, info={'normg': 0.08518818383235606}\n",
      "Iter 7: best_score=0.0009889552061308979, max_std=1.1772958148914703, info={'normg': 0.09000993917449461}\n",
      "Iter 8: best_score=0.0006586318020339291, max_std=1.024109070485738, info={'normg': 0.06882914264805107}\n",
      "Iter 9: best_score=0.0006586318020339291, max_std=1.0974909127010353, info={'normg': 0.0922613830345869}\n",
      "Iter 10: best_score=0.0006586318020339291, max_std=0.9518810190479945, info={'normg': 0.08999283289956882}\n",
      "Iter 11: best_score=0.0006586318020339291, max_std=0.8945710810052168, info={'normg': 0.10144263225661267}\n",
      "Iter 12: best_score=0.0006586318020339291, max_std=0.7288917997108284, info={'normg': 0.07536397248296436}\n",
      "Iter 13: best_score=0.0003649344987902314, max_std=0.8316380091520996, info={'normg': 0.08090243376291358}\n",
      "Iter 14: best_score=0.0003649344987902314, max_std=0.7590712849856411, info={'normg': 0.0663452028119802}\n",
      "Iter 15: best_score=0.0003649344987902314, max_std=0.6438844169211918, info={'normg': 0.02195853472836657}\n",
      "Iter 16: best_score=0.00027520291622068347, max_std=0.5772602881151687, info={'normg': 0.020240989776893377}\n",
      "Iter 17: best_score=0.00027520291622068347, max_std=0.5198112840635483, info={'normg': 0.055626152767178784}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     82\u001b[39m init_std = np.full(n_terms, \u001b[32m1.0\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m#  Run Cross-Entropy\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m best_b_log, best_score, ce_info = \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mce_user_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43melite_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_std\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1234\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_g\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_std\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# prints per-iteration progress\u001b[39;49;00m\n\u001b[32m    100\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m#  Report results\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m    105\u001b[39m best_b = np.exp(best_b_log)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(user_fn, initial_mean, initial_std, pop_size, elite_frac, alpha_mean, alpha_std, n_iters, random_state, verbose, tol_std, tol_g)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iters):\n\u001b[32m     53\u001b[39m     samples = rng.randn(pop_size, mean.size) * std + mean\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     samples, scores, info_dict = \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     idx = \u001b[38;5;28mint\u001b[39m(np.argmin(scores))\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# update best if improved\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mce_user_fn\u001b[39m\u001b[34m(b_log_samples)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# ---- (1) build starting point --------------------------------------\u001b[39;00m\n\u001b[32m     39\u001b[39m     b_pos = np.exp(b_log_samples[i])            \u001b[38;5;66;03m# (K,)  positive\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     a_opt = \u001b[43moptimal_a\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_pos\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# (K,)\u001b[39;00m\n\u001b[32m     41\u001b[39m     p0 = np.concatenate([a_opt, b_pos])      \u001b[38;5;66;03m# shape (2K,)\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# ---- (2) limited Newton polishing ----------------------------------\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36moptimal_a\u001b[39m\u001b[34m(d, w, target, b)\u001b[39m\n\u001b[32m     36\u001b[39m Phi_w = Phi * W_sqrt[:, \u001b[38;5;28;01mNone\u001b[39;00m]        \u001b[38;5;66;03m# shape (J, K)\u001b[39;00m\n\u001b[32m     37\u001b[39m y_w = target * W_sqrt              \u001b[38;5;66;03m# shape (J,)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m a, _ = \u001b[43mnnls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPhi_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/scipy/optimize/_nnls.py:93\u001b[39m, in \u001b[36mnnls\u001b[39m\u001b[34m(A, b, maxiter, atol)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m maxiter:\n\u001b[32m     92\u001b[39m     maxiter = \u001b[32m3\u001b[39m*n\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m x, rnorm, info = \u001b[43m_nnls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info == -\u001b[32m1\u001b[39m:\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMaximum number of iterations reached.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mscipy/optimize/_cython_nnls.pyx:216\u001b[39m, in \u001b[36mscipy.optimize._cython_nnls._nnls\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/numpy/linalg/_linalg.py:2618\u001b[39m, in \u001b[36m_norm_dispatcher\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2614\u001b[39m     result = op(svd(y, compute_uv=\u001b[38;5;28;01mFalse\u001b[39;00m), axis=-\u001b[32m1\u001b[39m, initial=initial)\n\u001b[32m   2615\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_norm_dispatcher\u001b[39m(x, \u001b[38;5;28mord\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, axis=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (x,)\n\u001b[32m   2622\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_norm_dispatcher)\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnorm\u001b[39m(x, \u001b[38;5;28mord\u001b[39m=\u001b[38;5;28;01mNone\u001b[39;00m, axis=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#  Cross-Entropy search over log-b with internal Newton polishing\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "\n",
    "# --- problem size -----------------------------------------------------------\n",
    "n_terms = 8          # K  (= length of b and a)\n",
    "newton_steps = 1     # how many Newton iterations per sample\n",
    "pop_size = 100       # CE population\n",
    "np.random.seed(0)    # reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  helper: CE user-defined update function operating on log-b only\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def ce_user_fn(b_log_samples: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b_log_samples : (N, K) ndarray\n",
    "        Current population in *log* space.\n",
    "    Returns\n",
    "    -------\n",
    "    updated_log_b : (N, K) ndarray\n",
    "        Polished & sorted log-b that CE will keep using.\n",
    "    scores        : (N,)  ndarray\n",
    "        Objective values after polishing (smaller is better).\n",
    "    info          : dict\n",
    "        Contains minimum gradient norm among all samples (for CE stopping).\n",
    "    \"\"\"\n",
    "    N, K = b_log_samples.shape\n",
    "    updated = np.empty_like(b_log_samples)\n",
    "    scores = np.empty(N)\n",
    "    grad_norm = np.empty(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        # ---- (1) build starting point --------------------------------------\n",
    "        b_pos = np.exp(b_log_samples[i])            # (K,)  positive\n",
    "        a_opt = optimal_a(d, w, target, b_pos)      # (K,)\n",
    "        p0 = np.concatenate([a_opt, b_pos])      # shape (2K,)\n",
    "\n",
    "        # ---- (2) limited Newton polishing ----------------------------------\n",
    "        p_new, gnorm, *_ = newton(\n",
    "            f=obj,\n",
    "            grad=grad,\n",
    "            hess=hess,\n",
    "            x0=p0,\n",
    "            max_iter=newton_steps,\n",
    "            tol_grad=1e-6,\n",
    "            tol_step=1e-6,\n",
    "            stall_iter=newton_steps,\n",
    "            ls_bounds=(-2., 2.),\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        if not np.any(np.isnan(p_new)) and not np.any(np.isinf(p_new)) and not np.any(p_new < 0):\n",
    "            # ---- (3) extract & sort b (log) ------------------------------------\n",
    "            b_new_pos = np.clip(p_new[K:], 1e-12, None)  # keep positivity\n",
    "            b_new_log = np.log(b_new_pos)\n",
    "            order = np.argsort(b_new_log)\n",
    "            a_opt = optimal_a(d, w, target, b_new_pos)      # (K,)\n",
    "            p0 = np.concatenate([a_opt, b_new_pos])      # shape (2K,)\n",
    "\n",
    "            updated[i] = b_new_log[order]\n",
    "            scores[i] = obj(p0)       # score *after* polishing\n",
    "            grad_norm[i] = np.linalg.norm(grad(p0))\n",
    "        else:\n",
    "            # if polishing failed, keep the original sample\n",
    "            updated[i] = b_log_samples[i]\n",
    "            scores[i] = np.inf\n",
    "            grad_norm[i] = np.inf\n",
    "\n",
    "    info = {'normg': float(grad_norm.min())}\n",
    "    return updated, scores, info\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  CE initial distribution over log-b\n",
    "# --------------------------------------------------------------------------- #\n",
    "init_mean = np.linspace(-1.5, 9.3, n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 1.0)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Run Cross-Entropy\n",
    "# --------------------------------------------------------------------------- #\n",
    "best_b_log, best_score, ce_info = cross_entropy(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    elite_frac=0.2,\n",
    "    alpha_mean=0.5,\n",
    "    alpha_std=0.5,\n",
    "    n_iters=2000,\n",
    "    random_state=1234,\n",
    "    tol_g=1e-6,\n",
    "    tol_std=1e-6,\n",
    "    verbose=True      # prints per-iteration progress\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Report results\n",
    "# --------------------------------------------------------------------------- #\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = obj(np.concatenate([best_a, best_b]))\n",
    "\n",
    "print(\"\\n====================  CE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"CE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"stopped at iteration\", ce_info['iter'],\n",
    "      \"| max Ïƒ =\", ce_info['max_std_history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "830c3d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.48353019,  0.04141208,  1.34315267,  2.62557281,  4.04747186,\n",
       "        5.72257207,  7.78431983, 10.25991933])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_b_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f43c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Differential-Evolution optimiser that works with the same `user_fn`\n",
    "###############################################################################\n",
    "from typing import Callable, Tuple, Dict, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def differential_evolution(\n",
    "    user_fn: Callable[[np.ndarray],\n",
    "                      Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],\n",
    "    initial_mean: np.ndarray,\n",
    "    initial_std: np.ndarray,\n",
    "    pop_size: int = 100,\n",
    "    F: float = 0.8,                 # differential weight\n",
    "    CR: float = 0.9,                # crossover probability\n",
    "    n_iters: int = 500,\n",
    "    random_state: Optional[int] = None,\n",
    "    tol_g: float = 1e-6,            # stop if minâ€–gâ€– drops below this\n",
    "    stall_generations: int = 50,    # stop if no improvement for so many gens\n",
    "    verbose: bool = False\n",
    ") -> Tuple[np.ndarray, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Differential Evolution (DE/rand/1/bin) with the CE-style `user_fn`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_sample : ndarray\n",
    "        The best parameter vector found (d,).\n",
    "    best_score  : float\n",
    "        Objective value of `best_sample`.\n",
    "    info        : dict\n",
    "        Diagnostics (`iter`, `best_score_history`).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    d = initial_mean.size\n",
    "\n",
    "    # --- initial population --------------------------------------------------\n",
    "    pop = rng.randn(pop_size, d) * initial_std + initial_mean       # (N, d)\n",
    "    pop, scores, info = user_fn(pop)                               # polish & score\n",
    "    best_idx = int(np.argmin(scores))\n",
    "    best_sample = pop[best_idx].copy()\n",
    "    best_score = float(scores[best_idx])\n",
    "\n",
    "    best_score_history = [best_score]\n",
    "    no_improve = 0\n",
    "\n",
    "    for it in range(1, n_iters + 1):\n",
    "        # ------------ mutation & crossover ----------------------------------\n",
    "        trial = np.empty_like(pop)\n",
    "        for i in range(pop_size):\n",
    "            # choose three *distinct* indices â‰  i\n",
    "            r1, r2, r3 = rng.choice([j for j in range(pop_size) if j != i],\n",
    "                                    size=3, replace=False)\n",
    "            mutant = pop[r1] + F * (pop[r2] - pop[r3])\n",
    "\n",
    "            # binomial crossover\n",
    "            cross_pts = rng.rand(d) < CR\n",
    "            cross_pts[rng.randint(0, d)] = True      # ensure at least one gene\n",
    "            trial[i] = np.where(cross_pts, mutant, pop[i])\n",
    "\n",
    "        # ------------ evaluate trial population ------------------------------\n",
    "        trial, trial_scores, info_trial = user_fn(trial)\n",
    "\n",
    "        # ------------ selection ----------------------------------------------\n",
    "        improved = trial_scores < scores\n",
    "        pop[improved] = trial[improved]\n",
    "        scores[improved] = trial_scores[improved]\n",
    "\n",
    "        # ------------ keep track of best -------------------------------------\n",
    "        gen_best_idx = int(np.argmin(scores))\n",
    "        gen_best_score = float(scores[gen_best_idx])\n",
    "        if gen_best_score + 1e-12 < best_score:      # strict improvement\n",
    "            best_score = gen_best_score\n",
    "            best_sample = pop[gen_best_idx].copy()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        best_score_history.append(best_score)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Gen {it:4d}  best={best_score:.4e} \"\n",
    "                  f\"minâ€–gâ€–={info_trial.get('normg', np.inf):.2e}\")\n",
    "\n",
    "        # ------------ stopping criteria --------------------------------------\n",
    "        if info_trial.get('normg', np.inf) <= tol_g:\n",
    "            if verbose:\n",
    "                print(\"Stopping: gradient norm below tol_g\")\n",
    "            break\n",
    "        if no_improve >= stall_generations:\n",
    "            if verbose:\n",
    "                print(\"Stopping: no improvement for\",\n",
    "                      stall_generations, \"generations\")\n",
    "            break\n",
    "\n",
    "    info = {\n",
    "        'iter': it,\n",
    "        'best_score_history': best_score_history\n",
    "    }\n",
    "    return best_sample, best_score, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ddd9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.981586e-13, alpha=0.000000\n",
      "Iter 1: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.981431e-13, alpha=0.000000\n",
      "Iter 2: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980968e-13, alpha=0.000000\n",
      "Iter 3: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 4: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 5: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 6: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 7: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 8: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 9: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 10: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 11: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 12: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 13: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 14: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 15: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 16: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 17: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 18: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 19: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 20: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 21: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 22: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 23: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 24: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 25: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 26: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 27: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 28: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 29: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 30: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 31: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 32: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 33: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 34: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 35: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 36: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 37: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 38: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 39: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 40: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 41: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 42: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 43: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 44: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 45: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 46: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 47: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 48: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 49: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 50: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 51: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 52: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 53: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 54: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 55: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 56: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 57: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 58: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 59: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 60: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 61: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 62: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 63: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 64: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 65: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 66: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 67: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 68: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 69: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 70: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 71: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 72: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 73: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 74: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 75: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 76: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 77: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 78: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 79: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 80: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 81: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 82: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 83: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 84: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 85: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 86: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 87: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 88: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 89: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 90: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 91: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 92: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 93: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 94: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 95: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 96: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 97: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "Iter 98: f=2.178304e-03, grad_norm=8.489888e-04, step_norm=1.980912e-13, alpha=0.000000\n",
      "[0.33317714 0.53021171 1.6022921  0.82615267]\n",
      "0.000848988804093238\n",
      "0.0008489888040933464\n",
      "Iter 0: f=2.783927e-03, grad_norm=3.156757e-02, step_norm=9.264849e-01, alpha=0.430773\n",
      "Iter 1: f=2.375601e-03, grad_norm=3.342312e-02, step_norm=1.454079e+00, alpha=0.984307\n",
      "Iter 2: f=2.375304e-03, grad_norm=2.094761e-03, step_norm=8.678825e-01, alpha=1.000000\n",
      "Iter 3: f=2.375265e-03, grad_norm=3.657921e-04, step_norm=2.686583e-01, alpha=1.000000\n",
      "Iter 4: f=2.375260e-03, grad_norm=8.101121e-05, step_norm=1.156072e-01, alpha=1.000000\n",
      "Iter 5: f=2.375259e-03, grad_norm=1.926759e-05, step_norm=5.428100e-02, alpha=1.000000\n",
      "Iter 6: f=2.375259e-03, grad_norm=4.686605e-06, step_norm=2.635987e-02, alpha=1.000000\n",
      "Iter 7: f=2.375259e-03, grad_norm=1.150936e-06, step_norm=1.299653e-02, alpha=1.000000\n",
      "Iter 8: f=2.375259e-03, grad_norm=2.838785e-07, step_norm=6.454036e-03, alpha=1.000000\n",
      "Iter 9: f=2.375259e-03, grad_norm=7.016089e-08, step_norm=3.216197e-03, alpha=0.999990\n",
      "Iter 10: f=2.375259e-03, grad_norm=1.735748e-08, step_norm=1.605468e-03, alpha=0.999992\n",
      "Iter 11: f=2.375259e-03, grad_norm=4.296225e-09, step_norm=8.017378e-04, alpha=0.999554\n",
      "Iter 12: f=2.375259e-03, grad_norm=1.064605e-09, step_norm=4.009567e-04, alpha=0.999728\n",
      "[-4.29049018e-07  8.58872574e-01  1.06849177e+00  1.06809070e+00]\n",
      "6.049230193883225e-07\n",
      "2.637564945333775e-10\n",
      "Iter 0: f=1.904628e-04, grad_norm=1.302523e-04, step_norm=1.778832e+00, alpha=0.614442\n",
      "Iter 1: f=1.361070e-04, grad_norm=6.070348e-03, step_norm=1.506543e+00, alpha=1.000000\n",
      "Iter 2: f=1.089493e-04, grad_norm=1.879127e-03, step_norm=2.309353e+00, alpha=1.000000\n",
      "Iter 3: f=9.752586e-05, grad_norm=3.262431e-03, step_norm=1.271651e+00, alpha=1.000000\n",
      "Iter 4: f=9.547980e-05, grad_norm=5.028674e-04, step_norm=1.037843e+00, alpha=1.000000\n",
      "Iter 5: f=9.536672e-05, grad_norm=2.985382e-04, step_norm=2.339362e-01, alpha=1.000000\n",
      "Iter 6: f=9.536613e-05, grad_norm=1.213095e-05, step_norm=2.150935e-02, alpha=1.000000\n",
      "Iter 7: f=9.536613e-05, grad_norm=1.039932e-07, step_norm=2.390515e-04, alpha=0.999965\n",
      "[ 0.71376704  0.24218187  0.70556617 14.75450223]\n",
      "2.1552461585046485e-12\n",
      "1.636698992490942e-11\n",
      "Iter 0: f=2.393408e-03, grad_norm=1.411431e-02, step_norm=4.459713e-01, alpha=0.776961\n",
      "Iter 1: f=2.375291e-03, grad_norm=7.725279e-03, step_norm=4.866355e-01, alpha=0.996587\n",
      "Iter 2: f=2.375267e-03, grad_norm=4.591959e-04, step_norm=3.073999e-01, alpha=1.000000\n",
      "Iter 3: f=2.375261e-03, grad_norm=1.031828e-04, step_norm=1.299713e-01, alpha=1.000000\n",
      "Iter 4: f=2.375259e-03, grad_norm=2.388795e-05, step_norm=6.037410e-02, alpha=1.000000\n",
      "Iter 5: f=2.375259e-03, grad_norm=5.734782e-06, step_norm=2.920527e-02, alpha=1.000000\n",
      "Iter 6: f=2.375259e-03, grad_norm=1.395059e-06, step_norm=1.437646e-02, alpha=1.000000\n",
      "Iter 7: f=2.375259e-03, grad_norm=3.412611e-07, step_norm=7.134441e-03, alpha=1.000000\n",
      "Iter 8: f=2.375259e-03, grad_norm=8.369093e-08, step_norm=3.554250e-03, alpha=0.999999\n",
      "Iter 9: f=2.375259e-03, grad_norm=2.055000e-08, step_norm=1.773967e-03, alpha=0.999991\n",
      "Iter 10: f=2.375259e-03, grad_norm=5.049674e-09, step_norm=8.861405e-04, alpha=0.999893\n",
      "Iter 11: f=2.375259e-03, grad_norm=1.241727e-09, step_norm=4.421288e-04, alpha=0.998081\n",
      "[-7.42569581e-07  8.58872888e-01  1.06853441e+00  1.06809070e+00]\n",
      "6.692335561806564e-07\n",
      "3.0662670413174714e-10\n",
      "[8.48988804e-04 6.04923019e-07 2.15524616e-12 6.69233556e-07]\n",
      "{'normg': 2.1552461585046485e-12}\n",
      "Iter 0: f=9.536613e-05, grad_norm=1.266508e-07, step_norm=3.258742e-04, alpha=0.999999\n",
      "[ 0.71376791  0.24218132  0.7055681  14.75464739]\n",
      "3.258042383797949e-10\n",
      "3.2616274989764187e-10\n",
      "Iter 0: f=2.214724e-04, grad_norm=4.235241e-03, step_norm=4.301314e+00, alpha=0.223542\n",
      "Iter 1: f=1.972286e-04, grad_norm=1.341320e-02, step_norm=1.904379e+00, alpha=0.323480\n",
      "Iter 2: f=1.336242e-04, grad_norm=6.468236e-03, step_norm=2.748773e+00, alpha=0.221936\n",
      "Iter 3: f=9.955911e-05, grad_norm=2.160289e-03, step_norm=1.547651e+00, alpha=1.000000\n",
      "Iter 4: f=9.570684e-05, grad_norm=7.382518e-04, step_norm=1.343204e+00, alpha=1.000000\n",
      "Iter 5: f=9.537009e-05, grad_norm=5.561466e-04, step_norm=3.624252e-01, alpha=1.000000\n",
      "Iter 6: f=9.536613e-05, grad_norm=2.928746e-05, step_norm=5.578282e-02, alpha=1.000000\n",
      "Iter 7: f=9.536613e-05, grad_norm=7.064873e-07, step_norm=9.698104e-04, alpha=0.999934\n",
      "[ 0.71376702  0.24218188  0.70556613 14.75449943]\n",
      "1.6441329315545372e-11\n",
      "2.53157047073725e-10\n",
      "Iter 0: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.452143e-13, alpha=0.000000\n",
      "Iter 1: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450170e-13, alpha=0.000000\n",
      "Iter 2: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450170e-13, alpha=0.000000\n",
      "Iter 3: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450170e-13, alpha=0.000000\n",
      "Iter 4: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 5: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 6: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 7: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 8: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 9: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 10: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 11: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 12: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 13: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 14: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 15: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 16: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 17: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 18: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 19: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 20: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 21: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 22: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 23: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 24: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 25: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 26: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 27: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 28: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 29: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 30: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 31: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 32: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 33: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 34: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 35: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 36: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 37: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 38: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 39: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 40: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 41: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 42: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 43: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 44: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 45: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 46: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 47: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 48: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 49: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 50: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 51: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 52: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 53: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 54: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 55: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 56: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 57: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 58: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 59: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 60: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 61: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 62: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 63: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 64: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 65: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 66: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 67: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 68: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 69: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 70: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 71: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 72: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 73: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 74: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 75: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 76: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 77: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 78: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 79: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 80: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 81: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 82: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 83: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 84: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 85: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 86: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 87: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 88: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 89: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 90: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 91: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 92: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 93: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 94: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 95: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 96: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 97: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "Iter 98: f=2.249982e-03, grad_norm=6.765825e-04, step_norm=1.450114e-13, alpha=0.000000\n",
      "[0.52804611 0.33384099 0.86970081 1.47752121]\n",
      "0.0006765825312033262\n",
      "0.0006765825312033116\n",
      "Iter 0: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.814803e-13, alpha=0.000000\n",
      "Iter 1: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.814073e-13, alpha=0.000000\n",
      "Iter 2: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813805e-13, alpha=0.000000\n",
      "Iter 3: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813809e-13, alpha=0.000000\n",
      "Iter 4: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813533e-13, alpha=0.000000\n",
      "Iter 5: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813533e-13, alpha=0.000000\n",
      "Iter 6: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813533e-13, alpha=0.000000\n",
      "Iter 7: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 8: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 9: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 10: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 11: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 12: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 13: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 14: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 15: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 16: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 17: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 18: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 19: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 20: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 21: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 22: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 23: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 24: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 25: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 26: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 27: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 28: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 29: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 30: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 31: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 32: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 33: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 34: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 35: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 36: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 37: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 38: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 39: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 40: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 41: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 42: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 43: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 44: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 45: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 46: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 47: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 48: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 49: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 50: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 51: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 52: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 53: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 54: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 55: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 56: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 57: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 58: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 59: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 60: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 61: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 62: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 63: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 64: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 65: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 66: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 67: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 68: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 69: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 70: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 71: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 72: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 73: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 74: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 75: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 76: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 77: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 78: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 79: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 80: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 81: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 82: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 83: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 84: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 85: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 86: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 87: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 88: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 89: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 90: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 91: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 92: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 93: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 94: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 95: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 96: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 97: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "Iter 98: f=2.245957e-03, grad_norm=1.279372e-03, step_norm=2.813537e-13, alpha=0.000000\n",
      "[0.78398848 0.07236654 1.15110944 0.19616385]\n",
      "0.00127937213618278\n",
      "0.0012793721361832914\n",
      "[3.25804238e-10 1.64413293e-11 6.76582531e-04 1.27937214e-03]\n",
      "{'normg': 1.6441329315545372e-11}\n",
      "Gen    1  best=9.7656e-03 minâ€–gâ€–=1.64e-11\n",
      "Stopping: gradient norm below tol_g\n",
      "\n",
      "====================  DE RESULT  ====================\n",
      "best log-b : [-0.34875472  2.69154827]\n",
      "best b     : [ 0.70556617 14.75450223]\n",
      "best a     : [0.71376704 0.24218187]\n",
      "DE score   : 0.009765558517102748\n",
      "check obj  : 0.009765558517102748\n",
      "generations: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "#  TEST SCRIPT  â€”  optimise log-b with the same `ce_user_fn`\n",
    "###############################################################################\n",
    "\n",
    "# --- problem & helper already defined earlier ---------------------------\n",
    "# n_terms, ce_user_fn, d, w, target, obj â€¦ are assumed to be in scope\n",
    "\n",
    "pop_size = 4\n",
    "rng_seed = 1234\n",
    "\n",
    "# --- problem size -----------------------------------------------------------\n",
    "n_terms = 2          # K  (= length of b and a)\n",
    "newton_steps = 100     # how many Newton iterations per sample\n",
    "np.random.seed(1)    # reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  helper: CE user-defined update function operating on log-b only\n",
    "# --------------------------------------------------------------------------- #\n",
    "d, w = gauss_legendre_rule_multilevel(0.0, 1.0, 100, L=5, ratio=0.2)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "\n",
    "def ce_user_fn(b_log_samples: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b_log_samples : (N, K) ndarray\n",
    "        Current population in *log* space.\n",
    "    Returns\n",
    "    -------\n",
    "    updated_log_b : (N, K) ndarray\n",
    "        Polished & sorted log-b that CE will keep using.\n",
    "    scores        : (N,)  ndarray\n",
    "        Objective values after polishing (smaller is better).\n",
    "    info          : dict\n",
    "        Contains minimum gradient norm among all samples (for CE stopping).\n",
    "    \"\"\"\n",
    "    N, K = b_log_samples.shape\n",
    "    updated = np.empty_like(b_log_samples)\n",
    "    scores = np.empty(N)\n",
    "    grad_norm = np.empty(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        # ---- (1) build starting point --------------------------------------\n",
    "        b_pos = np.exp(b_log_samples[i])            # (K,)  positive\n",
    "        a_opt = optimal_a(d, w, target, b_pos)      # (K,)\n",
    "        p0 = np.concatenate([a_opt, b_pos])      # shape (2K,)\n",
    "\n",
    "        # ---- (2) limited Newton polishing ----------------------------------\n",
    "        p_new, gnorm, *_ = newton(\n",
    "            f=obj,\n",
    "            grad=grad,\n",
    "            hess=hess,\n",
    "            x0=p0,\n",
    "            max_iter=newton_steps,\n",
    "            tol_grad=1e-9,\n",
    "            tol_step=1e-9,\n",
    "            stall_iter=newton_steps,\n",
    "            ls_bounds=(0., 1.),\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        if not np.any(np.isnan(p_new)) and not np.any(np.isinf(p_new)):\n",
    "            # ---- (3) extract & sort b (log) ------------------------------------\n",
    "            b_new_pos = np.clip(p_new[K:], 1e-12, None)  # keep positivity\n",
    "            b_new_log = np.log(b_new_pos)\n",
    "            order = np.argsort(b_new_log)\n",
    "            a_opt = optimal_a(d, w, target, b_new_pos)      # (K,)\n",
    "            p0 = np.concatenate([a_opt, b_new_pos])      # shape (2K,)\n",
    "\n",
    "            updated[i] = b_new_log[order]\n",
    "            scores[i] = np.sqrt(obj(p0))       # score *after* polishing\n",
    "            grad_norm[i] = np.linalg.norm(grad(p0))\n",
    "            print(p_new)\n",
    "            print(np.linalg.norm(grad(p0)))\n",
    "            print(gnorm)\n",
    "        else:\n",
    "            # if polishing failed, keep the original sample\n",
    "            updated[i] = b_log_samples[i]\n",
    "            scores[i] = np.inf\n",
    "            grad_norm[i] = np.inf\n",
    "\n",
    "    min_idx = np.argmin(scores)\n",
    "    info = {'normg': float(grad_norm[min_idx])}\n",
    "    print(grad_norm)\n",
    "    print(info)\n",
    "    return updated, scores, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init_mean = np.linspace(0,  1, n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 1.0)\n",
    "\n",
    "best_b_log, best_score, de_info = differential_evolution(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    F=0.8,\n",
    "    CR=0.9,\n",
    "    n_iters=10,\n",
    "    random_state=rng_seed,\n",
    "    tol_g=1e-9,\n",
    "    stall_generations=75,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- final reconstruction of a and objective ----------------------------\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = np.sqrt(obj(np.concatenate([best_a, best_b])))\n",
    "\n",
    "print(\"\\n====================  DE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"DE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"generations:\", de_info['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e3be2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen    1  best=3.5277e-02 minâ€–gâ€–=2.77e-02\n",
      "Gen    2  best=2.1982e-02 minâ€–gâ€–=2.44e-02\n",
      "Gen    3  best=2.0824e-02 minâ€–gâ€–=2.30e-02\n",
      "Gen    4  best=2.0202e-02 minâ€–gâ€–=2.23e-02\n",
      "Gen    5  best=1.5547e-02 minâ€–gâ€–=6.33e-03\n",
      "Gen    6  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen    7  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen    8  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen    9  best=1.5547e-02 minâ€–gâ€–=2.09e-02\n",
      "Gen   10  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   11  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   12  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   13  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   14  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   15  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   16  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen   17  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   18  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   19  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   20  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   21  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen   22  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   23  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   24  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   25  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen   26  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   27  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   28  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   29  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   30  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   31  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   32  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   33  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   34  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   35  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   36  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   37  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   38  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   39  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   40  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   41  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen   42  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   43  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   44  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen   45  best=1.5547e-02 minâ€–gâ€–=2.07e-02\n",
      "Gen   46  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen   47  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   48  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen   49  best=1.5547e-02 minâ€–gâ€–=2.23e-02\n",
      "Gen   50  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   51  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   52  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   53  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen   54  best=1.5547e-02 minâ€–gâ€–=2.07e-02\n",
      "Gen   55  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen   56  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen   57  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   58  best=1.5547e-02 minâ€–gâ€–=2.03e-02\n",
      "Gen   59  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen   60  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   61  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen   62  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   63  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   64  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen   65  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen   66  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   67  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   68  best=1.5547e-02 minâ€–gâ€–=2.08e-02\n",
      "Gen   69  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   70  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   71  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   72  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen   73  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   74  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen   75  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   76  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen   77  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   78  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   79  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   80  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   81  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   82  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   83  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen   84  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   85  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   86  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   87  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   88  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   89  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   90  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   91  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   92  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen   93  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen   94  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen   95  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen   96  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen   97  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen   98  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen   99  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  100  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  101  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  102  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  103  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  104  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  105  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  106  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  107  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  108  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  109  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  110  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  111  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  112  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  113  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  114  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  115  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  116  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  117  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  118  best=1.5547e-02 minâ€–gâ€–=2.02e-02\n",
      "Gen  119  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  120  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  121  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  122  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  123  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  124  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  125  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  126  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  127  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  128  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  129  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  130  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  131  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  132  best=1.5547e-02 minâ€–gâ€–=2.10e-02\n",
      "Gen  133  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  134  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  135  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  136  best=1.5547e-02 minâ€–gâ€–=2.08e-02\n",
      "Gen  137  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  138  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  139  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  140  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  141  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  142  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  143  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  144  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  145  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  146  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  147  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  148  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  149  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  150  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  151  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  152  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  153  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  154  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen  155  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  156  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  157  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  158  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  159  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  160  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  161  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  162  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  163  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  164  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  165  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  166  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  167  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  168  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  169  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  170  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  171  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  172  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  173  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  174  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  175  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  176  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  177  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  178  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  179  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  180  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  181  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  182  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  183  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  184  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  185  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  186  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  187  best=1.5547e-02 minâ€–gâ€–=2.09e-02\n",
      "Gen  188  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  189  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  190  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  191  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  192  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  193  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  194  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  195  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  196  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  197  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  198  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  199  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  200  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  201  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  202  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  203  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  204  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  205  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  206  best=1.5547e-02 minâ€–gâ€–=2.07e-02\n",
      "Gen  207  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  208  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  209  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  210  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  211  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  212  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  213  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  214  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  215  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  216  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  217  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  218  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  219  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  220  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  221  best=1.5547e-02 minâ€–gâ€–=2.06e-02\n",
      "Gen  222  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  223  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  224  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  225  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  226  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  227  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  228  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  229  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  230  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  231  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  232  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen  233  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  234  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  235  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  236  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  237  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  238  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  239  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  240  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  241  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  242  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  243  best=1.5547e-02 minâ€–gâ€–=2.10e-02\n",
      "Gen  244  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  245  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  246  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  247  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  248  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  249  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  250  best=1.5547e-02 minâ€–gâ€–=2.10e-02\n",
      "Gen  251  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  252  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  253  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  254  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  255  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  256  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  257  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  258  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  259  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  260  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  261  best=1.5547e-02 minâ€–gâ€–=2.08e-02\n",
      "Gen  262  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  263  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  264  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  265  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  266  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  267  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  268  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  269  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  270  best=1.5547e-02 minâ€–gâ€–=2.10e-02\n",
      "Gen  271  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  272  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  273  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  274  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  275  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  276  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  277  best=1.5547e-02 minâ€–gâ€–=2.23e-02\n",
      "Gen  278  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  279  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  280  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  281  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  282  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  283  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  284  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  285  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  286  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  287  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  288  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  289  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  290  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  291  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  292  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  293  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  294  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  295  best=1.5547e-02 minâ€–gâ€–=2.10e-02\n",
      "Gen  296  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  297  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  298  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  299  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  300  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  301  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  302  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  303  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  304  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  305  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  306  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  307  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  308  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  309  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  310  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  311  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  312  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  313  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  314  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  315  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  316  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  317  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  318  best=1.5547e-02 minâ€–gâ€–=2.08e-02\n",
      "Gen  319  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  320  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  321  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  322  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  323  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  324  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  325  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  326  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  327  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  328  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  329  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  330  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  331  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  332  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  333  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  334  best=1.5547e-02 minâ€–gâ€–=2.10e-02\n",
      "Gen  335  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  336  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  337  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  338  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  339  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  340  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  341  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  342  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  343  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen  344  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  345  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  346  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  347  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  348  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  349  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  350  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  351  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  352  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  353  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  354  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  355  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  356  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  357  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  358  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  359  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  360  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  361  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  362  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  363  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  364  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  365  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  366  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  367  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  368  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  369  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  370  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  371  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  372  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  373  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  374  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  375  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  376  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  377  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  378  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  379  best=1.5547e-02 minâ€–gâ€–=2.18e-02\n",
      "Gen  380  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  381  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen  382  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  383  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  384  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  385  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  386  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  387  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  388  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  389  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen  390  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  391  best=1.5547e-02 minâ€–gâ€–=2.17e-02\n",
      "Gen  392  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  393  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  394  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  395  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  396  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  397  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  398  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  399  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  400  best=1.5547e-02 minâ€–gâ€–=2.12e-02\n",
      "Gen  401  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  402  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  403  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  404  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen  405  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  406  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  407  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  408  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  409  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  410  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  411  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  412  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  413  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  414  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  415  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  416  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  417  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  418  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  419  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen  420  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  421  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  422  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  423  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  424  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  425  best=1.5547e-02 minâ€–gâ€–=2.06e-02\n",
      "Gen  426  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen  427  best=1.5547e-02 minâ€–gâ€–=2.11e-02\n",
      "Gen  428  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  429  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  430  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  431  best=1.5547e-02 minâ€–gâ€–=2.19e-02\n",
      "Gen  432  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  433  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  434  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  435  best=1.5547e-02 minâ€–gâ€–=2.22e-02\n",
      "Gen  436  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  437  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  438  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  439  best=1.5547e-02 minâ€–gâ€–=2.14e-02\n",
      "Gen  440  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  441  best=1.5547e-02 minâ€–gâ€–=2.08e-02\n",
      "Gen  442  best=1.5547e-02 minâ€–gâ€–=2.20e-02\n",
      "Gen  443  best=1.5547e-02 minâ€–gâ€–=2.13e-02\n",
      "Gen  444  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  445  best=1.5547e-02 minâ€–gâ€–=2.15e-02\n",
      "Gen  446  best=1.5547e-02 minâ€–gâ€–=2.16e-02\n",
      "Gen  447  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n",
      "Gen  448  best=1.5547e-02 minâ€–gâ€–=1.97e-02\n",
      "Gen  449  best=1.5547e-02 minâ€–gâ€–=2.21e-02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m init_mean = np.linspace(\u001b[38;5;28mmin\u001b[39m(best_b_log), \u001b[38;5;28mmax\u001b[39m(best_b_log), n_terms)   \u001b[38;5;66;03m# rough guess â‡’ b in ~[0.1, 400]\u001b[39;00m\n\u001b[32m      5\u001b[39m init_std = np.full(n_terms, \u001b[32m1.0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m best_b_log, best_score, de_info = \u001b[43mdifferential_evolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mce_user_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCR\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_g\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstall_generations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m750\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# --- final reconstruction of a and objective ----------------------------\u001b[39;00m\n\u001b[32m     22\u001b[39m best_b = np.exp(best_b_log)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mdifferential_evolution\u001b[39m\u001b[34m(user_fn, initial_mean, initial_std, pop_size, F, CR, n_iters, random_state, tol_g, stall_generations, verbose)\u001b[39m\n\u001b[32m     59\u001b[39m     trial[i] = np.where(cross_pts, mutant, pop[i])\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# ------------ evaluate trial population ------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m trial, trial_scores, info_trial = \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# ------------ selection ----------------------------------------------\u001b[39;00m\n\u001b[32m     65\u001b[39m improved = trial_scores < scores\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mce_user_fn\u001b[39m\u001b[34m(b_log_samples)\u001b[39m\n\u001b[32m     45\u001b[39m p0 = np.concatenate([a_opt, b_pos])      \u001b[38;5;66;03m# shape (2K,)\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# ---- (2) limited Newton polishing ----------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m p_new, gnorm, *_ = \u001b[43mnewton\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewton_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_step\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstall_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewton_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mls_bounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.any(np.isnan(p_new)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.any(np.isinf(p_new)):\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# ---- (3) extract & sort b (log) ------------------------------------\u001b[39;00m\n\u001b[32m     63\u001b[39m     b_new_pos = np.clip(p_new[K:], \u001b[32m1e-12\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# keep positivity\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mnewton\u001b[39m\u001b[34m(f, grad, hess, x0, tol_grad, tol_step, stall_iter, max_iter, ls_bounds, verbose)\u001b[39m\n\u001b[32m    130\u001b[39m     trust *= \u001b[32m2\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Backtracking line search\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m alpha, fx = \u001b[43mgolden_section_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mls_bounds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mls_bounds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m    135\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Update\u001b[39;00m\n\u001b[32m    138\u001b[39m x_new = x + alpha * p\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mgolden_section_search\u001b[39m\u001b[34m(f, a, b, tol, max_iter)\u001b[39m\n\u001b[32m     40\u001b[39m     b, d, f_d = d, c, f_c\n\u001b[32m     41\u001b[39m     c = b - phi * (b - a)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     f_c = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     44\u001b[39m     a, c, f_c = c, d, f_d\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mnewton.<locals>.<lambda>\u001b[39m\u001b[34m(alpha)\u001b[39m\n\u001b[32m    130\u001b[39m     trust *= \u001b[32m2\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Backtracking line search\u001b[39;00m\n\u001b[32m    132\u001b[39m alpha, fx = golden_section_search(\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m alpha: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    134\u001b[39m     a=ls_bounds[\u001b[32m0\u001b[39m], b=ls_bounds[\u001b[32m1\u001b[39m], tol=\u001b[32m1e-12\u001b[39m, max_iter=\u001b[32m100\u001b[39m\n\u001b[32m    135\u001b[39m )\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Update\u001b[39;00m\n\u001b[32m    138\u001b[39m x_new = x + alpha * p\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mobj\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobj\u001b[39m(p):\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# ensure NumPyâ†’JAX conversion to float64\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(obj_jax_jit(\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:5484\u001b[39m, in \u001b[36marray\u001b[39m\u001b[34m(object, dtype, copy, order, ndmin, device)\u001b[39m\n\u001b[32m   5482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, (\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m)):\n\u001b[32m   5483\u001b[39m   _ = dtypes.coerce_to_array(\u001b[38;5;28mobject\u001b[39m, dtype)\n\u001b[32m-> \u001b[39m\u001b[32m5484\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mArray\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   5485\u001b[39m   \u001b[38;5;66;03m# Check if object supports any of the data exchange protocols\u001b[39;00m\n\u001b[32m   5486\u001b[39m   \u001b[38;5;66;03m# (except dlpack, see data-apis/array-api#301). If it does,\u001b[39;00m\n\u001b[32m   5487\u001b[39m   \u001b[38;5;66;03m# consume the object as jax array and continue (but not return) so\u001b[39;00m\n\u001b[32m   5488\u001b[39m   \u001b[38;5;66;03m# that other array() arguments get processed against the input\u001b[39;00m\n\u001b[32m   5489\u001b[39m   \u001b[38;5;66;03m# object.\u001b[39;00m\n\u001b[32m   5490\u001b[39m   \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   5491\u001b[39m   \u001b[38;5;66;03m# Notice that data exchange protocols define dtype in the\u001b[39;00m\n\u001b[32m   5492\u001b[39m   \u001b[38;5;66;03m# corresponding data structures and it may not be available as\u001b[39;00m\n\u001b[32m   5493\u001b[39m   \u001b[38;5;66;03m# object.dtype. So, we'll resolve the protocols here before\u001b[39;00m\n\u001b[32m   5494\u001b[39m   \u001b[38;5;66;03m# evaluating object.dtype.\u001b[39;00m\n\u001b[32m   5495\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mobject\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m__jax_array__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   5496\u001b[39m     \u001b[38;5;28mobject\u001b[39m = \u001b[38;5;28mobject\u001b[39m.__jax_array__()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen abc>:117\u001b[39m, in \u001b[36m__instancecheck__\u001b[39m\u001b[34m(cls, instance)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "newton_steps = 3\n",
    "rng_seed = 123\n",
    "n_terms = 2\n",
    "init_mean = np.linspace(min(best_b_log), max(best_b_log), n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 1.0)\n",
    "\n",
    "best_b_log, best_score, de_info = differential_evolution(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    F=0.8,\n",
    "    CR=0.9,\n",
    "    n_iters=50000,\n",
    "    random_state=rng_seed,\n",
    "    tol_g=1e-9,\n",
    "    stall_generations=750,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- final reconstruction of a and objective ----------------------------\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = obj(np.concatenate([best_a, best_b]))\n",
    "\n",
    "print(\"\\n====================  DE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"DE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"generations:\", de_info['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46c7b368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f86d6530e10>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(best_b_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = grad(x)\n",
    "H = hess(x)\n",
    "d = np.linalg.solve(H, -g)  # Newton step\n",
    "dir_obj = lambda alpha: obj(x + alpha * d)\n",
    "alphas = np.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(alphas, [dir_obj(alpha) for alpha in alphas])\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Objective along Newton direction\")\n",
    "plt.title(\"Line search objective along Newton step\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae89cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 5.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "f_min_all = []\n",
    "min_b = -1.0\n",
    "max_b = 1.5\n",
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "for n_terms in range(2, 16):               # number of (a_i, b_i) pairs\n",
    "\n",
    "    b = np.linspace(min_b * 1.05, max_b * 1.1, n_terms)\n",
    "    a = optimal_a(d, w, target, np.exp(b))\n",
    "    # print(\"Optimal a:\", a)\n",
    "\n",
    "    means = np.concatenate((a, np.exp(b)))  # initial guess in log-space\n",
    "\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    x, grad_norm, f_history, grad_norm_history = newton(\n",
    "        f=obj,\n",
    "        grad=grad,\n",
    "        hess=hess,\n",
    "        x0=means,\n",
    "        tol_grad=1e-5,\n",
    "        tol_step=1e-8,\n",
    "        stall_iter=50,\n",
    "        max_iter=1000,\n",
    "        ls_bounds=(-2.0, 2.0)\n",
    "    )\n",
    "    min_b = min(np.log(x[n_terms:]))\n",
    "    max_b = max(np.log(x[n_terms:]))\n",
    "    print(f\"Min a log: {min_b}, Max b log: {max_b}\")\n",
    "    print(\"Final objective value:\", f_history[-1])\n",
    "    f_min_all.append(f_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f_min_all)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "\n",
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "best_init = np.array([0,0])\n",
    "\n",
    "for n_terms in range(1,10):                 # number of (a_i, b_i) pairs\n",
    "    means = best_init  # initial mean for (a_i, b_i)\n",
    "    cov = np.eye(2 * n_terms) * 4  # initial covariance matrix\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    best, stats = cross_entropy_numba(\n",
    "        d=d, target=target, w=w,\n",
    "        n_terms=n_terms,\n",
    "        max_iter=10000,\n",
    "        pop_size=10000,\n",
    "        elite_frac=0.1,\n",
    "        mean=means,\n",
    "        cov=cov\n",
    "    )\n",
    "\n",
    "    print(\"Best score :\", stats[\"best_score\"])\n",
    "    print(\"Best params:\", best)\n",
    "    print(\"Iterations :\", stats[\"iterations\"])\n",
    "    print(\"Runtime    :\", stats[\"runtime\"], \"s\")\n",
    "\n",
    "    first_half = best[:n_terms]\n",
    "    second_half = best[n_terms:]\n",
    "\n",
    "    # Original equidistant indices (0 to n_terms-1), new indices from 0 to n_terms\n",
    "    x_old = np.linspace(0, 1, n_terms)\n",
    "    x_new = np.linspace(0, 1, n_terms + 1)\n",
    "\n",
    "    interp_first = np.interp(x_new, x_old, first_half)\n",
    "    interp_second = np.interp(x_new, x_old, second_half)\n",
    "\n",
    "    best_init = np.hstack([interp_first, interp_second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d75dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(stats[\"history\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.title(\"Convergence of the Cross-Entropy Method\")\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best[n_terms:], 'o-')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"b_i (log scale)\")\n",
    "plt.title(\"Fitted b_i parameters\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f276bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import lsq_linear\n",
    "\n",
    "# compute using least squares the coefficients for approximation with squared exponential kernels\n",
    "# Construct the design matrix for squared exponential kernels\n",
    "A = np.zeros((len(d), n_terms))\n",
    "b_params = np.exp(best[n_terms:])\n",
    "\n",
    "for i in range(n_terms):\n",
    "    A[:, i] = np.exp(-b_params[i] * d**2)\n",
    "\n",
    "# Solve the weighted least squares problem: minimize ||W(Aa - target)||^2\n",
    "W = np.sqrt(w)\n",
    "Aw = A * W[:, None]\n",
    "tw = target * W\n",
    "\n",
    "res = lsq_linear(Aw, tw, bounds=(0, np.inf))\n",
    "squared_exponential_coeffs = res.x\n",
    "print(\"Squared exponential coefficients:\", np.log(squared_exponential_coeffs))\n",
    "\n",
    "# compute fit with this coefficients\n",
    "params = np.hstack([np.log(squared_exponential_coeffs), np.log(b_params)])\n",
    "\n",
    "res = objective_numba(np.array([params]), d, target, w)\n",
    "print(\"Objective value with least squares coefficients:\", res[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "f = lambda t: np.exp(-t)\n",
    "a, b, info = fit_exp_sum_ce(5, x, w, f, iterations=2000, pop_size=1000)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('best_score:', info.best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
