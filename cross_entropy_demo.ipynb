{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy kernel fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path('..') / 'src'))\n",
    "from kl_decomposition import rectangle_rule\n",
    "from kl_decomposition.kernel_fit import fit_exp_sum_ce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def objective_numba(params: np.ndarray,\n",
    "                    d: np.ndarray,\n",
    "                    target: np.ndarray,\n",
    "                    w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorised objective for K parameter sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params  : (K, 2*n_terms) log-space parameters\n",
    "    d       : (N,)           data values (distances); will use d**2\n",
    "    target  : (N,)           reference curve\n",
    "    w       : (N,)           weights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    obj_val : (K,)           objective for each parameter row\n",
    "    \"\"\"\n",
    "    K, P = params.shape\n",
    "    n_terms = P // 2\n",
    "    N = d.size\n",
    "    d2 = d * d                      # pre-compute d²\n",
    "    out = np.empty(K, dtype=params.dtype)\n",
    "\n",
    "    for k in prange(K):                  # loop over parameter sets in parallel\n",
    "        # split, exponentiate (back to linear scale)\n",
    "        a = np.exp(params[k, :n_terms])\n",
    "        b = np.exp(params[k, n_terms:])\n",
    "\n",
    "        acc = 0.0                        # accumulate Σ w*(pred-target)²\n",
    "        for j in range(N):               # loop over data points\n",
    "            dj2 = d2[j]\n",
    "            predj = 0.0\n",
    "            for i in range(n_terms):     # Σ_i a_i * exp(-b_i * d²)\n",
    "                predj += a[i] * np.exp(-b[i] * dj2)\n",
    "            diff = predj - target[j]\n",
    "            acc += w[j] * diff * diff\n",
    "\n",
    "        out[k] = acc\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8852ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "\n",
    "def cross_entropy_numba(\n",
    "    *,\n",
    "    d: np.ndarray,\n",
    "    target: np.ndarray,\n",
    "    w: np.ndarray,\n",
    "    n_terms: int,\n",
    "    max_iter: int = 50,\n",
    "    pop_size: int = 50,\n",
    "    elite_frac: float = 0.1,\n",
    "    mean: np.ndarray | None = None,\n",
    "    cov: np.ndarray | None = None,\n",
    "    rng: np.random.Generator | None = None,\n",
    ") -> Tuple[np.ndarray, Dict[str, float | list[float] | np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Cross-entropy minimiser that evaluates the whole population\n",
    "    via the parallel `objective_numba`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_params : (2*n_terms,)  vector giving the best solution found\n",
    "    stats       : dict with keys ('iterations', 'best_score', 'eval_count',\n",
    "                                   'history', 'runtime')\n",
    "    \"\"\"\n",
    "    rng = rng or np.random.default_rng()\n",
    "    dim = 2 * n_terms\n",
    "    mean = np.zeros(dim) if mean is None else np.asarray(mean, dtype=float)\n",
    "    cov = np.eye(dim)*9 if cov is None else np.asarray(cov, dtype=float)\n",
    "\n",
    "    history: list[float] = []\n",
    "    eval_count = 0\n",
    "    best_params = mean.copy()\n",
    "    best_score = float(\"inf\")\n",
    "    elite_num = max(1, int(pop_size * elite_frac))\n",
    "\n",
    "    # --- warm-up call to compile objective_numba just once ---\n",
    "    _ = objective_numba(np.zeros((1, dim)), d, target, w)\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(max_iter):\n",
    "        samples = rng.multivariate_normal(mean, cov, size=pop_size)      # (pop, dim)\n",
    "        # --- keep (a_i , b_i) pairs sorted by ascending b_i -----------------\n",
    "        a = samples[:, :n_terms]                 # first  half (log-a)\n",
    "        b = samples[:, n_terms:]                 # second half (log-b)\n",
    "\n",
    "        order = np.argsort(b, axis=1)            # sort indices for every row\n",
    "        rows  = np.arange(samples.shape[0])[:, None]\n",
    "\n",
    "        samples[:, :n_terms]  = a[rows, order]   # reorder a’s with same indices\n",
    "        samples[:, n_terms:]  = b[rows, order]   # reorder b’s (already sorted)\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        scores = objective_numba(samples, d, target, w)                 # (pop,)\n",
    "        eval_count += pop_size\n",
    "\n",
    "        idx = np.argsort(scores)\n",
    "        elite = samples[idx[:elite_num]]\n",
    "        mean_new = elite.mean(axis=0)\n",
    "        cov_new = np.cov(elite, rowvar=False)\n",
    "        mean = 0.75 * mean + 0.25 * mean_new\n",
    "        cov = 0.75 * cov + 0.25 * cov_new  # update covariance with some inertia\n",
    "\n",
    "        best_iter_score = float(scores[idx[0]])\n",
    "        history.append(best_iter_score)\n",
    "        if best_iter_score < best_score:\n",
    "            best_score = best_iter_score\n",
    "            best_params = samples[idx[0]]\n",
    "        if np.linalg.norm(cov_new) < 1e-6:\n",
    "            # covariance matrix is too small, stop early\n",
    "            print(\"Covariance matrix is too small, stopping early.\")\n",
    "            break\n",
    "\n",
    "    runtime = time.time() - start\n",
    "    stats = {\"iterations\": len(history),\n",
    "             \"best_score\": best_score,\n",
    "             \"eval_count\": eval_count,\n",
    "             \"history\": history,\n",
    "             \"runtime\": runtime,\n",
    "             \"final_cov\": cov,\n",
    "             \"final_mean\": mean}\n",
    "    return best_params, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270cb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- synthetic data ---------------------------------------------------\n",
    "n_terms = 4                 # number of (a_i, b_i) pairs\n",
    "N       = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "\n",
    "means = np.zeros(2 * n_terms)  # initial mean for (a_i, b_i)\n",
    "cov = np.eye(2 * n_terms) * 9  # initial covariance matrix\n",
    "# --- optimiser call ---------------------------------------------------\n",
    "best, stats = cross_entropy_numba(\n",
    "    d=d, target=target, w=w,\n",
    "    n_terms=n_terms,\n",
    "    max_iter=1000,\n",
    "    pop_size=10000,\n",
    "    elite_frac=0.01,\n",
    "    mean=means,\n",
    "    cov=cov\n",
    ")\n",
    "\n",
    "print(\"Best score :\", stats[\"best_score\"])\n",
    "print(\"Best params:\", best)\n",
    "print(\"Iterations :\", stats[\"iterations\"])\n",
    "print(\"Runtime    :\", stats[\"runtime\"], \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "\n",
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "best_init = np.array([0,0])\n",
    "\n",
    "for n_terms in range(1,10):                 # number of (a_i, b_i) pairs\n",
    "    means = best_init  # initial mean for (a_i, b_i)\n",
    "    cov = np.eye(2 * n_terms) * 4  # initial covariance matrix\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    best, stats = cross_entropy_numba(\n",
    "        d=d, target=target, w=w,\n",
    "        n_terms=n_terms,\n",
    "        max_iter=10000,\n",
    "        pop_size=10000,\n",
    "        elite_frac=0.1,\n",
    "        mean=means,\n",
    "        cov=cov\n",
    "    )\n",
    "\n",
    "    print(\"Best score :\", stats[\"best_score\"])\n",
    "    print(\"Best params:\", best)\n",
    "    print(\"Iterations :\", stats[\"iterations\"])\n",
    "    print(\"Runtime    :\", stats[\"runtime\"], \"s\")\n",
    "\n",
    "    first_half = best[:n_terms]\n",
    "    second_half = best[n_terms:]\n",
    "\n",
    "    # Original equidistant indices (0 to n_terms-1), new indices from 0 to n_terms\n",
    "    x_old = np.linspace(0, 1, n_terms)\n",
    "    x_new = np.linspace(0, 1, n_terms + 1)\n",
    "\n",
    "    interp_first = np.interp(x_new, x_old, first_half)\n",
    "    interp_second = np.interp(x_new, x_old, second_half)\n",
    "\n",
    "    best_init = np.hstack([interp_first, interp_second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d75dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(stats[\"history\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.title(\"Convergence of the Cross-Entropy Method\")\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best[n_terms:], 'o-')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"b_i (log scale)\")\n",
    "plt.title(\"Fitted b_i parameters\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f276bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import lsq_linear\n",
    "\n",
    "# compute using least squares the coefficients for approximation with squared exponential kernels\n",
    "# Construct the design matrix for squared exponential kernels\n",
    "A = np.zeros((len(d), n_terms))\n",
    "b_params = np.exp(best[n_terms:])\n",
    "\n",
    "for i in range(n_terms):\n",
    "    A[:, i] = np.exp(-b_params[i] * d**2)\n",
    "\n",
    "# Solve the weighted least squares problem: minimize ||W(Aa - target)||^2\n",
    "W = np.sqrt(w)\n",
    "Aw = A * W[:, None]\n",
    "tw = target * W\n",
    "\n",
    "res = lsq_linear(Aw, tw, bounds=(0, np.inf))\n",
    "squared_exponential_coeffs = res.x\n",
    "print(\"Squared exponential coefficients:\", np.log(squared_exponential_coeffs))\n",
    "\n",
    "# compute fit with this coefficients\n",
    "params = np.hstack([np.log(squared_exponential_coeffs), np.log(b_params)])\n",
    "\n",
    "res = objective_numba(np.array([params]), d, target, w)\n",
    "print(\"Objective value with least squares coefficients:\", res[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "f = lambda t: np.exp(-t)\n",
    "a, b, info = fit_exp_sum_ce(5, x, w, f, iterations=2000, pop_size=1000)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('best_score:', info.best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
