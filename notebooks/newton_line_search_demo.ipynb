{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Newton method with line search"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "import jax, jax.numpy as jnp\nimport numpy as np"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def func(x):\n    return (x[0]-1)**2 + (x[1]+2)**2"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "grad = jax.jit(jax.grad(func))\nhess = jax.jit(jax.hessian(func))"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def bisection_line_search(f, df, a=0.0, b=1.0, tol=1e-6, max_iter=20):\n    fa, fb = df(a), df(b)\n    if fa == 0: return a\n    if fb == 0: return b\n    for _ in range(max_iter):\n        mid = 0.5*(a+b); fm = df(mid)\n        if abs(fm) < tol: return mid\n        if fa*fm < 0: b, fb = mid, fm\n        else: a, fa = mid, fm\n    return 0.5*(a+b)"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "def newton(x0, max_iter=5, tol=1e-6):\n    x = np.array(x0, dtype=float)\n    for i in range(max_iter):\n        g = np.array(grad(x))\n        print(f'iter {i}, x={x}, grad_norm={np.linalg.norm(g):.2e}')\n        if np.linalg.norm(g) < tol: break\n        H = np.array(hess(x))\n        step = np.linalg.solve(H, g)\n        line_obj = lambda a: func(x - a*step)\n        line_grad = lambda a: float(jax.grad(line_obj)(a))\n        alpha = bisection_line_search(line_obj, line_grad)\n        x = x - alpha*step\n    return x"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "res = newton(np.array([0.0, 0.0]))\nprint('optimum:', res)"}], "metadata": {"kernelspec": {"display_name": "Python", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 2}
