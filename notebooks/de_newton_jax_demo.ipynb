{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential evolution with JAX gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import isinf\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, str(Path('..') / 'src'))\n",
    "from kl_decomposition import rectangle_rule\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmin_DENewton(\n",
    "    F, dF, ddF,\n",
    "    lower_bounds, upper_bounds,\n",
    "    beta_min, beta_max, pCR,\n",
    "    n_pop, tol_cost, tol_grad, max_it,\n",
    "    newton_LS_size=8,\n",
    "    rng=None,\n",
    "):\n",
    "    rng = rng or np.random.default_rng()\n",
    "    n_var = len(lower_bounds)\n",
    "    lower_bounds = np.asarray(lower_bounds, dtype=float)\n",
    "    upper_bounds = np.asarray(upper_bounds, dtype=float)\n",
    "    pop_x = np.empty((n_pop, n_var))\n",
    "    for j in range(n_var):\n",
    "        lo, hi = lower_bounds[j], upper_bounds[j]\n",
    "        if not isinf(lo) and not isinf(hi):\n",
    "            pop_x[:, j] = rng.uniform(lo, hi, size=n_pop)\n",
    "        elif isinf(lo) and isinf(hi):\n",
    "            pop_x[:, j] = rng.normal(size=n_pop)\n",
    "        elif isinf(hi):\n",
    "            pop_x[:, j] = lo + rng.exponential(size=n_pop)\n",
    "        else:\n",
    "            pop_x[:, j] = hi - rng.exponential(size=n_pop)\n",
    "    pop_cost = np.apply_along_axis(F, 1, pop_x)\n",
    "    best_idx = int(np.argmin(pop_cost))\n",
    "    best_x = pop_x[best_idx].copy()\n",
    "    best_cost = pop_cost[best_idx]\n",
    "    best_grad = np.zeros(n_var)\n",
    "    prev_mean = np.inf\n",
    "    for it in range(1, max_it + 1):\n",
    "        mean_cost = pop_cost.mean()\n",
    "        if abs(prev_mean - mean_cost) <= tol_cost and np.linalg.norm(best_grad) <= tol_grad:\n",
    "            break\n",
    "        prev_mean = mean_cost\n",
    "        for i in range(n_pop):\n",
    "            others = [j for j in range(n_pop) if j != i]\n",
    "            a, b, c = rng.choice(others, 3, replace=False)\n",
    "            beta = rng.uniform(beta_min, beta_max, size=n_var)\n",
    "            y = pop_x[a] + beta * (pop_x[b] - pop_x[c])\n",
    "            y = np.minimum(np.maximum(y, lower_bounds), upper_bounds)\n",
    "            j0 = rng.integers(n_var)\n",
    "            mask = rng.random(n_var) < pCR\n",
    "            mask[j0] = True\n",
    "            z = np.where(mask, y, pop_x[i])\n",
    "            z_cost = F(z)\n",
    "            if z_cost < pop_cost[i]:\n",
    "                pop_x[i], pop_cost[i] = z, z_cost\n",
    "                if z_cost < best_cost:\n",
    "                    best_x, best_cost = z.copy(), z_cost\n",
    "        for i in range(n_pop):\n",
    "            g = dF(pop_x[i])\n",
    "            H = ddF(pop_x[i])\n",
    "            try:\n",
    "                direction = -np.linalg.solve(H, g)\n",
    "            except np.linalg.LinAlgError:\n",
    "                continue\n",
    "            alphas = np.linspace(-1, 1, newton_LS_size)\n",
    "            candidates = pop_x[i] + np.outer(alphas, direction)\n",
    "            candidates = np.clip(candidates, lower_bounds, upper_bounds)\n",
    "            costs_line = np.apply_along_axis(F, 1, candidates)\n",
    "            idx_min = int(np.argmin(costs_line))\n",
    "            z_new = candidates[idx_min]\n",
    "            z_cost = costs_line[idx_min]\n",
    "            if z_cost < pop_cost[i]:\n",
    "                pop_x[i], pop_cost[i] = z_new, z_cost\n",
    "                if z_cost < best_cost:\n",
    "                    best_x, best_cost = z_new.copy(), z_cost\n",
    "                    best_grad = g\n",
    "    return best_x, it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup squared exponential approximation\n",
    "x, w = rectangle_rule(0.0, 2.0, 50)\n",
    "target = 2.0 * np.exp(-3.0 * x**2) + 0.5 * np.exp(-1.0 * x**2)\n",
    "x_j = jnp.array(x)\n",
    "w_j = jnp.array(w)\n",
    "target_j = jnp.array(target)\n",
    "\n",
    "def obj_jax(p):\n",
    "    a = jnp.exp(p[:2])\n",
    "    b = jnp.exp(p[2:])\n",
    "    pred = jnp.sum(a[:, None] * jnp.exp(-b[:, None] * x_j[None, :] ** 2), axis=0)\n",
    "    diff = pred - target_j\n",
    "    return 0.5 * jnp.sum(w_j * diff * diff)\n",
    "\n",
    "grad_jax = jax.grad(obj_jax)\n",
    "hess_jax = jax.hessian(obj_jax)\n",
    "\n",
    "def obj(p):\n",
    "    return float(obj_jax(jnp.array(p)))\n",
    "def grad(p):\n",
    "    return np.array(grad_jax(jnp.array(p)))\n",
    "def hess(p):\n",
    "    return np.array(hess_jax(jnp.array(p)))\n",
    "\n",
    "lower = np.full(4, -np.inf)\n",
    "upper = np.full(4, np.inf)\n",
    "best, it = fmin_DENewton(\n",
    "    obj, grad, hess,\n",
    "    lower, upper,\n",
    "    beta_min=0.5, beta_max=1.0, pCR=0.9,\n",
    "    n_pop=20, tol_cost=1e-8, tol_grad=1e-8, max_it=100\n",
    ")\n",
    "print('best params:', best)\n",
    "print('iterations:', it)\n",
    "print('recovered a:', np.exp(best[:2]))\n",
    "print('recovered b:', np.exp(best[2:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
