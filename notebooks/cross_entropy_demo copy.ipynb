{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy kernel fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from typing import Callable, Tuple, Dict, Any, Optional\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path('..') / 'src'))\n",
    "from kl_decomposition import rectangle_rule\n",
    "\n",
    "# Enable 64-bit (double) precision in JAX\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8852ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(\n",
    "    user_fn: Callable[[np.ndarray], Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],\n",
    "    initial_mean: np.ndarray,\n",
    "    initial_std: np.ndarray,\n",
    "    pop_size: int = 100,\n",
    "    elite_frac: float = 0.2,\n",
    "    alpha_mean: float = 0.7,\n",
    "    alpha_std: float = 0.7,\n",
    "    n_iters: int = 100,\n",
    "    random_state: Optional[int] = None,\n",
    "    verbose: bool = False,\n",
    "    tol_std: float = 1e-6,\n",
    "    tol_g: float = 1e-6\n",
    ") -> Tuple[np.ndarray, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Method with independent Gaussian distribution and stopping criteria.\n",
    "\n",
    "    Args:\n",
    "        user_fn: function taking samples (shape Nxd) and returning\n",
    "            (updated_samples, scores, info_dict)\n",
    "        initial_mean: initial mean vector (d,)\n",
    "        initial_std: initial std deviation vector (d,)\n",
    "        pop_size: number of samples per iteration\n",
    "        elite_frac: fraction of samples selected as elites\n",
    "        alpha_mean: learning rate for mean update\n",
    "        alpha_std: learning rate for std update\n",
    "        n_iters: maximum number of iterations\n",
    "        random_state: RNG seed for reproducibility\n",
    "        verbose: print debug info each iteration\n",
    "        tol_std: stop if max(std) <= tol_std\n",
    "        tol_g: stop if info_dict['normg'] <= tol_g\n",
    "\n",
    "    Returns:\n",
    "        best_sample: best found sample (d,)\n",
    "        best_score: best objective value\n",
    "        info: dict with:\n",
    "            'iter': iteration index at stop\n",
    "            'best_score_history': list of best_score after each iteration\n",
    "            'max_std_history': list of max_std after each iteration\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    mean = np.array(initial_mean, dtype=float)\n",
    "    std = np.array(initial_std, dtype=float)\n",
    "    n_elite = max(1, int(np.ceil(pop_size * elite_frac)))\n",
    "    best_sample: Optional[np.ndarray] = None\n",
    "    best_score = np.inf\n",
    "\n",
    "    # history lists\n",
    "    best_score_history: list = []\n",
    "    max_std_history: list = []\n",
    "\n",
    "    for iteration in range(n_iters):\n",
    "        samples = rng.randn(pop_size, mean.size) * std + mean\n",
    "        samples, scores, info_dict = user_fn(samples)\n",
    "        idx = int(np.argmin(scores))\n",
    "        # update best if improved\n",
    "        if scores[idx] < best_score:\n",
    "            best_score = float(scores[idx])\n",
    "            best_sample = samples[idx]\n",
    "        # select elites and update distribution\n",
    "        elite = samples[np.argsort(scores)[:n_elite]]\n",
    "        elite_mean = elite.mean(axis=0)\n",
    "        elite_std = elite.std(axis=0, ddof=0)\n",
    "        mean = alpha_mean * elite_mean + (1 - alpha_mean) * mean\n",
    "        std = alpha_std * elite_std + (1 - alpha_std) * std\n",
    "        # record history\n",
    "        max_std = float(np.max(std))\n",
    "        best_score_history.append(best_score)\n",
    "        max_std_history.append(max_std)\n",
    "        # debug print\n",
    "        if verbose:\n",
    "            print(f\"Iter {iteration}: best_score={best_score}, max_std={max_std}, info={info_dict}\")\n",
    "        # stopping criteria\n",
    "        if max_std <= tol_std:\n",
    "            break\n",
    "        if info_dict.get('normg', np.inf) <= tol_g:\n",
    "            break\n",
    "\n",
    "    info = {\n",
    "        'iter': iteration,\n",
    "        'best_score_history': best_score_history,\n",
    "        'max_std_history': max_std_history\n",
    "    }\n",
    "    return best_sample, best_score, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84814de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup squared exponential approximation\n",
    "x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "target = np.exp(-x)\n",
    "\n",
    "# Convert to JAX arrays with float64 dtype\n",
    "x_j = jnp.array(x, dtype=jnp.float64)\n",
    "w_j = jnp.array(w, dtype=jnp.float64)\n",
    "target_j = jnp.array(target, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def obj_jax(p):\n",
    "    # p is already float64\n",
    "    n_param = p.shape[0] // 2\n",
    "    a = p[:n_param]\n",
    "    b = p[n_param:]\n",
    "    # compute prediction and weighted RMS error\n",
    "    pred = jnp.sum(a[:, None] * jnp.exp(-b[:, None] * x_j[None, :] ** 2), axis=0)\n",
    "    diff = pred - target_j\n",
    "    return jnp.sqrt(jnp.sum(w_j * diff ** 2))\n",
    "\n",
    "\n",
    "# gradients and Hessians in double precision\n",
    "grad_jax = jax.jit(jax.grad(obj_jax))\n",
    "hess_jax = jax.jit(jax.hessian(obj_jax))\n",
    "obj_jax_jit = jax.jit(obj_jax)\n",
    "\n",
    "\n",
    "def obj(p):\n",
    "    # ensure NumPyâ†’JAX conversion to float64\n",
    "    return float(obj_jax_jit(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def grad(p):\n",
    "    return np.array(grad_jax(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def hess(p):\n",
    "    return np.array(hess_jax(jnp.array(p, dtype=jnp.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43e6be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def golden_section_search(f, a=0.0, b=1.0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Golden-section search to find the minimum of a unimodal function f on [a, b].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        The objective function to minimize.\n",
    "    a : float\n",
    "        Left endpoint of the initial interval.\n",
    "    b : float\n",
    "        Right endpoint of the initial interval.\n",
    "    tol : float\n",
    "        Tolerance for the interval width (stopping criterion).\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_min : float\n",
    "        Estimated position of the minimum.\n",
    "    f_min : float\n",
    "        Value of f at x_min.\n",
    "    \"\"\"\n",
    "    # Golden ratio constant\n",
    "    phi = (np.sqrt(5.0) - 1) / 2\n",
    "\n",
    "    # Initialize interior points\n",
    "    c = b - phi * (b - a)\n",
    "    d = a + phi * (b - a)\n",
    "    f_c = f(c)\n",
    "    f_d = f(d)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        if abs(b - a) < tol:\n",
    "            break\n",
    "\n",
    "        if f_c < f_d:\n",
    "            b, d, f_d = d, c, f_c\n",
    "            c = b - phi * (b - a)\n",
    "            f_c = f(c)\n",
    "        else:\n",
    "            a, c, f_c = c, d, f_d\n",
    "            d = a + phi * (b - a)\n",
    "            f_d = f(d)\n",
    "\n",
    "    # Choose the best of the final points\n",
    "    if f_c < f_d:\n",
    "        x_min, f_min = c, f_c\n",
    "    else:\n",
    "        x_min, f_min = d, f_d\n",
    "    # print(f\"Iterations: {_ + 1}\")\n",
    "    return x_min, f_min\n",
    "\n",
    "\n",
    "def newton(f, grad, hess, x0,\n",
    "                                   tol_grad=1e-6,\n",
    "                                   tol_step=1e-8,\n",
    "                                   stall_iter=5,\n",
    "                                   max_iter=100,\n",
    "                                   ls_bounds=(-1.0, 1.0),\n",
    "                                   verbose=False):\n",
    "    \"\"\"\n",
    "    Newton's method with backtracking line search and fallback to gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Objective function f(x).\n",
    "    grad : callable\n",
    "        Gradient of f: grad(x).\n",
    "    hess : callable\n",
    "        Hessian of f: hess(x).\n",
    "    x0 : ndarray\n",
    "        Initial guess.\n",
    "    tol_grad : float, optional\n",
    "        Tolerance for gradient norm.\n",
    "    tol_step : float, optional\n",
    "        Tolerance for step size norm for stall criterion.\n",
    "    stall_iter : int, optional\n",
    "        Number of consecutive small steps to trigger stop.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations.\n",
    "    ls_bounds : (float, float), optional\n",
    "        Initial interval for line search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : ndarray\n",
    "        Estimated minimizer.\n",
    "    grad_norm : float\n",
    "        Norm of gradient at the solution.\n",
    "    f_history : list of float\n",
    "        History of f values.\n",
    "    grad_norm_history : list of float\n",
    "        History of gradient norms.\n",
    "    \"\"\"\n",
    "    x = x0.astype(float)\n",
    "    f_history = []\n",
    "    grad_norm_history = []\n",
    "    fx = f(x)\n",
    "\n",
    "    stall_count = 0\n",
    "    trust = 1e-6 * np.eye(len(x))  # trust region to ensure positive definiteness\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        gx = grad(x)\n",
    "        grad_norm = np.linalg.norm(gx)\n",
    "\n",
    "        f_history.append(fx)\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "        # Check gradient convergence\n",
    "        if grad_norm < tol_grad:\n",
    "            break\n",
    "\n",
    "        # Try Newton step\n",
    "        try:\n",
    "            Hx = hess(x)\n",
    "            p = -np.linalg.solve(Hx + trust, gx)\n",
    "            trust /= 2\n",
    "            # ensure descent direction\n",
    "            # if np.dot(p, gx) >= 0:\n",
    "            #     raise np.linalg.LinAlgError\n",
    "        except np.linalg.LinAlgError:\n",
    "            # fallback to steepest descent\n",
    "            print(\"Hessian not positive definite, using gradient descent\")\n",
    "            p = -gx\n",
    "            trust *= 2\n",
    "        # Backtracking line search\n",
    "        alpha, fx = golden_section_search(\n",
    "            lambda alpha: f(x + alpha * p),\n",
    "            a=ls_bounds[0], b=ls_bounds[1], tol=1e-6, max_iter=100\n",
    "        )\n",
    "\n",
    "        # Update\n",
    "        x_new = x + alpha * p\n",
    "        step_norm = np.linalg.norm(x_new - x)\n",
    "        x = x_new\n",
    "\n",
    "        # Stall criterion\n",
    "        if step_norm < tol_step:\n",
    "            stall_count += 1\n",
    "            if stall_count >= stall_iter:\n",
    "                break\n",
    "        else:\n",
    "            stall_count = 0\n",
    "\n",
    "        # print debug info\n",
    "        if verbose:\n",
    "            print(f\"Iter {k}: f={fx:.6e}, grad_norm={grad_norm:.6e}, step_norm={step_norm:.6e}, alpha={alpha:.6f}\")\n",
    "\n",
    "    return x, grad_norm, f_history, grad_norm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910c5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "\n",
    "def optimal_a(d: np.ndarray,\n",
    "              w: np.ndarray,\n",
    "              target: np.ndarray,\n",
    "              b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve for a >= 0 minimizing\n",
    "        sum_j w[j] * (sum_k a[k] * exp(-b[k] * d[j]**2) - target[j])**2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : (J,) array\n",
    "        Sample points x_j.\n",
    "    w : (J,) array\n",
    "        Quadrature weights.\n",
    "    target : (J,) array\n",
    "        Target values at each d[j].\n",
    "    b : (K,) array\n",
    "        Exponents b_k.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : (K,) array\n",
    "        Non-negative coefficient vector minimizing the weighted LS error.\n",
    "    \"\"\"\n",
    "    # Build design matrix Î¦[j,k] = exp(-b[k] * d[j]**2)\n",
    "    # Note: np.outer(d**2, b) yields shape (J, K)\n",
    "    Phi = np.exp(-np.outer(d**2, b))     # shape (J, K)\n",
    "\n",
    "    # Incorporate weights by scaling rows\n",
    "    W_sqrt = np.sqrt(w)                  # shape (J,)\n",
    "    Phi_w = Phi * W_sqrt[:, None]        # shape (J, K)\n",
    "    y_w = target * W_sqrt              # shape (J,)\n",
    "\n",
    "    a, _ = nnls(Phi_w, y_w)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "270cb850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal a: [0.30869798 0.29722181 0.20486527 0.08991123 0.05465842 0.02056889\n",
      " 0.0121404  0.00894426]\n",
      "Optimised parameters: [3.10225902e-01 2.98387520e-01 1.69149724e-01 1.00889324e-01\n",
      " 6.00502655e-02 3.38693340e-02 1.75737665e-02 8.08160805e-03\n",
      " 2.19150999e-01 9.71349066e-01 3.38629401e+00 1.11679747e+01\n",
      " 4.03597872e+01 1.74832296e+02 1.02682248e+03 1.14880291e+04]\n",
      "Final gradient norm: 2.013948433568799e-06\n",
      "Final objective value: 7.475518925393422e-05\n"
     ]
    }
   ],
   "source": [
    "# --- synthetic data ---------------------------------------------------\n",
    "n_terms = 8               # number of (a_i, b_i) pairs\n",
    "N       = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "\n",
    "b = np.linspace(-1.5, 8.5, n_terms)\n",
    "a = optimal_a(d, w, target, np.exp(b))\n",
    "print(\"Optimal a:\", a)\n",
    "\n",
    "means = np.concatenate((a, np.exp(b)))  # initial guess in log-space\n",
    "\n",
    "# --- optimiser call ---------------------------------------------------\n",
    "x, grad_norm, f_history, grad_norm_history = newton(\n",
    "    f=obj,\n",
    "    grad=grad,\n",
    "    hess=hess,\n",
    "    x0=means,\n",
    "    tol_grad=1e-5,\n",
    "    tol_step=1e-8,\n",
    "    stall_iter=50,\n",
    "    max_iter=3000,\n",
    "    ls_bounds=(-2.0, 2.0)\n",
    ")\n",
    "\n",
    "print(\"Optimised parameters:\", x)\n",
    "print(\"Final gradient norm:\", grad_norm)\n",
    "print(\"Final objective value:\", f_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dddd19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: best_score=0.0012231538402284704, max_std=1.0648627729774303, info={'normg': 0.005489534972295633}\n",
      "Iter 1: best_score=0.001087920857002942, max_std=1.0170097907580031, info={'normg': 0.0068795041869161595}\n",
      "Iter 2: best_score=0.001087920857002942, max_std=1.0572491856296948, info={'normg': 0.005760717395194456}\n",
      "Iter 3: best_score=0.0006826017340082785, max_std=0.9864402855632646, info={'normg': 0.011424167959125302}\n",
      "Iter 4: best_score=0.0006826017340082785, max_std=0.9138857911092906, info={'normg': 0.00953240434976474}\n",
      "Iter 5: best_score=0.000625183421641857, max_std=0.8084116276720698, info={'normg': 0.002493088576820987}\n",
      "Iter 6: best_score=0.000625183421641857, max_std=0.7444497770850459, info={'normg': 0.0048675573075749115}\n",
      "Iter 7: best_score=0.000625183421641857, max_std=0.6736094107764038, info={'normg': 0.007120815189342109}\n",
      "Iter 8: best_score=0.000590396793454758, max_std=0.7364524401297582, info={'normg': 0.005056994365353775}\n",
      "Iter 9: best_score=0.00026200823897513985, max_std=0.758658412398816, info={'normg': 0.004151764902742238}\n",
      "Iter 10: best_score=0.00026200823897513985, max_std=0.6868411305193314, info={'normg': 0.004159005780662894}\n",
      "Iter 11: best_score=0.00026200823897513985, max_std=0.6540640184652742, info={'normg': 0.0013612975383660816}\n",
      "Iter 12: best_score=0.00026200823897513985, max_std=0.5342497099215988, info={'normg': 0.002928808447658258}\n",
      "Iter 13: best_score=0.00026200823897513985, max_std=0.5193281486442758, info={'normg': 0.002811071399348057}\n",
      "Iter 14: best_score=0.00026200823897513985, max_std=0.4861979568102496, info={'normg': 0.0008268432179825493}\n",
      "Iter 15: best_score=0.00026200823897513985, max_std=0.5392996873460192, info={'normg': 0.0015884294420015757}\n",
      "Iter 16: best_score=0.00017974161836024812, max_std=0.5203844208508772, info={'normg': 0.0014304131982689624}\n",
      "Iter 17: best_score=0.00015244556256138845, max_std=0.5821790101123752, info={'normg': 0.0012084265849592066}\n",
      "Iter 18: best_score=0.00015244556256138845, max_std=0.6072993906907409, info={'normg': 0.00037342713468296103}\n",
      "Iter 19: best_score=0.00011911506012496629, max_std=0.5832357855388322, info={'normg': 0.0007032916850024898}\n",
      "Iter 20: best_score=0.00011911506012496629, max_std=0.5685447594723545, info={'normg': 0.0006900752517688216}\n",
      "Iter 21: best_score=0.00010020366530425169, max_std=0.5085778569280152, info={'normg': 0.0003449132591485609}\n",
      "Iter 22: best_score=0.00010020366530425169, max_std=0.48785275780536586, info={'normg': 0.0016788200329343956}\n",
      "Iter 23: best_score=0.00010020366530425169, max_std=0.49716498658707237, info={'normg': 0.0008456937658709205}\n",
      "Iter 24: best_score=0.00010020366530425169, max_std=0.4472218359289597, info={'normg': 0.0004958017012293069}\n",
      "Iter 25: best_score=9.539280918704186e-05, max_std=0.41561111240395177, info={'normg': 0.0007217999548322689}\n",
      "Iter 26: best_score=9.539280918704186e-05, max_std=0.3892246003707169, info={'normg': 0.0003701741167993179}\n",
      "Iter 27: best_score=9.298565092496789e-05, max_std=0.38780671113727994, info={'normg': 0.00022490002997184772}\n",
      "Iter 28: best_score=9.298565092496789e-05, max_std=0.3606780614367604, info={'normg': 0.0004194952636589574}\n",
      "Iter 29: best_score=9.298565092496789e-05, max_std=0.3295141093918078, info={'normg': 0.0001240537910906151}\n",
      "Iter 30: best_score=8.877885109489353e-05, max_std=0.3189734586885325, info={'normg': 0.00028343724783071726}\n",
      "Iter 31: best_score=8.877885109489353e-05, max_std=0.34170667098194507, info={'normg': 0.00029317295626635133}\n",
      "Iter 32: best_score=8.661206084138409e-05, max_std=0.31709785045905314, info={'normg': 0.00032161218189235587}\n",
      "Iter 33: best_score=8.661206084138409e-05, max_std=0.31027849180084277, info={'normg': 0.00012491579932046536}\n",
      "Iter 34: best_score=8.661206084138409e-05, max_std=0.27271517485060515, info={'normg': 0.0001603945965366772}\n",
      "Iter 35: best_score=8.661206084138409e-05, max_std=0.26384682798885156, info={'normg': 0.00013442025622479204}\n",
      "Iter 36: best_score=8.587017763718906e-05, max_std=0.20372735991335994, info={'normg': 0.00010930291678349036}\n",
      "Iter 37: best_score=8.415480439852772e-05, max_std=0.1948634876225686, info={'normg': 0.00013092644624003878}\n",
      "Iter 38: best_score=8.389239073358575e-05, max_std=0.2040991086006312, info={'normg': 0.0001738067549894123}\n",
      "Iter 39: best_score=8.389239073358575e-05, max_std=0.17633513130277823, info={'normg': 0.00018832726602832682}\n",
      "Iter 40: best_score=8.346218529565429e-05, max_std=0.14114222230729784, info={'normg': 0.00018223881812393704}\n",
      "Iter 41: best_score=8.244229490858711e-05, max_std=0.13452991689670937, info={'normg': 0.00011864615043785796}\n",
      "Iter 42: best_score=8.244229490858711e-05, max_std=0.11519487028684725, info={'normg': 0.00013145920989578435}\n",
      "Iter 43: best_score=8.099303435556493e-05, max_std=0.10501810664984557, info={'normg': 0.0001392395556923532}\n",
      "Iter 44: best_score=8.099303435556493e-05, max_std=0.0955288057735937, info={'normg': 0.00012535291281517025}\n",
      "Iter 45: best_score=7.947344639941413e-05, max_std=0.09818620946594642, info={'normg': 0.00010424118749801538}\n",
      "Iter 46: best_score=7.947344639941413e-05, max_std=0.09354210482176475, info={'normg': 0.0002959488763142827}\n",
      "Iter 47: best_score=7.874014301615574e-05, max_std=0.09443032655815473, info={'normg': 0.00027558240210654914}\n",
      "Iter 48: best_score=7.874014301615574e-05, max_std=0.09049252025355278, info={'normg': 0.00015482386682783816}\n",
      "Iter 49: best_score=7.874014301615574e-05, max_std=0.08497225101842776, info={'normg': 0.00011608680528047018}\n",
      "Iter 50: best_score=7.828622754764463e-05, max_std=0.08096458704253526, info={'normg': 0.00018149231030093165}\n",
      "Iter 51: best_score=7.816012716809251e-05, max_std=0.089738242854897, info={'normg': 0.00014702946979585398}\n",
      "Iter 52: best_score=7.771970737498008e-05, max_std=0.087602970862669, info={'normg': 0.00022675255592963806}\n",
      "Iter 53: best_score=7.770541733300851e-05, max_std=0.08566555823201102, info={'normg': 0.00013711502878340878}\n",
      "Iter 54: best_score=7.714171579474312e-05, max_std=0.06922400209213386, info={'normg': 0.0001430629213509457}\n",
      "Iter 55: best_score=7.656162305374438e-05, max_std=0.05814931196552667, info={'normg': 0.00015673526293565624}\n",
      "Iter 56: best_score=7.570364914993695e-05, max_std=0.05995891166955324, info={'normg': 0.00014465866964340142}\n",
      "Iter 57: best_score=7.570364914993695e-05, max_std=0.05803931324455054, info={'normg': 0.000204336040162666}\n",
      "Iter 58: best_score=7.544632594718134e-05, max_std=0.05868478947861628, info={'normg': 0.0001648918747696329}\n",
      "Iter 59: best_score=7.485009065792983e-05, max_std=0.04499353489012693, info={'normg': 0.00012293590679279023}\n",
      "Iter 60: best_score=7.417705236881975e-05, max_std=0.04864026533635028, info={'normg': 0.0001574428198617449}\n",
      "Iter 61: best_score=7.347308628613303e-05, max_std=0.04725196699712379, info={'normg': 0.00011789422402126492}\n",
      "Iter 62: best_score=7.316916806462698e-05, max_std=0.04432203419603191, info={'normg': 0.00022249987671812648}\n",
      "Iter 63: best_score=7.316916806462698e-05, max_std=0.045452989495474515, info={'normg': 0.0001293918528359872}\n",
      "Iter 64: best_score=7.245960779242336e-05, max_std=0.047458574817403607, info={'normg': 0.00025043492288657327}\n",
      "Iter 65: best_score=7.196704063362132e-05, max_std=0.04783934222258599, info={'normg': 0.00014822732851727286}\n",
      "Iter 66: best_score=7.187599640611861e-05, max_std=0.04776022410295229, info={'normg': 0.0001919634535494679}\n",
      "Iter 67: best_score=7.154952191450459e-05, max_std=0.04418758149029483, info={'normg': 0.00015100432543771025}\n",
      "Iter 68: best_score=7.154952191450459e-05, max_std=0.04931553488015546, info={'normg': 0.00014958635263231793}\n",
      "Iter 69: best_score=7.130034961486672e-05, max_std=0.04302883873888125, info={'normg': 0.00019749524410728704}\n",
      "Iter 70: best_score=7.076631453301157e-05, max_std=0.04872887836186019, info={'normg': 0.00013554455516507057}\n",
      "Iter 71: best_score=6.957127643123724e-05, max_std=0.05133564663869541, info={'normg': 0.00019254864744134708}\n",
      "Iter 72: best_score=6.957127643123724e-05, max_std=0.0504647548928946, info={'normg': 0.0002367187642420504}\n",
      "Iter 73: best_score=6.900964282458923e-05, max_std=0.05386944049682838, info={'normg': 0.0001883801099278585}\n",
      "Iter 74: best_score=6.896711609065622e-05, max_std=0.06056497989435468, info={'normg': 0.00024552636717289987}\n",
      "Iter 75: best_score=6.880248110968616e-05, max_std=0.0490390045612291, info={'normg': 0.00016256890838664822}\n",
      "Iter 76: best_score=6.880248110968616e-05, max_std=0.049946638111818584, info={'normg': 0.0002660543422471559}\n",
      "Iter 77: best_score=6.818643485348062e-05, max_std=0.04536923763650501, info={'normg': 0.00020238148116146954}\n",
      "Iter 78: best_score=6.818643485348062e-05, max_std=0.04926097517719603, info={'normg': 0.00012064802329066174}\n",
      "Iter 79: best_score=6.742567824266202e-05, max_std=0.047404746361235844, info={'normg': 0.00016162358735384466}\n",
      "Iter 80: best_score=6.742567824266202e-05, max_std=0.04733031174064349, info={'normg': 0.0001278024024860622}\n",
      "Iter 81: best_score=6.742567824266202e-05, max_std=0.050348923177429034, info={'normg': 0.0004391015051712007}\n",
      "Iter 82: best_score=6.674957441766118e-05, max_std=0.050777107806651886, info={'normg': 0.00016314599615232993}\n",
      "Iter 83: best_score=6.674957441766118e-05, max_std=0.04845192203104262, info={'normg': 0.00013324314525456726}\n",
      "Iter 84: best_score=6.641592358562475e-05, max_std=0.05182530704274157, info={'normg': 0.00014933711297314496}\n",
      "Iter 85: best_score=6.591912654487787e-05, max_std=0.052005233609455095, info={'normg': 0.00014338431328997278}\n",
      "Iter 86: best_score=6.591912654487787e-05, max_std=0.04707954052999323, info={'normg': 0.00013383287471151843}\n",
      "Iter 87: best_score=6.591912654487787e-05, max_std=0.04863625245310668, info={'normg': 0.0003205083269147663}\n",
      "Iter 88: best_score=6.587507683108719e-05, max_std=0.048920512878220135, info={'normg': 0.00021485325739389362}\n",
      "Iter 89: best_score=6.522483528810233e-05, max_std=0.04326040796785013, info={'normg': 0.00038000701350494913}\n",
      "Iter 90: best_score=6.522483528810233e-05, max_std=0.03812928713555756, info={'normg': 0.0003056885509268186}\n",
      "Iter 91: best_score=6.400247887257336e-05, max_std=0.038188701087757015, info={'normg': 0.00023293128635577869}\n",
      "Iter 92: best_score=6.400247887257336e-05, max_std=0.0357456267548805, info={'normg': 0.00019503540258584352}\n",
      "Iter 93: best_score=6.354818847271456e-05, max_std=0.03610341496587442, info={'normg': 0.00024010845889040574}\n",
      "Iter 94: best_score=6.354818847271456e-05, max_std=0.04082284512201553, info={'normg': 0.00035067744816959587}\n",
      "Iter 95: best_score=6.349976150419055e-05, max_std=0.03622793103766985, info={'normg': 0.0001889586323172999}\n",
      "Iter 96: best_score=6.314217111155215e-05, max_std=0.03739823586374397, info={'normg': 0.0003358359049204621}\n",
      "Iter 97: best_score=6.308326618083308e-05, max_std=0.03823981498336233, info={'normg': 0.000191943992613518}\n",
      "Iter 98: best_score=6.308326618083308e-05, max_std=0.04164542177983917, info={'normg': 0.00023560580528915752}\n",
      "Iter 99: best_score=6.2451810500961e-05, max_std=0.03963719773265612, info={'normg': 0.00018182128231382193}\n",
      "Iter 100: best_score=6.22534238930083e-05, max_std=0.0336432089363236, info={'normg': 0.0001783420707256949}\n",
      "Iter 101: best_score=6.22534238930083e-05, max_std=0.028876813863346854, info={'normg': 0.00021631536194109699}\n",
      "Iter 102: best_score=6.22534238930083e-05, max_std=0.030525156889087746, info={'normg': 0.00011374604849598839}\n",
      "Iter 103: best_score=6.129852992601118e-05, max_std=0.02865518111356808, info={'normg': 0.0002203539740402397}\n",
      "Iter 104: best_score=6.0955993392681945e-05, max_std=0.02572763663366777, info={'normg': 0.00018800449739109662}\n",
      "Iter 105: best_score=6.09225262701796e-05, max_std=0.025255620154356465, info={'normg': 0.00030445625927426014}\n",
      "Iter 106: best_score=6.08464295282869e-05, max_std=0.028695976488714147, info={'normg': 0.00026991652906982087}\n",
      "Iter 107: best_score=6.0021212508335534e-05, max_std=0.029416312927546005, info={'normg': 0.00033079126471002407}\n",
      "Iter 108: best_score=5.9605932183572333e-05, max_std=0.025076646099492984, info={'normg': 0.00020386022630670695}\n",
      "Iter 109: best_score=5.931980456432898e-05, max_std=0.023141589333731307, info={'normg': 0.00029268630540641116}\n",
      "Iter 110: best_score=5.931980456432898e-05, max_std=0.02050756910811314, info={'normg': 0.00010577879368356612}\n",
      "Iter 111: best_score=5.911301773022288e-05, max_std=0.019057620318603807, info={'normg': 0.0014764835353117064}\n",
      "Iter 112: best_score=5.8390686378015854e-05, max_std=0.01763009205677458, info={'normg': 0.000213009516286915}\n",
      "Iter 113: best_score=5.8208785895246994e-05, max_std=0.016562297169380125, info={'normg': 0.00018386313175707483}\n",
      "Iter 114: best_score=5.8208785895246994e-05, max_std=0.01652758352154695, info={'normg': 0.0005766391161691343}\n",
      "Iter 115: best_score=5.720653163773315e-05, max_std=0.015692714179365182, info={'normg': 0.00010629059360368327}\n",
      "Iter 116: best_score=5.691884378852092e-05, max_std=0.017569927371988495, info={'normg': 0.00019672138995762965}\n",
      "Iter 117: best_score=5.6869620886238494e-05, max_std=0.016896554173740086, info={'normg': 0.00027979402911067154}\n",
      "Iter 118: best_score=5.6869620886238494e-05, max_std=0.017763311795779183, info={'normg': 0.00033163494028045975}\n",
      "Iter 119: best_score=5.601969629895568e-05, max_std=0.016756059249581262, info={'normg': 0.0001286055171545913}\n",
      "Iter 120: best_score=5.601969629895568e-05, max_std=0.015230666098438771, info={'normg': 0.00024424450551184643}\n",
      "Iter 121: best_score=5.589946891947621e-05, max_std=0.0140793296952545, info={'normg': 0.00019735507040736768}\n",
      "Iter 122: best_score=5.589946891947621e-05, max_std=0.013698169921226609, info={'normg': 0.00014904606928145848}\n",
      "Iter 123: best_score=5.53338045511657e-05, max_std=0.014935296213254668, info={'normg': 0.00017002563789443722}\n",
      "Iter 124: best_score=5.469588389822188e-05, max_std=0.01566626710924713, info={'normg': 0.0006673256305894796}\n",
      "Iter 125: best_score=5.469588389822188e-05, max_std=0.01572896532978304, info={'normg': 0.0002450507242759711}\n",
      "Iter 126: best_score=5.4591840235602266e-05, max_std=0.013969465105705743, info={'normg': 0.0003382493310906739}\n",
      "Iter 127: best_score=5.4591840235602266e-05, max_std=0.013914810982244705, info={'normg': 0.0006520554187714093}\n",
      "Iter 128: best_score=5.420352652761296e-05, max_std=0.013444237981789588, info={'normg': 0.00035229835978607606}\n",
      "Iter 129: best_score=5.303666401109112e-05, max_std=0.013050237340417051, info={'normg': 0.00022727172075152465}\n",
      "Iter 130: best_score=5.303666401109112e-05, max_std=0.011564067453223744, info={'normg': 8.389296893403331e-05}\n",
      "Iter 131: best_score=5.245125169597344e-05, max_std=0.012456346848957306, info={'normg': 0.00026030817434601186}\n",
      "Iter 132: best_score=5.1963937776745056e-05, max_std=0.012183535803093767, info={'normg': 0.00019668766201958886}\n",
      "Iter 133: best_score=5.1963937776745056e-05, max_std=0.011847026872934598, info={'normg': 0.0005164701356184161}\n",
      "Iter 134: best_score=5.1963937776745056e-05, max_std=0.012780340836483297, info={'normg': 0.0008254728823787868}\n",
      "Iter 135: best_score=5.181935326720465e-05, max_std=0.010322659932620205, info={'normg': 0.00010661715125670185}\n",
      "Iter 136: best_score=5.181935326720465e-05, max_std=0.010740996195307745, info={'normg': 0.0006403616924121995}\n",
      "Iter 137: best_score=5.181935326720465e-05, max_std=0.010460317254469172, info={'normg': 0.0002285738708575164}\n",
      "Iter 138: best_score=5.0450261629150866e-05, max_std=0.010416739915957304, info={'normg': 0.00019705320712923682}\n",
      "Iter 139: best_score=5.0450261629150866e-05, max_std=0.009074233911580304, info={'normg': 0.00023415186453497403}\n",
      "Iter 140: best_score=5.0450261629150866e-05, max_std=0.008966122277223466, info={'normg': 0.0006807967272625905}\n",
      "Iter 141: best_score=5.0450261629150866e-05, max_std=0.00849121525014953, info={'normg': 0.00019893682909169258}\n",
      "Iter 142: best_score=4.96364817168841e-05, max_std=0.008756697405145137, info={'normg': 0.0005282203103319622}\n",
      "Iter 143: best_score=4.929992295927785e-05, max_std=0.008916063528989257, info={'normg': 0.0003479126770168528}\n",
      "Iter 144: best_score=4.929992295927785e-05, max_std=0.009137958370108416, info={'normg': 0.0002680080707820955}\n",
      "Iter 145: best_score=4.929992295927785e-05, max_std=0.00849195404940586, info={'normg': 0.00024800541797727473}\n",
      "Iter 146: best_score=4.929992295927785e-05, max_std=0.009030909943540288, info={'normg': 0.00055554413345177}\n",
      "Iter 147: best_score=4.866430629701417e-05, max_std=0.008223191890900582, info={'normg': 0.00020856899216116102}\n",
      "Iter 148: best_score=4.835841130048297e-05, max_std=0.008397735122900523, info={'normg': 0.0003204602157902714}\n",
      "Iter 149: best_score=4.835841130048297e-05, max_std=0.008868962643330308, info={'normg': 0.0004789042188280298}\n",
      "Iter 150: best_score=4.759336787047985e-05, max_std=0.009148154945248762, info={'normg': 0.00020915705303404565}\n",
      "Iter 151: best_score=4.759336787047985e-05, max_std=0.010882398824669933, info={'normg': 0.00011777142850134467}\n",
      "Iter 152: best_score=4.757555557248584e-05, max_std=0.011135376055914997, info={'normg': 0.0005764319402523206}\n",
      "Iter 153: best_score=4.757555557248584e-05, max_std=0.009483233020886071, info={'normg': 0.0008831282012011713}\n",
      "Iter 154: best_score=4.666403915156432e-05, max_std=0.009758961091601664, info={'normg': 0.0001177511736573581}\n",
      "Iter 155: best_score=4.666403915156432e-05, max_std=0.011672163996938302, info={'normg': 0.0002991815975029233}\n",
      "Iter 156: best_score=4.6095772511454774e-05, max_std=0.011941507749388257, info={'normg': 0.00027139655301190854}\n",
      "Iter 157: best_score=4.6095772511454774e-05, max_std=0.011133729818392522, info={'normg': 0.00010072384998591864}\n",
      "Iter 158: best_score=4.6095772511454774e-05, max_std=0.010372332645244498, info={'normg': 0.00017319906789912303}\n",
      "Iter 159: best_score=4.6095772511454774e-05, max_std=0.010425183119647772, info={'normg': 0.00023219965230089542}\n",
      "Iter 160: best_score=4.589998198145817e-05, max_std=0.009573630141407575, info={'normg': 0.00039676022544762944}\n",
      "Iter 161: best_score=4.589998198145817e-05, max_std=0.010435037192660418, info={'normg': 0.00029267637991399606}\n",
      "Iter 162: best_score=4.5452797442878784e-05, max_std=0.009240406767347545, info={'normg': 0.000342970426945258}\n",
      "Iter 163: best_score=4.485741773741887e-05, max_std=0.013185687853065408, info={'normg': 0.0001975563237301951}\n",
      "Iter 164: best_score=4.485741773741887e-05, max_std=0.011509920017235626, info={'normg': 0.00023611162072999416}\n",
      "Iter 165: best_score=4.485741773741887e-05, max_std=0.010278020422613537, info={'normg': 0.00034724127689656685}\n",
      "Iter 166: best_score=4.483823833826663e-05, max_std=0.00901754723885206, info={'normg': 0.00017237350109516146}\n",
      "Iter 167: best_score=4.454003679205855e-05, max_std=0.008987979290015762, info={'normg': 0.0003066266428486344}\n",
      "Iter 168: best_score=4.4020073760761884e-05, max_std=0.008904440170805076, info={'normg': 3.76354277259682e-05}\n",
      "Iter 169: best_score=4.3670737338409615e-05, max_std=0.009353430629904617, info={'normg': 0.00021740532623024488}\n",
      "Iter 170: best_score=4.3670737338409615e-05, max_std=0.009250749600411227, info={'normg': 0.00022716155644017088}\n",
      "Iter 171: best_score=4.3530492737059527e-05, max_std=0.009471748837341375, info={'normg': 9.427648628215948e-05}\n",
      "Iter 172: best_score=4.327574153019061e-05, max_std=0.008499823460191595, info={'normg': 0.00038155169139279213}\n",
      "Iter 173: best_score=4.3030073795528214e-05, max_std=0.007572228506507848, info={'normg': 5.2100620069655146e-05}\n",
      "Iter 174: best_score=4.3030073795528214e-05, max_std=0.007377596759801868, info={'normg': 0.0008099759685073129}\n",
      "Iter 175: best_score=4.279093147803791e-05, max_std=0.0060939355822931356, info={'normg': 0.00024013772093415902}\n",
      "Iter 176: best_score=4.2505445115800095e-05, max_std=0.005904217442962441, info={'normg': 0.00014750367177930975}\n",
      "Iter 177: best_score=4.2417826066918615e-05, max_std=0.0056743368449705366, info={'normg': 6.572268905508964e-05}\n",
      "Iter 178: best_score=4.22004043455197e-05, max_std=0.006051046281362216, info={'normg': 9.001775406746925e-05}\n",
      "Iter 179: best_score=4.2140370658263874e-05, max_std=0.006102840865448917, info={'normg': 6.336772856682553e-05}\n",
      "Iter 180: best_score=4.210160627668915e-05, max_std=0.005365548350280129, info={'normg': 1.840926252162164e-05}\n",
      "Iter 181: best_score=4.204861000077343e-05, max_std=0.005112630911721582, info={'normg': 0.00011166346652964755}\n",
      "Iter 182: best_score=4.194000220542489e-05, max_std=0.004772738561518256, info={'normg': 0.00017891620921967549}\n",
      "Iter 183: best_score=4.1841499821038576e-05, max_std=0.004103679667316134, info={'normg': 2.139251638153976e-05}\n",
      "Iter 184: best_score=4.17718817953383e-05, max_std=0.004557379543694435, info={'normg': 1.3445668338790214e-05}\n",
      "Iter 185: best_score=4.1737017785929525e-05, max_std=0.00496431294539406, info={'normg': 2.0515867142829173e-05}\n",
      "Iter 186: best_score=4.1653068908390844e-05, max_std=0.004569095337713232, info={'normg': 2.131825611153241e-06}\n",
      "Iter 187: best_score=4.161434344713152e-05, max_std=0.0048282770704603534, info={'normg': 7.781797964869675e-06}\n",
      "Iter 188: best_score=4.158218493611517e-05, max_std=0.005090916095720672, info={'normg': 9.086170618994924e-06}\n",
      "Iter 189: best_score=4.152971371944372e-05, max_std=0.004787125898348648, info={'normg': 1.401284105690144e-05}\n",
      "Iter 190: best_score=4.1470025902412974e-05, max_std=0.005154713581735529, info={'normg': 2.7344964332393236e-05}\n",
      "Iter 191: best_score=4.1434498539814225e-05, max_std=0.004228256704398779, info={'normg': 2.4540256233019075e-05}\n",
      "Iter 192: best_score=4.1363700723622156e-05, max_std=0.0037851941991278847, info={'normg': 6.224057825199947e-05}\n",
      "Iter 193: best_score=4.13444855489375e-05, max_std=0.003234739535092725, info={'normg': 4.281494775887328e-05}\n",
      "Iter 194: best_score=4.130872485706805e-05, max_std=0.002906980534667511, info={'normg': 2.2543441209943916e-05}\n",
      "Iter 195: best_score=4.1265883084435376e-05, max_std=0.002879198983178261, info={'normg': 2.9369975713077153e-05}\n",
      "Iter 196: best_score=4.1241168096345844e-05, max_std=0.0026646093866444067, info={'normg': 2.4925833481878336e-05}\n",
      "Iter 197: best_score=4.122953552330127e-05, max_std=0.0026061323387834246, info={'normg': 6.062129925217472e-05}\n",
      "Iter 198: best_score=4.12067157573855e-05, max_std=0.002554776992464963, info={'normg': 3.0238635946738217e-05}\n",
      "Iter 199: best_score=4.118097174198329e-05, max_std=0.0022958600804498363, info={'normg': 2.575150868322449e-05}\n",
      "Iter 200: best_score=4.116653798519967e-05, max_std=0.0024234225059411524, info={'normg': 3.253065464015424e-05}\n",
      "Iter 201: best_score=4.115003808837827e-05, max_std=0.0022351304712597084, info={'normg': 3.070473370241844e-05}\n",
      "Iter 202: best_score=4.113305183614496e-05, max_std=0.0021827463487305754, info={'normg': 2.948036452483713e-05}\n",
      "Iter 203: best_score=4.112101954328934e-05, max_std=0.002082792734016907, info={'normg': 1.4095963483559382e-05}\n",
      "Iter 204: best_score=4.110976470275692e-05, max_std=0.0021056732659796994, info={'normg': 1.2187952954378818e-05}\n",
      "Iter 205: best_score=4.109850683737607e-05, max_std=0.0020361076064623848, info={'normg': 4.9257862602820905e-05}\n",
      "Iter 206: best_score=4.10873723207179e-05, max_std=0.002018609248890335, info={'normg': 7.313499237065788e-05}\n",
      "Iter 207: best_score=4.107763672330256e-05, max_std=0.0018832250573178216, info={'normg': 1.2220086633010944e-05}\n",
      "Iter 208: best_score=4.107078873894274e-05, max_std=0.0017825396039600435, info={'normg': 9.382740944766728e-06}\n",
      "Iter 209: best_score=4.106068357247985e-05, max_std=0.0017400627057243224, info={'normg': 1.7231135415390152e-05}\n",
      "Iter 210: best_score=4.105852922893098e-05, max_std=0.0016694748647924575, info={'normg': 4.012090386096197e-05}\n",
      "Iter 211: best_score=4.1053271442056456e-05, max_std=0.001533990013025852, info={'normg': 5.2417327965697695e-06}\n",
      "Iter 212: best_score=4.104410974661634e-05, max_std=0.0014253070655099974, info={'normg': 1.1052273619691004e-05}\n",
      "Iter 213: best_score=4.104217159873431e-05, max_std=0.001282427890373433, info={'normg': 1.7674052346964264e-05}\n",
      "Iter 214: best_score=4.1036937672050456e-05, max_std=0.0011719496862185959, info={'normg': 5.896447679281486e-06}\n",
      "Iter 215: best_score=4.103432913812081e-05, max_std=0.0011183474542362064, info={'normg': 4.060568473665661e-05}\n",
      "Iter 216: best_score=4.102932485496248e-05, max_std=0.0010708821034797452, info={'normg': 3.1295951845179665e-05}\n",
      "Iter 217: best_score=4.1023833197545426e-05, max_std=0.0010885719453738714, info={'normg': 1.5186771830172142e-05}\n",
      "Iter 218: best_score=4.102096724865614e-05, max_std=0.0011338258456646767, info={'normg': 1.8798187712790792e-05}\n",
      "Iter 219: best_score=4.101767416050353e-05, max_std=0.0010577008148453392, info={'normg': 1.5718615985923406e-05}\n",
      "Iter 220: best_score=4.1014433017901e-05, max_std=0.0010628114131647265, info={'normg': 8.951847082730695e-06}\n",
      "Iter 221: best_score=4.101064703532419e-05, max_std=0.0009357856647278734, info={'normg': 1.0741242860116348e-05}\n",
      "Iter 222: best_score=4.10083773687489e-05, max_std=0.0008982063374755831, info={'normg': 1.0362129289521087e-05}\n",
      "Iter 223: best_score=4.1004680639136716e-05, max_std=0.0008933489825279514, info={'normg': 1.704080407682436e-05}\n",
      "Iter 224: best_score=4.1002281132062865e-05, max_std=0.0007543430841608688, info={'normg': 8.930924742585301e-06}\n",
      "Iter 225: best_score=4.099868295246139e-05, max_std=0.000762994736439966, info={'normg': 7.198708444444118e-06}\n",
      "Iter 226: best_score=4.099735582315949e-05, max_std=0.0006435328381655351, info={'normg': 5.549615367392221e-06}\n",
      "Iter 227: best_score=4.099462865125481e-05, max_std=0.0006270161796775513, info={'normg': 7.910837658005378e-06}\n",
      "Iter 228: best_score=4.0992836390025926e-05, max_std=0.0005287783505813888, info={'normg': 2.571543360065058e-05}\n",
      "Iter 229: best_score=4.0990065038013534e-05, max_std=0.0005594236307306768, info={'normg': 1.0272751796141624e-05}\n",
      "Iter 230: best_score=4.0988804764830054e-05, max_std=0.0005350382248086877, info={'normg': 9.043241204946564e-06}\n",
      "Iter 231: best_score=4.098672074847127e-05, max_std=0.0005490962256028178, info={'normg': 5.7496085966013445e-06}\n",
      "Iter 232: best_score=4.098437026959251e-05, max_std=0.0005565962820273356, info={'normg': 7.087579355967048e-06}\n",
      "Iter 233: best_score=4.09829110919735e-05, max_std=0.0005206249054134624, info={'normg': 1.619069586089363e-05}\n",
      "Iter 234: best_score=4.098107245610528e-05, max_std=0.0005098084725956608, info={'normg': 6.65500942305404e-06}\n",
      "Iter 235: best_score=4.097838981414744e-05, max_std=0.00040301650418330195, info={'normg': 2.440941605336233e-05}\n",
      "Iter 236: best_score=4.097683605635373e-05, max_std=0.0004208252639783263, info={'normg': 1.713055150328166e-05}\n",
      "Iter 237: best_score=4.097499855774845e-05, max_std=0.00038660341017159717, info={'normg': 9.122438046628755e-06}\n",
      "Iter 238: best_score=4.0973197210639795e-05, max_std=0.00035776310065008433, info={'normg': 3.70155380700409e-06}\n",
      "Iter 239: best_score=4.097166374037316e-05, max_std=0.0003643019512489024, info={'normg': 3.877486342428105e-06}\n",
      "Iter 240: best_score=4.096928904833969e-05, max_std=0.00034713644038307105, info={'normg': 8.629589377179963e-06}\n",
      "Iter 241: best_score=4.096825045895562e-05, max_std=0.00035244573526643195, info={'normg': 8.754805328726634e-06}\n",
      "Iter 242: best_score=4.096691743165885e-05, max_std=0.00033990082210030485, info={'normg': 1.8639005106243966e-07}\n",
      "\n",
      "====================  CE RESULT  ====================\n",
      "best log-b : [-1.53659286 -0.06624804  1.15663614  2.3069685   3.52685784  4.89183596\n",
      "  6.46014616  8.26276553 10.41400434]\n",
      "best b     : [2.15112773e-01 9.35898691e-01 3.17922082e+00 1.00439303e+01\n",
      " 3.40169131e+01 1.33197896e+02 6.39154471e+02 3.87680071e+03\n",
      " 3.33230403e+04]\n",
      "best a     : [0.30321217 0.29650421 0.16884332 0.10151606 0.0615543  0.0355727\n",
      " 0.0187767  0.00872474 0.00401095]\n",
      "CE score   : 4.096691743165885e-05\n",
      "check obj  : 4.096691743165875e-05\n",
      "stopped at iteration 242 | max Ïƒ = 0.00033990082210030485\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#  Cross-Entropy search over log-b with internal Newton polishing\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "\n",
    "# --- problem size -----------------------------------------------------------\n",
    "n_terms = 9          # K  (= length of b and a)\n",
    "newton_steps = 1     # how many Newton iterations per sample\n",
    "pop_size = 100       # CE population\n",
    "np.random.seed(0)    # reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  helper: CE user-defined update function operating on log-b only\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def ce_user_fn(b_log_samples: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b_log_samples : (N, K) ndarray\n",
    "        Current population in *log* space.\n",
    "    Returns\n",
    "    -------\n",
    "    updated_log_b : (N, K) ndarray\n",
    "        Polished & sorted log-b that CE will keep using.\n",
    "    scores        : (N,)  ndarray\n",
    "        Objective values after polishing (smaller is better).\n",
    "    info          : dict\n",
    "        Contains minimum gradient norm among all samples (for CE stopping).\n",
    "    \"\"\"\n",
    "    N, K = b_log_samples.shape\n",
    "    updated = np.empty_like(b_log_samples)\n",
    "    scores = np.empty(N)\n",
    "    grad_norm = np.empty(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        # ---- (1) build starting point --------------------------------------\n",
    "        b_pos = np.exp(b_log_samples[i])            # (K,)  positive\n",
    "        a_opt = optimal_a(d, w, target, b_pos)      # (K,)\n",
    "        p0 = np.concatenate([a_opt, b_pos])      # shape (2K,)\n",
    "\n",
    "        # ---- (2) limited Newton polishing ----------------------------------\n",
    "        p_new, gnorm, *_ = newton(\n",
    "            f=obj,\n",
    "            grad=grad,\n",
    "            hess=hess,\n",
    "            x0=p0,\n",
    "            max_iter=newton_steps,\n",
    "            tol_grad=1e-6,\n",
    "            tol_step=1e-6,\n",
    "            stall_iter=newton_steps,\n",
    "            ls_bounds=(-2., 2.),\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        if not np.any(np.isnan(p_new)) and not np.any(np.isinf(p_new)) and not np.any(p_new < 0):\n",
    "            # ---- (3) extract & sort b (log) ------------------------------------\n",
    "            b_new_pos = np.clip(p_new[K:], 1e-12, None)  # keep positivity\n",
    "            b_new_log = np.log(b_new_pos)\n",
    "            order = np.argsort(b_new_log)\n",
    "            a_opt = optimal_a(d, w, target, b_new_pos)      # (K,)\n",
    "            p0 = np.concatenate([a_opt, b_new_pos])      # shape (2K,)\n",
    "\n",
    "            updated[i] = b_new_log[order]\n",
    "            scores[i] = obj(p0)       # score *after* polishing\n",
    "            grad_norm[i] = np.linalg.norm(grad(p0))\n",
    "        else:\n",
    "            # if polishing failed, keep the original sample\n",
    "            updated[i] = b_log_samples[i]\n",
    "            scores[i] = np.inf\n",
    "            grad_norm[i] = np.inf\n",
    "\n",
    "    info = {'normg': float(grad_norm.min())}\n",
    "    return updated, scores, info\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  CE initial distribution over log-b\n",
    "# --------------------------------------------------------------------------- #\n",
    "init_mean = np.linspace(-1.5, 9.3, n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 1.0)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Run Cross-Entropy\n",
    "# --------------------------------------------------------------------------- #\n",
    "best_b_log, best_score, ce_info = cross_entropy(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    elite_frac=0.2,\n",
    "    alpha_mean=0.5,\n",
    "    alpha_std=0.5,\n",
    "    n_iters=2000,\n",
    "    random_state=1234,\n",
    "    tol_g=1e-6,\n",
    "    tol_std=1e-6,\n",
    "    verbose=True      # prints per-iteration progress\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Report results\n",
    "# --------------------------------------------------------------------------- #\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = obj(np.concatenate([best_a, best_b]))\n",
    "\n",
    "print(\"\\n====================  CE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"CE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"stopped at iteration\", ce_info['iter'],\n",
    "      \"| max Ïƒ =\", ce_info['max_std_history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "830c3d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.54387056, -0.0805901 ,  1.13304965,  2.26870079,  3.46671661,\n",
       "        4.7994511 ,  6.30903264,  8.00312236, 10.14581073])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_b_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5f43c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Differential-Evolution optimiser that works with the same `user_fn`\n",
    "###############################################################################\n",
    "from typing import Callable, Tuple, Dict, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def differential_evolution(\n",
    "    user_fn: Callable[[np.ndarray],\n",
    "                      Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],\n",
    "    initial_mean: np.ndarray,\n",
    "    initial_std: np.ndarray,\n",
    "    pop_size: int = 100,\n",
    "    F: float = 0.8,                 # differential weight\n",
    "    CR: float = 0.9,                # crossover probability\n",
    "    n_iters: int = 500,\n",
    "    random_state: Optional[int] = None,\n",
    "    tol_g: float = 1e-6,            # stop if minâ€–gâ€– drops below this\n",
    "    stall_generations: int = 50,    # stop if no improvement for so many gens\n",
    "    verbose: bool = False\n",
    ") -> Tuple[np.ndarray, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Differential Evolution (DE/rand/1/bin) with the CE-style `user_fn`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_sample : ndarray\n",
    "        The best parameter vector found (d,).\n",
    "    best_score  : float\n",
    "        Objective value of `best_sample`.\n",
    "    info        : dict\n",
    "        Diagnostics (`iter`, `best_score_history`).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    d = initial_mean.size\n",
    "\n",
    "    # --- initial population --------------------------------------------------\n",
    "    pop = rng.randn(pop_size, d) * initial_std + initial_mean       # (N, d)\n",
    "    pop, scores, info = user_fn(pop)                               # polish & score\n",
    "    best_idx = int(np.argmin(scores))\n",
    "    best_sample = pop[best_idx].copy()\n",
    "    best_score = float(scores[best_idx])\n",
    "\n",
    "    best_score_history = [best_score]\n",
    "    no_improve = 0\n",
    "\n",
    "    for it in range(1, n_iters + 1):\n",
    "        # ------------ mutation & crossover ----------------------------------\n",
    "        trial = np.empty_like(pop)\n",
    "        for i in range(pop_size):\n",
    "            # choose three *distinct* indices â‰  i\n",
    "            r1, r2, r3 = rng.choice([j for j in range(pop_size) if j != i],\n",
    "                                    size=3, replace=False)\n",
    "            mutant = pop[r1] + F * (pop[r2] - pop[r3])\n",
    "\n",
    "            # binomial crossover\n",
    "            cross_pts = rng.rand(d) < CR\n",
    "            cross_pts[rng.randint(0, d)] = True      # ensure at least one gene\n",
    "            trial[i] = np.where(cross_pts, mutant, pop[i])\n",
    "\n",
    "        # ------------ evaluate trial population ------------------------------\n",
    "        trial, trial_scores, info_trial = user_fn(trial)\n",
    "\n",
    "        # ------------ selection ----------------------------------------------\n",
    "        improved = trial_scores < scores\n",
    "        pop[improved] = trial[improved]\n",
    "        scores[improved] = trial_scores[improved]\n",
    "\n",
    "        # ------------ keep track of best -------------------------------------\n",
    "        gen_best_idx = int(np.argmin(scores))\n",
    "        gen_best_score = float(scores[gen_best_idx])\n",
    "        if gen_best_score + 1e-12 < best_score:      # strict improvement\n",
    "            best_score = gen_best_score\n",
    "            best_sample = pop[gen_best_idx].copy()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        best_score_history.append(best_score)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Gen {it:4d}  best={best_score:.4e} \"\n",
    "                  f\"minâ€–gâ€–={info_trial.get('normg', np.inf):.2e}\")\n",
    "\n",
    "        # ------------ stopping criteria --------------------------------------\n",
    "        if info_trial.get('normg', np.inf) <= tol_g:\n",
    "            if verbose:\n",
    "                print(\"Stopping: gradient norm below tol_g\")\n",
    "            break\n",
    "        if no_improve >= stall_generations:\n",
    "            if verbose:\n",
    "                print(\"Stopping: no improvement for\",\n",
    "                      stall_generations, \"generations\")\n",
    "            break\n",
    "\n",
    "    info = {\n",
    "        'iter': it,\n",
    "        'best_score_history': best_score_history\n",
    "    }\n",
    "    return best_sample, best_score, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f1ddd9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen    1  best=1.4301e-04 minâ€–gâ€–=2.76e-04\n",
      "Gen    2  best=1.4301e-04 minâ€–gâ€–=2.92e-03\n",
      "Gen    3  best=1.3752e-04 minâ€–gâ€–=2.12e-03\n",
      "Gen    4  best=1.3752e-04 minâ€–gâ€–=1.98e-03\n",
      "Gen    5  best=1.3752e-04 minâ€–gâ€–=3.79e-04\n",
      "Gen    6  best=1.0894e-04 minâ€–gâ€–=5.51e-04\n",
      "Gen    7  best=1.0894e-04 minâ€–gâ€–=5.11e-04\n",
      "Gen    8  best=1.0701e-04 minâ€–gâ€–=2.75e-04\n",
      "Gen    9  best=7.0519e-05 minâ€–gâ€–=1.22e-03\n",
      "Gen   10  best=7.0519e-05 minâ€–gâ€–=4.45e-04\n",
      "Gen   11  best=7.0519e-05 minâ€–gâ€–=6.20e-04\n",
      "Gen   12  best=4.9375e-05 minâ€–gâ€–=6.88e-04\n",
      "Gen   13  best=4.9375e-05 minâ€–gâ€–=2.86e-04\n",
      "Gen   14  best=4.9375e-05 minâ€–gâ€–=1.05e-03\n",
      "Gen   15  best=4.9375e-05 minâ€–gâ€–=2.53e-04\n",
      "Gen   16  best=4.9375e-05 minâ€–gâ€–=1.43e-04\n",
      "Gen   17  best=4.9375e-05 minâ€–gâ€–=1.35e-05\n",
      "Gen   18  best=4.9375e-05 minâ€–gâ€–=2.42e-04\n",
      "Gen   19  best=4.9375e-05 minâ€–gâ€–=7.41e-05\n",
      "Gen   20  best=4.9375e-05 minâ€–gâ€–=1.70e-04\n",
      "Gen   21  best=4.9375e-05 minâ€–gâ€–=5.85e-05\n",
      "Gen   22  best=4.9375e-05 minâ€–gâ€–=3.84e-04\n",
      "Gen   23  best=4.9375e-05 minâ€–gâ€–=1.02e-04\n",
      "Gen   24  best=4.9375e-05 minâ€–gâ€–=7.47e-04\n",
      "Gen   25  best=4.9375e-05 minâ€–gâ€–=1.13e-05\n",
      "Gen   26  best=4.9375e-05 minâ€–gâ€–=9.31e-05\n",
      "Gen   27  best=4.9375e-05 minâ€–gâ€–=8.41e-05\n",
      "Gen   28  best=4.9375e-05 minâ€–gâ€–=7.91e-04\n",
      "Gen   29  best=4.9375e-05 minâ€–gâ€–=1.13e-04\n",
      "Gen   30  best=4.9375e-05 minâ€–gâ€–=3.90e-04\n",
      "Gen   31  best=4.9375e-05 minâ€–gâ€–=1.19e-04\n",
      "Gen   32  best=4.4916e-05 minâ€–gâ€–=9.33e-05\n",
      "Gen   33  best=4.3726e-05 minâ€–gâ€–=7.01e-06\n",
      "Gen   34  best=4.3726e-05 minâ€–gâ€–=1.91e-04\n",
      "Gen   35  best=4.3726e-05 minâ€–gâ€–=4.17e-04\n",
      "Gen   36  best=4.3726e-05 minâ€–gâ€–=2.13e-05\n",
      "Gen   37  best=4.3726e-05 minâ€–gâ€–=6.54e-04\n",
      "Gen   38  best=4.3726e-05 minâ€–gâ€–=4.56e-06\n",
      "Gen   39  best=4.3726e-05 minâ€–gâ€–=9.73e-05\n",
      "Gen   40  best=4.3726e-05 minâ€–gâ€–=6.36e-05\n",
      "Gen   41  best=4.3726e-05 minâ€–gâ€–=2.02e-05\n",
      "Gen   42  best=4.3726e-05 minâ€–gâ€–=8.73e-05\n",
      "Gen   43  best=3.3922e-05 minâ€–gâ€–=5.71e-06\n",
      "Gen   44  best=3.3922e-05 minâ€–gâ€–=1.35e-04\n",
      "Gen   45  best=3.3922e-05 minâ€–gâ€–=1.94e-05\n",
      "Gen   46  best=3.3922e-05 minâ€–gâ€–=4.24e-04\n",
      "Gen   47  best=3.3922e-05 minâ€–gâ€–=4.70e-06\n",
      "Gen   48  best=3.3922e-05 minâ€–gâ€–=2.53e-05\n",
      "Gen   49  best=2.8156e-05 minâ€–gâ€–=1.35e-05\n",
      "Gen   50  best=2.8156e-05 minâ€–gâ€–=1.11e-04\n",
      "Gen   51  best=2.8156e-05 minâ€–gâ€–=2.36e-06\n",
      "Gen   52  best=2.8156e-05 minâ€–gâ€–=1.51e-05\n",
      "Gen   53  best=2.8126e-05 minâ€–gâ€–=3.49e-05\n",
      "Gen   54  best=2.0066e-05 minâ€–gâ€–=5.68e-06\n",
      "Gen   55  best=2.0066e-05 minâ€–gâ€–=6.11e-05\n",
      "Gen   56  best=2.0066e-05 minâ€–gâ€–=2.31e-05\n",
      "Gen   57  best=2.0066e-05 minâ€–gâ€–=1.22e-05\n",
      "Gen   58  best=2.0066e-05 minâ€–gâ€–=8.11e-06\n",
      "Gen   59  best=2.0066e-05 minâ€–gâ€–=2.95e-05\n",
      "Gen   60  best=2.0066e-05 minâ€–gâ€–=5.66e-06\n",
      "Gen   61  best=2.0066e-05 minâ€–gâ€–=5.80e-05\n",
      "Gen   62  best=2.0066e-05 minâ€–gâ€–=1.17e-05\n",
      "Gen   63  best=2.0066e-05 minâ€–gâ€–=8.65e-05\n",
      "Gen   64  best=2.0066e-05 minâ€–gâ€–=2.07e-06\n",
      "Gen   65  best=2.0066e-05 minâ€–gâ€–=1.88e-05\n",
      "Gen   66  best=1.9764e-05 minâ€–gâ€–=2.27e-05\n",
      "Gen   67  best=1.9764e-05 minâ€–gâ€–=1.09e-05\n",
      "Gen   68  best=1.9764e-05 minâ€–gâ€–=1.27e-05\n",
      "Gen   69  best=1.9764e-05 minâ€–gâ€–=3.35e-05\n",
      "Gen   70  best=1.9764e-05 minâ€–gâ€–=1.79e-05\n",
      "Gen   71  best=1.7249e-05 minâ€–gâ€–=7.97e-06\n",
      "Gen   72  best=1.7249e-05 minâ€–gâ€–=1.20e-05\n",
      "Gen   73  best=1.7249e-05 minâ€–gâ€–=1.46e-05\n",
      "Gen   74  best=1.7249e-05 minâ€–gâ€–=5.65e-05\n",
      "Gen   75  best=1.7249e-05 minâ€–gâ€–=2.79e-05\n",
      "Gen   76  best=1.7249e-05 minâ€–gâ€–=1.41e-05\n",
      "Gen   77  best=1.7249e-05 minâ€–gâ€–=1.44e-05\n",
      "Gen   78  best=1.7249e-05 minâ€–gâ€–=9.55e-06\n",
      "Gen   79  best=1.7249e-05 minâ€–gâ€–=6.89e-05\n",
      "Gen   80  best=1.7249e-05 minâ€–gâ€–=2.22e-04\n",
      "Gen   81  best=1.7249e-05 minâ€–gâ€–=9.56e-06\n",
      "Gen   82  best=1.7249e-05 minâ€–gâ€–=1.21e-05\n",
      "Gen   83  best=1.7249e-05 minâ€–gâ€–=1.09e-05\n",
      "Gen   84  best=1.7249e-05 minâ€–gâ€–=2.16e-05\n",
      "Gen   85  best=1.7249e-05 minâ€–gâ€–=3.19e-05\n",
      "Gen   86  best=1.7249e-05 minâ€–gâ€–=2.07e-05\n",
      "Gen   87  best=1.7249e-05 minâ€–gâ€–=3.61e-05\n",
      "Gen   88  best=1.7249e-05 minâ€–gâ€–=5.51e-06\n",
      "Gen   89  best=1.7249e-05 minâ€–gâ€–=4.12e-05\n",
      "Gen   90  best=1.7249e-05 minâ€–gâ€–=1.14e-04\n",
      "Gen   91  best=1.7249e-05 minâ€–gâ€–=6.82e-05\n",
      "Gen   92  best=1.7249e-05 minâ€–gâ€–=6.74e-05\n",
      "Gen   93  best=1.7249e-05 minâ€–gâ€–=1.67e-05\n",
      "Gen   94  best=1.7249e-05 minâ€–gâ€–=1.30e-05\n",
      "Gen   95  best=1.7249e-05 minâ€–gâ€–=1.38e-06\n",
      "Gen   96  best=1.2731e-05 minâ€–gâ€–=2.71e-05\n",
      "Gen   97  best=1.2731e-05 minâ€–gâ€–=2.49e-05\n",
      "Gen   98  best=1.2731e-05 minâ€–gâ€–=3.50e-05\n",
      "Gen   99  best=1.2731e-05 minâ€–gâ€–=7.11e-06\n",
      "Gen  100  best=1.2731e-05 minâ€–gâ€–=3.33e-06\n",
      "Gen  101  best=1.2731e-05 minâ€–gâ€–=5.27e-06\n",
      "Gen  102  best=1.2731e-05 minâ€–gâ€–=8.65e-06\n",
      "Gen  103  best=1.2731e-05 minâ€–gâ€–=4.40e-06\n",
      "Gen  104  best=1.2731e-05 minâ€–gâ€–=1.22e-05\n",
      "Gen  105  best=1.2731e-05 minâ€–gâ€–=3.13e-06\n",
      "Gen  106  best=1.2731e-05 minâ€–gâ€–=1.38e-05\n",
      "Gen  107  best=1.2731e-05 minâ€–gâ€–=9.90e-06\n",
      "Gen  108  best=1.2731e-05 minâ€–gâ€–=5.02e-06\n",
      "Gen  109  best=1.2731e-05 minâ€–gâ€–=1.29e-05\n",
      "Gen  110  best=1.2731e-05 minâ€–gâ€–=2.55e-05\n",
      "Gen  111  best=1.2731e-05 minâ€–gâ€–=2.71e-05\n",
      "Gen  112  best=1.2731e-05 minâ€–gâ€–=1.11e-05\n",
      "Gen  113  best=1.2731e-05 minâ€–gâ€–=7.11e-05\n",
      "Gen  114  best=1.2731e-05 minâ€–gâ€–=1.34e-06\n",
      "Gen  115  best=1.2731e-05 minâ€–gâ€–=4.72e-05\n",
      "Gen  116  best=1.2731e-05 minâ€–gâ€–=4.77e-05\n",
      "Gen  117  best=1.2731e-05 minâ€–gâ€–=1.56e-05\n",
      "Gen  118  best=1.2330e-05 minâ€–gâ€–=3.93e-06\n",
      "Gen  119  best=1.2330e-05 minâ€–gâ€–=4.54e-06\n",
      "Gen  120  best=1.0939e-05 minâ€–gâ€–=1.10e-05\n",
      "Gen  121  best=1.0939e-05 minâ€–gâ€–=1.24e-05\n",
      "Gen  122  best=1.0939e-05 minâ€–gâ€–=1.39e-05\n",
      "Gen  123  best=1.0939e-05 minâ€–gâ€–=3.09e-05\n",
      "Gen  124  best=1.0939e-05 minâ€–gâ€–=2.19e-05\n",
      "Gen  125  best=1.0939e-05 minâ€–gâ€–=4.40e-05\n",
      "Gen  126  best=1.0939e-05 minâ€–gâ€–=5.52e-06\n",
      "Gen  127  best=1.0939e-05 minâ€–gâ€–=9.57e-06\n",
      "Gen  128  best=1.0939e-05 minâ€–gâ€–=1.61e-06\n",
      "Gen  129  best=1.0939e-05 minâ€–gâ€–=2.51e-05\n",
      "Gen  130  best=1.0939e-05 minâ€–gâ€–=5.37e-06\n",
      "Gen  131  best=1.0939e-05 minâ€–gâ€–=1.77e-05\n",
      "Gen  132  best=1.0120e-05 minâ€–gâ€–=4.94e-05\n",
      "Gen  133  best=1.0120e-05 minâ€–gâ€–=1.63e-05\n",
      "Gen  134  best=1.0120e-05 minâ€–gâ€–=2.36e-05\n",
      "Gen  135  best=9.4456e-06 minâ€–gâ€–=5.74e-06\n",
      "Gen  136  best=9.4456e-06 minâ€–gâ€–=3.50e-05\n",
      "Gen  137  best=9.4456e-06 minâ€–gâ€–=7.78e-06\n",
      "Gen  138  best=9.4219e-06 minâ€–gâ€–=3.86e-06\n",
      "Gen  139  best=7.7633e-06 minâ€–gâ€–=1.62e-05\n",
      "Gen  140  best=7.7633e-06 minâ€–gâ€–=1.83e-05\n",
      "Gen  141  best=7.7633e-06 minâ€–gâ€–=5.04e-06\n",
      "Gen  142  best=7.7633e-06 minâ€–gâ€–=1.13e-05\n",
      "Gen  143  best=7.7633e-06 minâ€–gâ€–=1.81e-05\n",
      "Gen  144  best=7.7633e-06 minâ€–gâ€–=1.24e-05\n",
      "Gen  145  best=7.7633e-06 minâ€–gâ€–=3.06e-06\n",
      "Gen  146  best=7.7633e-06 minâ€–gâ€–=3.58e-06\n",
      "Gen  147  best=7.7633e-06 minâ€–gâ€–=2.11e-05\n",
      "Gen  148  best=7.7633e-06 minâ€–gâ€–=7.09e-06\n",
      "Gen  149  best=7.7633e-06 minâ€–gâ€–=8.87e-06\n",
      "Gen  150  best=7.7633e-06 minâ€–gâ€–=4.53e-06\n",
      "Gen  151  best=7.7397e-06 minâ€–gâ€–=2.96e-06\n",
      "Gen  152  best=7.7397e-06 minâ€–gâ€–=2.74e-05\n",
      "Gen  153  best=7.0428e-06 minâ€–gâ€–=1.54e-05\n",
      "Gen  154  best=7.0428e-06 minâ€–gâ€–=8.63e-06\n",
      "Gen  155  best=7.0428e-06 minâ€–gâ€–=1.75e-05\n",
      "Gen  156  best=7.0428e-06 minâ€–gâ€–=8.17e-06\n",
      "Gen  157  best=7.0428e-06 minâ€–gâ€–=8.90e-06\n",
      "Gen  158  best=7.0428e-06 minâ€–gâ€–=1.12e-05\n",
      "Gen  159  best=7.0428e-06 minâ€–gâ€–=7.41e-06\n",
      "Gen  160  best=7.0428e-06 minâ€–gâ€–=4.81e-06\n",
      "Gen  161  best=7.0428e-06 minâ€–gâ€–=8.63e-06\n",
      "Gen  162  best=7.0428e-06 minâ€–gâ€–=6.94e-06\n",
      "Gen  163  best=7.0428e-06 minâ€–gâ€–=3.24e-06\n",
      "Gen  164  best=7.0428e-06 minâ€–gâ€–=2.86e-06\n",
      "Gen  165  best=7.0428e-06 minâ€–gâ€–=1.97e-06\n",
      "Gen  166  best=6.8309e-06 minâ€–gâ€–=4.10e-06\n",
      "Gen  167  best=6.5976e-06 minâ€–gâ€–=1.29e-06\n",
      "Gen  168  best=6.2127e-06 minâ€–gâ€–=1.36e-06\n",
      "Gen  169  best=6.2127e-06 minâ€–gâ€–=5.01e-06\n",
      "Gen  170  best=6.2127e-06 minâ€–gâ€–=5.60e-06\n",
      "Gen  171  best=6.2127e-06 minâ€–gâ€–=6.49e-06\n",
      "Gen  172  best=6.2127e-06 minâ€–gâ€–=3.67e-05\n",
      "Gen  173  best=6.2127e-06 minâ€–gâ€–=6.98e-06\n",
      "Gen  174  best=6.2127e-06 minâ€–gâ€–=1.25e-05\n",
      "Gen  175  best=6.2127e-06 minâ€–gâ€–=9.82e-06\n",
      "Gen  176  best=6.2127e-06 minâ€–gâ€–=3.49e-06\n",
      "Gen  177  best=6.2127e-06 minâ€–gâ€–=9.24e-06\n",
      "Gen  178  best=6.2127e-06 minâ€–gâ€–=1.01e-05\n",
      "Gen  179  best=6.2127e-06 minâ€–gâ€–=4.61e-06\n",
      "Gen  180  best=6.2127e-06 minâ€–gâ€–=1.73e-05\n",
      "Gen  181  best=5.8728e-06 minâ€–gâ€–=5.64e-06\n",
      "Gen  182  best=5.8728e-06 minâ€–gâ€–=5.40e-06\n",
      "Gen  183  best=5.8728e-06 minâ€–gâ€–=3.27e-06\n",
      "Gen  184  best=5.8728e-06 minâ€–gâ€–=7.90e-06\n",
      "Gen  185  best=5.8728e-06 minâ€–gâ€–=2.68e-06\n",
      "Gen  186  best=5.8728e-06 minâ€–gâ€–=1.11e-06\n",
      "Gen  187  best=5.6696e-06 minâ€–gâ€–=1.68e-06\n",
      "Gen  188  best=5.6696e-06 minâ€–gâ€–=2.56e-06\n",
      "Gen  189  best=5.6696e-06 minâ€–gâ€–=1.77e-05\n",
      "Gen  190  best=5.6696e-06 minâ€–gâ€–=2.13e-06\n",
      "Gen  191  best=5.6696e-06 minâ€–gâ€–=7.91e-06\n",
      "Gen  192  best=5.6696e-06 minâ€–gâ€–=4.64e-06\n",
      "Gen  193  best=5.6696e-06 minâ€–gâ€–=7.30e-06\n",
      "Gen  194  best=5.6696e-06 minâ€–gâ€–=1.31e-05\n",
      "Gen  195  best=5.6696e-06 minâ€–gâ€–=6.56e-06\n",
      "Gen  196  best=5.6696e-06 minâ€–gâ€–=5.22e-06\n",
      "Gen  197  best=5.3816e-06 minâ€–gâ€–=8.82e-06\n",
      "Gen  198  best=5.3816e-06 minâ€–gâ€–=8.98e-06\n",
      "Gen  199  best=5.3816e-06 minâ€–gâ€–=3.07e-06\n",
      "Gen  200  best=5.3816e-06 minâ€–gâ€–=8.55e-07\n",
      "Gen  201  best=5.3816e-06 minâ€–gâ€–=3.36e-06\n",
      "Gen  202  best=5.3816e-06 minâ€–gâ€–=8.27e-06\n",
      "Gen  203  best=5.3816e-06 minâ€–gâ€–=1.64e-06\n",
      "Gen  204  best=5.3816e-06 minâ€–gâ€–=1.09e-06\n",
      "Gen  205  best=5.3816e-06 minâ€–gâ€–=2.65e-06\n",
      "Gen  206  best=5.3816e-06 minâ€–gâ€–=1.42e-06\n",
      "Gen  207  best=5.3816e-06 minâ€–gâ€–=2.19e-06\n",
      "Gen  208  best=5.3816e-06 minâ€–gâ€–=4.64e-06\n",
      "Gen  209  best=5.3816e-06 minâ€–gâ€–=2.12e-06\n",
      "Gen  210  best=5.3816e-06 minâ€–gâ€–=2.20e-06\n",
      "Gen  211  best=5.3816e-06 minâ€–gâ€–=5.72e-06\n",
      "Gen  212  best=5.3816e-06 minâ€–gâ€–=1.20e-06\n",
      "Gen  213  best=5.3816e-06 minâ€–gâ€–=2.09e-06\n",
      "Gen  214  best=5.3816e-06 minâ€–gâ€–=3.48e-06\n",
      "Gen  215  best=5.3816e-06 minâ€–gâ€–=4.30e-06\n",
      "Gen  216  best=5.3816e-06 minâ€–gâ€–=7.59e-06\n",
      "Gen  217  best=5.3515e-06 minâ€–gâ€–=8.61e-07\n",
      "Gen  218  best=5.3515e-06 minâ€–gâ€–=1.40e-06\n",
      "Gen  219  best=5.3357e-06 minâ€–gâ€–=2.96e-06\n",
      "Gen  220  best=5.3357e-06 minâ€–gâ€–=2.91e-06\n",
      "Gen  221  best=5.3041e-06 minâ€–gâ€–=1.68e-06\n",
      "Gen  222  best=5.3041e-06 minâ€–gâ€–=4.92e-06\n",
      "Gen  223  best=5.3041e-06 minâ€–gâ€–=2.33e-06\n",
      "Gen  224  best=5.3041e-06 minâ€–gâ€–=2.46e-06\n",
      "Gen  225  best=5.3041e-06 minâ€–gâ€–=3.28e-06\n",
      "Gen  226  best=4.9916e-06 minâ€–gâ€–=2.40e-06\n",
      "Gen  227  best=4.9916e-06 minâ€–gâ€–=2.33e-06\n",
      "Gen  228  best=4.9916e-06 minâ€–gâ€–=3.23e-06\n",
      "Gen  229  best=4.9916e-06 minâ€–gâ€–=2.35e-06\n",
      "Gen  230  best=4.9916e-06 minâ€–gâ€–=1.68e-06\n",
      "Gen  231  best=4.9916e-06 minâ€–gâ€–=2.05e-06\n",
      "Gen  232  best=4.9916e-06 minâ€–gâ€–=8.81e-06\n",
      "Gen  233  best=4.9916e-06 minâ€–gâ€–=6.07e-06\n",
      "Gen  234  best=4.9916e-06 minâ€–gâ€–=1.56e-06\n",
      "Gen  235  best=4.9916e-06 minâ€–gâ€–=7.20e-06\n",
      "Gen  236  best=4.9916e-06 minâ€–gâ€–=6.41e-06\n",
      "Gen  237  best=4.9916e-06 minâ€–gâ€–=3.40e-06\n",
      "Gen  238  best=4.9916e-06 minâ€–gâ€–=2.19e-06\n",
      "Gen  239  best=4.9916e-06 minâ€–gâ€–=2.03e-06\n",
      "Gen  240  best=4.9916e-06 minâ€–gâ€–=1.88e-06\n",
      "Gen  241  best=4.9916e-06 minâ€–gâ€–=2.11e-06\n",
      "Gen  242  best=4.9916e-06 minâ€–gâ€–=5.12e-06\n",
      "Gen  243  best=4.9916e-06 minâ€–gâ€–=1.70e-06\n",
      "Gen  244  best=4.9916e-06 minâ€–gâ€–=4.42e-06\n",
      "Gen  245  best=4.9916e-06 minâ€–gâ€–=1.96e-06\n",
      "Gen  246  best=4.9916e-06 minâ€–gâ€–=5.15e-06\n",
      "Gen  247  best=4.9916e-06 minâ€–gâ€–=1.45e-05\n",
      "Gen  248  best=4.9151e-06 minâ€–gâ€–=6.04e-06\n",
      "Gen  249  best=4.9151e-06 minâ€–gâ€–=6.97e-06\n",
      "Gen  250  best=4.9151e-06 minâ€–gâ€–=5.32e-06\n",
      "Gen  251  best=4.9151e-06 minâ€–gâ€–=7.39e-06\n",
      "Gen  252  best=4.9151e-06 minâ€–gâ€–=1.24e-05\n",
      "Gen  253  best=4.9151e-06 minâ€–gâ€–=2.03e-06\n",
      "Gen  254  best=4.7865e-06 minâ€–gâ€–=2.18e-06\n",
      "Gen  255  best=4.7865e-06 minâ€–gâ€–=3.91e-06\n",
      "Gen  256  best=4.7865e-06 minâ€–gâ€–=2.70e-06\n",
      "Gen  257  best=4.7865e-06 minâ€–gâ€–=6.20e-06\n",
      "Gen  258  best=4.7865e-06 minâ€–gâ€–=3.90e-06\n",
      "Gen  259  best=4.5896e-06 minâ€–gâ€–=2.88e-06\n",
      "Gen  260  best=4.5896e-06 minâ€–gâ€–=6.72e-06\n",
      "Gen  261  best=4.5308e-06 minâ€–gâ€–=1.15e-05\n",
      "Gen  262  best=4.4849e-06 minâ€–gâ€–=4.59e-06\n",
      "Gen  263  best=4.4849e-06 minâ€–gâ€–=1.96e-06\n",
      "Gen  264  best=4.2924e-06 minâ€–gâ€–=4.26e-06\n",
      "Gen  265  best=4.2377e-06 minâ€–gâ€–=7.71e-06\n",
      "Gen  266  best=4.2377e-06 minâ€–gâ€–=5.70e-06\n",
      "Gen  267  best=4.2377e-06 minâ€–gâ€–=3.53e-06\n",
      "Gen  268  best=4.2152e-06 minâ€–gâ€–=6.77e-06\n",
      "Gen  269  best=4.1360e-06 minâ€–gâ€–=6.37e-06\n",
      "Gen  270  best=4.1360e-06 minâ€–gâ€–=7.07e-06\n",
      "Gen  271  best=3.9868e-06 minâ€–gâ€–=2.06e-05\n",
      "Gen  272  best=3.9868e-06 minâ€–gâ€–=4.85e-06\n",
      "Gen  273  best=3.9868e-06 minâ€–gâ€–=7.78e-06\n",
      "Gen  274  best=3.9416e-06 minâ€–gâ€–=5.12e-06\n",
      "Gen  275  best=3.9416e-06 minâ€–gâ€–=2.25e-06\n",
      "Gen  276  best=3.9416e-06 minâ€–gâ€–=2.37e-06\n",
      "Gen  277  best=3.9416e-06 minâ€–gâ€–=2.44e-05\n",
      "Gen  278  best=3.9416e-06 minâ€–gâ€–=1.52e-06\n",
      "Gen  279  best=3.9416e-06 minâ€–gâ€–=1.01e-05\n",
      "Gen  280  best=3.8553e-06 minâ€–gâ€–=5.61e-06\n",
      "Gen  281  best=3.8553e-06 minâ€–gâ€–=3.88e-06\n",
      "Gen  282  best=3.8500e-06 minâ€–gâ€–=3.06e-06\n",
      "Gen  283  best=3.8500e-06 minâ€–gâ€–=6.47e-06\n",
      "Gen  284  best=3.8500e-06 minâ€–gâ€–=6.13e-06\n",
      "Gen  285  best=3.8309e-06 minâ€–gâ€–=3.14e-06\n",
      "Gen  286  best=3.8129e-06 minâ€–gâ€–=1.35e-05\n",
      "Gen  287  best=3.8018e-06 minâ€–gâ€–=1.91e-06\n",
      "Gen  288  best=3.6859e-06 minâ€–gâ€–=5.60e-06\n",
      "Gen  289  best=3.6859e-06 minâ€–gâ€–=4.77e-06\n",
      "Gen  290  best=3.6830e-06 minâ€–gâ€–=2.19e-05\n",
      "Gen  291  best=3.5522e-06 minâ€–gâ€–=4.32e-06\n",
      "Gen  292  best=3.5522e-06 minâ€–gâ€–=2.07e-06\n",
      "Gen  293  best=3.5522e-06 minâ€–gâ€–=2.96e-06\n",
      "Gen  294  best=3.5133e-06 minâ€–gâ€–=2.77e-06\n",
      "Gen  295  best=3.4500e-06 minâ€–gâ€–=4.44e-06\n",
      "Gen  296  best=3.4500e-06 minâ€–gâ€–=4.69e-06\n",
      "Gen  297  best=3.4500e-06 minâ€–gâ€–=2.93e-06\n",
      "Gen  298  best=3.4500e-06 minâ€–gâ€–=1.28e-06\n",
      "Gen  299  best=3.4500e-06 minâ€–gâ€–=5.59e-06\n",
      "Gen  300  best=3.4500e-06 minâ€–gâ€–=3.63e-06\n",
      "Gen  301  best=3.4500e-06 minâ€–gâ€–=1.12e-06\n",
      "Gen  302  best=3.4488e-06 minâ€–gâ€–=6.36e-06\n",
      "Gen  303  best=3.3988e-06 minâ€–gâ€–=2.00e-06\n",
      "Gen  304  best=3.3988e-06 minâ€–gâ€–=4.14e-06\n",
      "Gen  305  best=3.3988e-06 minâ€–gâ€–=2.60e-06\n",
      "Gen  306  best=3.3988e-06 minâ€–gâ€–=7.65e-06\n",
      "Gen  307  best=3.3988e-06 minâ€–gâ€–=7.75e-06\n",
      "Gen  308  best=3.3310e-06 minâ€–gâ€–=1.28e-05\n",
      "Gen  309  best=3.3007e-06 minâ€–gâ€–=3.44e-06\n",
      "Gen  310  best=3.3007e-06 minâ€–gâ€–=2.52e-06\n",
      "Gen  311  best=3.3007e-06 minâ€–gâ€–=1.82e-06\n",
      "Gen  312  best=3.3007e-06 minâ€–gâ€–=1.98e-06\n",
      "Gen  313  best=3.3007e-06 minâ€–gâ€–=2.07e-06\n",
      "Gen  314  best=3.3007e-06 minâ€–gâ€–=6.48e-07\n",
      "Gen  315  best=3.3007e-06 minâ€–gâ€–=1.43e-06\n",
      "Gen  316  best=3.3007e-06 minâ€–gâ€–=4.87e-06\n",
      "Gen  317  best=3.2707e-06 minâ€–gâ€–=7.01e-06\n",
      "Gen  318  best=3.2550e-06 minâ€–gâ€–=6.20e-06\n",
      "Gen  319  best=3.2550e-06 minâ€–gâ€–=2.74e-06\n",
      "Gen  320  best=3.2550e-06 minâ€–gâ€–=1.82e-06\n",
      "Gen  321  best=3.2042e-06 minâ€–gâ€–=2.49e-06\n",
      "Gen  322  best=3.2042e-06 minâ€–gâ€–=5.39e-06\n",
      "Gen  323  best=3.2042e-06 minâ€–gâ€–=4.26e-06\n",
      "Gen  324  best=3.2042e-06 minâ€–gâ€–=1.72e-05\n",
      "Gen  325  best=3.1661e-06 minâ€–gâ€–=1.17e-06\n",
      "Gen  326  best=3.1661e-06 minâ€–gâ€–=2.86e-06\n",
      "Gen  327  best=3.1303e-06 minâ€–gâ€–=3.04e-06\n",
      "Gen  328  best=3.1303e-06 minâ€–gâ€–=1.16e-06\n",
      "Gen  329  best=3.1303e-06 minâ€–gâ€–=8.91e-07\n",
      "Gen  330  best=3.1303e-06 minâ€–gâ€–=2.87e-06\n",
      "Gen  331  best=3.1303e-06 minâ€–gâ€–=1.19e-06\n",
      "Gen  332  best=3.1076e-06 minâ€–gâ€–=1.15e-06\n",
      "Gen  333  best=3.0184e-06 minâ€–gâ€–=4.24e-06\n",
      "Gen  334  best=3.0184e-06 minâ€–gâ€–=1.46e-06\n",
      "Gen  335  best=3.0184e-06 minâ€–gâ€–=2.70e-06\n",
      "Gen  336  best=3.0184e-06 minâ€–gâ€–=1.74e-06\n",
      "Gen  337  best=3.0184e-06 minâ€–gâ€–=1.80e-06\n",
      "Gen  338  best=3.0184e-06 minâ€–gâ€–=2.33e-06\n",
      "Gen  339  best=2.9176e-06 minâ€–gâ€–=1.86e-06\n",
      "Gen  340  best=2.9176e-06 minâ€–gâ€–=3.83e-06\n",
      "Gen  341  best=2.9176e-06 minâ€–gâ€–=3.52e-06\n",
      "Gen  342  best=2.9176e-06 minâ€–gâ€–=2.60e-06\n",
      "Gen  343  best=2.9176e-06 minâ€–gâ€–=2.31e-06\n",
      "Gen  344  best=2.9176e-06 minâ€–gâ€–=1.18e-06\n",
      "Gen  345  best=2.9176e-06 minâ€–gâ€–=2.45e-06\n",
      "Gen  346  best=2.9156e-06 minâ€–gâ€–=1.09e-06\n",
      "Gen  347  best=2.8868e-06 minâ€–gâ€–=2.60e-06\n",
      "Gen  348  best=2.8868e-06 minâ€–gâ€–=4.88e-06\n",
      "Gen  349  best=2.8311e-06 minâ€–gâ€–=2.25e-06\n",
      "Gen  350  best=2.8311e-06 minâ€–gâ€–=2.72e-06\n",
      "Gen  351  best=2.8311e-06 minâ€–gâ€–=1.47e-06\n",
      "Gen  352  best=2.8311e-06 minâ€–gâ€–=1.03e-06\n",
      "Gen  353  best=2.8311e-06 minâ€–gâ€–=1.95e-06\n",
      "Gen  354  best=2.8204e-06 minâ€–gâ€–=1.48e-06\n",
      "Gen  355  best=2.7964e-06 minâ€–gâ€–=1.11e-06\n",
      "Gen  356  best=2.7546e-06 minâ€–gâ€–=4.04e-06\n",
      "Gen  357  best=2.6822e-06 minâ€–gâ€–=7.14e-06\n",
      "Gen  358  best=2.6822e-06 minâ€–gâ€–=2.19e-06\n",
      "Gen  359  best=2.6822e-06 minâ€–gâ€–=1.18e-06\n",
      "Gen  360  best=2.6822e-06 minâ€–gâ€–=1.30e-06\n",
      "Gen  361  best=2.6764e-06 minâ€–gâ€–=2.10e-06\n",
      "Gen  362  best=2.6764e-06 minâ€–gâ€–=2.43e-06\n",
      "Gen  363  best=2.5927e-06 minâ€–gâ€–=8.52e-07\n",
      "Gen  364  best=2.5927e-06 minâ€–gâ€–=1.16e-06\n",
      "Gen  365  best=2.5571e-06 minâ€–gâ€–=4.02e-06\n",
      "Gen  366  best=2.5571e-06 minâ€–gâ€–=1.24e-06\n",
      "Gen  367  best=2.5571e-06 minâ€–gâ€–=1.25e-06\n",
      "Gen  368  best=2.5511e-06 minâ€–gâ€–=5.93e-06\n",
      "Gen  369  best=2.5509e-06 minâ€–gâ€–=1.38e-06\n",
      "Gen  370  best=2.5509e-06 minâ€–gâ€–=2.62e-06\n",
      "Gen  371  best=2.4967e-06 minâ€–gâ€–=1.33e-06\n",
      "Gen  372  best=2.3371e-06 minâ€–gâ€–=2.58e-06\n",
      "Gen  373  best=2.3371e-06 minâ€–gâ€–=1.41e-06\n",
      "Gen  374  best=2.3371e-06 minâ€–gâ€–=2.98e-06\n",
      "Gen  375  best=2.3371e-06 minâ€–gâ€–=6.32e-06\n",
      "Gen  376  best=2.3371e-06 minâ€–gâ€–=5.42e-06\n",
      "Gen  377  best=2.2940e-06 minâ€–gâ€–=3.31e-06\n",
      "Gen  378  best=2.2727e-06 minâ€–gâ€–=3.60e-06\n",
      "Gen  379  best=2.2673e-06 minâ€–gâ€–=9.17e-07\n",
      "Gen  380  best=2.2673e-06 minâ€–gâ€–=6.29e-07\n",
      "Gen  381  best=2.2673e-06 minâ€–gâ€–=2.73e-06\n",
      "Gen  382  best=2.2673e-06 minâ€–gâ€–=1.84e-06\n",
      "Gen  383  best=2.2673e-06 minâ€–gâ€–=3.28e-06\n",
      "Gen  384  best=2.2673e-06 minâ€–gâ€–=3.22e-06\n",
      "Gen  385  best=2.2673e-06 minâ€–gâ€–=6.33e-06\n",
      "Gen  386  best=2.2108e-06 minâ€–gâ€–=3.20e-06\n",
      "Gen  387  best=2.2108e-06 minâ€–gâ€–=4.89e-06\n",
      "Gen  388  best=2.1759e-06 minâ€–gâ€–=2.09e-06\n",
      "Gen  389  best=2.1759e-06 minâ€–gâ€–=2.45e-06\n",
      "Gen  390  best=2.1759e-06 minâ€–gâ€–=9.01e-07\n",
      "Gen  391  best=2.1759e-06 minâ€–gâ€–=5.35e-06\n",
      "Gen  392  best=2.1759e-06 minâ€–gâ€–=1.14e-06\n",
      "Gen  393  best=2.1745e-06 minâ€–gâ€–=1.09e-06\n",
      "Gen  394  best=2.1745e-06 minâ€–gâ€–=2.41e-06\n",
      "Gen  395  best=2.1745e-06 minâ€–gâ€–=4.63e-06\n",
      "Gen  396  best=2.1503e-06 minâ€–gâ€–=1.87e-06\n",
      "Gen  397  best=2.1503e-06 minâ€–gâ€–=1.17e-06\n",
      "Gen  398  best=2.1351e-06 minâ€–gâ€–=1.55e-06\n",
      "Gen  399  best=2.1351e-06 minâ€–gâ€–=2.47e-06\n",
      "Gen  400  best=2.1351e-06 minâ€–gâ€–=2.03e-06\n",
      "Gen  401  best=2.1111e-06 minâ€–gâ€–=5.97e-07\n",
      "Gen  402  best=2.1111e-06 minâ€–gâ€–=9.68e-07\n",
      "Gen  403  best=2.1111e-06 minâ€–gâ€–=9.74e-07\n",
      "Gen  404  best=2.1111e-06 minâ€–gâ€–=2.92e-06\n",
      "Gen  405  best=2.0992e-06 minâ€–gâ€–=3.75e-06\n",
      "Gen  406  best=2.0992e-06 minâ€–gâ€–=8.26e-07\n",
      "Gen  407  best=2.0992e-06 minâ€–gâ€–=7.96e-07\n",
      "Gen  408  best=2.0992e-06 minâ€–gâ€–=7.46e-07\n",
      "Gen  409  best=2.0992e-06 minâ€–gâ€–=6.27e-07\n",
      "Gen  410  best=2.0992e-06 minâ€–gâ€–=5.81e-07\n",
      "Gen  411  best=2.0992e-06 minâ€–gâ€–=1.27e-06\n",
      "Gen  412  best=2.0992e-06 minâ€–gâ€–=7.89e-07\n",
      "Gen  413  best=2.0992e-06 minâ€–gâ€–=1.18e-06\n",
      "Gen  414  best=2.0992e-06 minâ€–gâ€–=2.72e-06\n",
      "Gen  415  best=2.0992e-06 minâ€–gâ€–=6.84e-07\n",
      "Gen  416  best=2.0966e-06 minâ€–gâ€–=2.88e-06\n",
      "Gen  417  best=2.0966e-06 minâ€–gâ€–=2.07e-07\n",
      "Gen  418  best=2.0966e-06 minâ€–gâ€–=5.46e-07\n",
      "Gen  419  best=2.0966e-06 minâ€–gâ€–=1.36e-06\n",
      "Gen  420  best=2.0966e-06 minâ€–gâ€–=1.24e-06\n",
      "Gen  421  best=2.0966e-06 minâ€–gâ€–=1.60e-06\n",
      "Gen  422  best=2.0966e-06 minâ€–gâ€–=5.96e-07\n",
      "Gen  423  best=2.0966e-06 minâ€–gâ€–=1.20e-07\n",
      "Gen  424  best=2.0966e-06 minâ€–gâ€–=1.41e-06\n",
      "Gen  425  best=2.0966e-06 minâ€–gâ€–=1.06e-06\n",
      "Gen  426  best=2.0966e-06 minâ€–gâ€–=1.59e-06\n",
      "Gen  427  best=2.0966e-06 minâ€–gâ€–=9.71e-07\n",
      "Gen  428  best=2.0966e-06 minâ€–gâ€–=1.86e-07\n",
      "Gen  429  best=2.0966e-06 minâ€–gâ€–=1.02e-06\n",
      "Gen  430  best=2.0966e-06 minâ€–gâ€–=2.04e-07\n",
      "Gen  431  best=2.0966e-06 minâ€–gâ€–=5.20e-07\n",
      "Gen  432  best=2.0966e-06 minâ€–gâ€–=2.05e-07\n",
      "Gen  433  best=2.0966e-06 minâ€–gâ€–=1.63e-07\n",
      "Gen  434  best=2.0966e-06 minâ€–gâ€–=6.21e-08\n",
      "Gen  435  best=2.0932e-06 minâ€–gâ€–=1.70e-07\n",
      "Gen  436  best=2.0932e-06 minâ€–gâ€–=3.30e-07\n",
      "Gen  437  best=2.0932e-06 minâ€–gâ€–=1.79e-07\n",
      "Gen  438  best=2.0932e-06 minâ€–gâ€–=8.30e-07\n",
      "Gen  439  best=2.0932e-06 minâ€–gâ€–=4.14e-07\n",
      "Gen  440  best=2.0929e-06 minâ€–gâ€–=1.96e-07\n",
      "Gen  441  best=2.0929e-06 minâ€–gâ€–=7.22e-07\n",
      "Gen  442  best=2.0922e-06 minâ€–gâ€–=9.78e-07\n",
      "Gen  443  best=2.0922e-06 minâ€–gâ€–=3.74e-07\n",
      "Gen  444  best=2.0922e-06 minâ€–gâ€–=3.84e-07\n",
      "Gen  445  best=2.0922e-06 minâ€–gâ€–=1.49e-07\n",
      "Gen  446  best=2.0920e-06 minâ€–gâ€–=8.61e-08\n",
      "Gen  447  best=2.0920e-06 minâ€–gâ€–=4.14e-07\n",
      "Gen  448  best=2.0920e-06 minâ€–gâ€–=5.14e-08\n",
      "Gen  449  best=2.0920e-06 minâ€–gâ€–=1.74e-07\n",
      "Gen  450  best=2.0920e-06 minâ€–gâ€–=1.13e-07\n",
      "Gen  451  best=2.0920e-06 minâ€–gâ€–=1.74e-07\n",
      "Gen  452  best=2.0920e-06 minâ€–gâ€–=6.09e-08\n",
      "Gen  453  best=2.0917e-06 minâ€–gâ€–=3.31e-07\n",
      "Gen  454  best=2.0915e-06 minâ€–gâ€–=2.76e-07\n",
      "Gen  455  best=2.0915e-06 minâ€–gâ€–=4.73e-07\n",
      "Gen  456  best=2.0915e-06 minâ€–gâ€–=1.56e-07\n",
      "Gen  457  best=2.0915e-06 minâ€–gâ€–=1.92e-08\n",
      "Gen  458  best=2.0915e-06 minâ€–gâ€–=9.36e-08\n",
      "Gen  459  best=2.0915e-06 minâ€–gâ€–=2.40e-08\n",
      "Gen  460  best=2.0915e-06 minâ€–gâ€–=1.90e-07\n",
      "Gen  461  best=2.0913e-06 minâ€–gâ€–=7.11e-08\n",
      "Gen  462  best=2.0913e-06 minâ€–gâ€–=1.84e-07\n",
      "Gen  463  best=2.0913e-06 minâ€–gâ€–=1.62e-08\n",
      "Gen  464  best=2.0913e-06 minâ€–gâ€–=5.29e-08\n",
      "Gen  465  best=2.0913e-06 minâ€–gâ€–=3.65e-08\n",
      "Gen  466  best=2.0913e-06 minâ€–gâ€–=8.15e-08\n",
      "Gen  467  best=2.0913e-06 minâ€–gâ€–=5.12e-08\n",
      "Gen  468  best=2.0913e-06 minâ€–gâ€–=1.79e-07\n",
      "Gen  469  best=2.0913e-06 minâ€–gâ€–=6.32e-08\n",
      "Gen  470  best=2.0913e-06 minâ€–gâ€–=1.61e-07\n",
      "Gen  471  best=2.0911e-06 minâ€–gâ€–=3.13e-07\n",
      "Gen  472  best=2.0911e-06 minâ€–gâ€–=3.40e-08\n",
      "Gen  473  best=2.0911e-06 minâ€–gâ€–=1.03e-07\n",
      "Gen  474  best=2.0911e-06 minâ€–gâ€–=4.54e-08\n",
      "Gen  475  best=2.0911e-06 minâ€–gâ€–=1.34e-08\n",
      "Gen  476  best=2.0911e-06 minâ€–gâ€–=8.05e-08\n",
      "Gen  477  best=2.0911e-06 minâ€–gâ€–=2.46e-08\n",
      "Gen  478  best=2.0911e-06 minâ€–gâ€–=2.70e-08\n",
      "Gen  479  best=2.0909e-06 minâ€–gâ€–=7.24e-08\n",
      "Gen  480  best=2.0909e-06 minâ€–gâ€–=1.56e-07\n",
      "Gen  481  best=2.0909e-06 minâ€–gâ€–=9.36e-08\n",
      "Gen  482  best=2.0909e-06 minâ€–gâ€–=1.58e-07\n",
      "Gen  483  best=2.0909e-06 minâ€–gâ€–=4.16e-09\n",
      "Gen  484  best=2.0909e-06 minâ€–gâ€–=1.76e-08\n",
      "Gen  485  best=2.0909e-06 minâ€–gâ€–=3.60e-08\n",
      "Gen  486  best=2.0909e-06 minâ€–gâ€–=3.45e-08\n",
      "Gen  487  best=2.0908e-06 minâ€–gâ€–=4.82e-08\n",
      "Gen  488  best=2.0908e-06 minâ€–gâ€–=2.70e-07\n",
      "Gen  489  best=2.0908e-06 minâ€–gâ€–=9.46e-08\n",
      "Gen  490  best=2.0908e-06 minâ€–gâ€–=3.83e-08\n",
      "Gen  491  best=2.0908e-06 minâ€–gâ€–=1.66e-07\n",
      "Gen  492  best=2.0908e-06 minâ€–gâ€–=7.61e-08\n",
      "Gen  493  best=2.0908e-06 minâ€–gâ€–=6.36e-08\n",
      "Gen  494  best=2.0907e-06 minâ€–gâ€–=1.52e-08\n",
      "Gen  495  best=2.0907e-06 minâ€–gâ€–=5.07e-08\n",
      "Gen  496  best=2.0907e-06 minâ€–gâ€–=1.98e-08\n",
      "Gen  497  best=2.0907e-06 minâ€–gâ€–=1.40e-08\n",
      "Gen  498  best=2.0907e-06 minâ€–gâ€–=1.34e-08\n",
      "Gen  499  best=2.0907e-06 minâ€–gâ€–=3.54e-08\n",
      "Gen  500  best=2.0907e-06 minâ€–gâ€–=4.16e-08\n",
      "Gen  501  best=2.0907e-06 minâ€–gâ€–=4.72e-09\n",
      "Gen  502  best=2.0907e-06 minâ€–gâ€–=9.72e-08\n",
      "Gen  503  best=2.0907e-06 minâ€–gâ€–=2.97e-09\n",
      "Gen  504  best=2.0907e-06 minâ€–gâ€–=1.61e-08\n",
      "Gen  505  best=2.0907e-06 minâ€–gâ€–=1.61e-09\n",
      "Gen  506  best=2.0907e-06 minâ€–gâ€–=1.11e-08\n",
      "Gen  507  best=2.0907e-06 minâ€–gâ€–=1.11e-08\n",
      "Gen  508  best=2.0907e-06 minâ€–gâ€–=1.29e-08\n",
      "Gen  509  best=2.0907e-06 minâ€–gâ€–=1.61e-08\n",
      "Gen  510  best=2.0907e-06 minâ€–gâ€–=5.97e-09\n",
      "Gen  511  best=2.0907e-06 minâ€–gâ€–=1.42e-08\n",
      "Gen  512  best=2.0907e-06 minâ€–gâ€–=3.67e-08\n",
      "Gen  513  best=2.0907e-06 minâ€–gâ€–=2.69e-09\n",
      "Gen  514  best=2.0907e-06 minâ€–gâ€–=1.79e-08\n",
      "Gen  515  best=2.0907e-06 minâ€–gâ€–=1.10e-08\n",
      "Gen  516  best=2.0907e-06 minâ€–gâ€–=3.14e-08\n",
      "Gen  517  best=2.0907e-06 minâ€–gâ€–=3.82e-08\n",
      "Gen  518  best=2.0907e-06 minâ€–gâ€–=1.87e-08\n",
      "Gen  519  best=2.0907e-06 minâ€–gâ€–=9.34e-09\n",
      "Gen  520  best=2.0907e-06 minâ€–gâ€–=1.90e-08\n",
      "Gen  521  best=2.0907e-06 minâ€–gâ€–=2.61e-08\n",
      "Gen  522  best=2.0907e-06 minâ€–gâ€–=9.26e-09\n",
      "Gen  523  best=2.0907e-06 minâ€–gâ€–=6.55e-09\n",
      "Gen  524  best=2.0907e-06 minâ€–gâ€–=1.53e-08\n",
      "Gen  525  best=2.0907e-06 minâ€–gâ€–=4.71e-09\n",
      "Gen  526  best=2.0907e-06 minâ€–gâ€–=3.95e-08\n",
      "Gen  527  best=2.0907e-06 minâ€–gâ€–=1.04e-08\n",
      "Gen  528  best=2.0907e-06 minâ€–gâ€–=2.19e-08\n",
      "Gen  529  best=2.0907e-06 minâ€–gâ€–=1.97e-08\n",
      "Gen  530  best=2.0907e-06 minâ€–gâ€–=1.32e-08\n",
      "Gen  531  best=2.0907e-06 minâ€–gâ€–=2.41e-09\n",
      "Gen  532  best=2.0907e-06 minâ€–gâ€–=4.59e-09\n",
      "Gen  533  best=2.0907e-06 minâ€–gâ€–=9.37e-08\n",
      "Gen  534  best=2.0907e-06 minâ€–gâ€–=7.15e-09\n",
      "Gen  535  best=2.0907e-06 minâ€–gâ€–=5.08e-09\n",
      "Gen  536  best=2.0907e-06 minâ€–gâ€–=2.24e-08\n",
      "Gen  537  best=2.0907e-06 minâ€–gâ€–=4.11e-09\n",
      "Gen  538  best=2.0907e-06 minâ€–gâ€–=1.41e-08\n",
      "Gen  539  best=2.0907e-06 minâ€–gâ€–=8.90e-10\n",
      "Stopping: gradient norm below tol_g\n",
      "\n",
      "====================  DE RESULT  ====================\n",
      "best log-b : [-1.6830887  -0.34321049  0.72453579  1.63293205  2.50086158  3.38993398\n",
      "  4.32776802  5.33148355  6.41936298  7.61786141  8.97688886 10.71716555]\n",
      "best b     : [1.85799210e-01 7.09488855e-01 2.06377285e+00 5.11886150e+00\n",
      " 1.21929947e+01 2.96639938e+01 7.57749698e+01 2.06744463e+02\n",
      " 6.13612107e+02 2.03420714e+03 7.91795991e+03 4.51238202e+04]\n",
      "best a     : [0.2497941  0.28163879 0.16762354 0.10372011 0.06902224 0.04675113\n",
      " 0.03126864 0.02042429 0.01295091 0.00792139 0.00467292 0.00298887]\n",
      "DE score   : 2.0906946502112276e-06\n",
      "check obj  : 2.0906946501979364e-06\n",
      "generations: 539\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "#  TEST SCRIPT  â€”  optimise log-b with the same `ce_user_fn`\n",
    "###############################################################################\n",
    "\n",
    "# --- problem & helper already defined earlier ---------------------------\n",
    "# n_terms, ce_user_fn, d, w, target, obj â€¦ are assumed to be in scope\n",
    "\n",
    "pop_size = 20\n",
    "rng_seed = 1234\n",
    "\n",
    "# --- problem size -----------------------------------------------------------\n",
    "n_terms = 12          # K  (= length of b and a)\n",
    "newton_steps = 5     # how many Newton iterations per sample\n",
    "np.random.seed(1)    # reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  helper: CE user-defined update function operating on log-b only\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def ce_user_fn(b_log_samples: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b_log_samples : (N, K) ndarray\n",
    "        Current population in *log* space.\n",
    "    Returns\n",
    "    -------\n",
    "    updated_log_b : (N, K) ndarray\n",
    "        Polished & sorted log-b that CE will keep using.\n",
    "    scores        : (N,)  ndarray\n",
    "        Objective values after polishing (smaller is better).\n",
    "    info          : dict\n",
    "        Contains minimum gradient norm among all samples (for CE stopping).\n",
    "    \"\"\"\n",
    "    N, K = b_log_samples.shape\n",
    "    updated = np.empty_like(b_log_samples)\n",
    "    scores = np.empty(N)\n",
    "    grad_norm = np.empty(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        # ---- (1) build starting point --------------------------------------\n",
    "        b_pos = np.exp(b_log_samples[i])            # (K,)  positive\n",
    "        a_opt = optimal_a(d, w, target, b_pos)      # (K,)\n",
    "        p0 = np.concatenate([a_opt, b_pos])      # shape (2K,)\n",
    "\n",
    "        # ---- (2) limited Newton polishing ----------------------------------\n",
    "        p_new, gnorm, *_ = newton(\n",
    "            f=obj,\n",
    "            grad=grad,\n",
    "            hess=hess,\n",
    "            x0=p0,\n",
    "            max_iter=newton_steps,\n",
    "            tol_grad=1e-6,\n",
    "            tol_step=1e-6,\n",
    "            stall_iter=newton_steps,\n",
    "            ls_bounds=(-2., 2.),\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        if not np.any(np.isnan(p_new)) and not np.any(np.isinf(p_new)):\n",
    "            # ---- (3) extract & sort b (log) ------------------------------------\n",
    "            b_new_pos = np.clip(p_new[K:], 1e-12, None)  # keep positivity\n",
    "            b_new_log = np.log(b_new_pos)\n",
    "            order = np.argsort(b_new_log)\n",
    "            a_opt = optimal_a(d, w, target, b_new_pos)      # (K,)\n",
    "            p0 = np.concatenate([a_opt, b_new_pos])      # shape (2K,)\n",
    "\n",
    "            updated[i] = b_new_log[order]\n",
    "            scores[i] = obj(p0)       # score *after* polishing\n",
    "            grad_norm[i] = np.linalg.norm(grad(p0))\n",
    "        else:\n",
    "            # if polishing failed, keep the original sample\n",
    "            updated[i] = b_log_samples[i]\n",
    "            scores[i] = np.inf\n",
    "            grad_norm[i] = np.inf\n",
    "\n",
    "    info = {'normg': float(grad_norm.min())}\n",
    "    return updated, scores, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init_mean = np.linspace(-1.6, 10.3, n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 1.0)\n",
    "\n",
    "best_b_log, best_score, de_info = differential_evolution(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    F=0.8,\n",
    "    CR=0.9,\n",
    "    n_iters=5000,\n",
    "    random_state=rng_seed,\n",
    "    tol_g=1e-9,\n",
    "    stall_generations=75,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- final reconstruction of a and objective ----------------------------\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = obj(np.concatenate([best_a, best_b]))\n",
    "\n",
    "print(\"\\n====================  DE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"DE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"generations:\", de_info['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log(x[:n_terms]), label='a_i')\n",
    "plt.plot(np.log(x[n_terms:]), label='b_i')\n",
    "plt.xlabel('Parameter index')\n",
    "plt.ylabel('Parameter value (log scale for a_i)')\n",
    "plt.title('Optimised Parameters')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# Plot the convergence of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = grad(x)\n",
    "H = hess(x)\n",
    "d = np.linalg.solve(H, -g)  # Newton step\n",
    "dir_obj = lambda alpha: obj(x + alpha * d)\n",
    "alphas = np.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(alphas, [dir_obj(alpha) for alpha in alphas])\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Objective along Newton direction\")\n",
    "plt.title(\"Line search objective along Newton step\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae89cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 5.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "f_min_all = []\n",
    "min_b = -1.0\n",
    "max_b = 1.5\n",
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "for n_terms in range(2, 16):               # number of (a_i, b_i) pairs\n",
    "\n",
    "    b = np.linspace(min_b * 1.05, max_b * 1.1, n_terms)\n",
    "    a = optimal_a(d, w, target, np.exp(b))\n",
    "    # print(\"Optimal a:\", a)\n",
    "\n",
    "    means = np.concatenate((a, np.exp(b)))  # initial guess in log-space\n",
    "\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    x, grad_norm, f_history, grad_norm_history = newton(\n",
    "        f=obj,\n",
    "        grad=grad,\n",
    "        hess=hess,\n",
    "        x0=means,\n",
    "        tol_grad=1e-5,\n",
    "        tol_step=1e-8,\n",
    "        stall_iter=50,\n",
    "        max_iter=1000,\n",
    "        ls_bounds=(-2.0, 2.0)\n",
    "    )\n",
    "    min_b = min(np.log(x[n_terms:]))\n",
    "    max_b = max(np.log(x[n_terms:]))\n",
    "    print(f\"Min a log: {min_b}, Max b log: {max_b}\")\n",
    "    print(\"Final objective value:\", f_history[-1])\n",
    "    f_min_all.append(f_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f_min_all)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "\n",
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "best_init = np.array([0,0])\n",
    "\n",
    "for n_terms in range(1,10):                 # number of (a_i, b_i) pairs\n",
    "    means = best_init  # initial mean for (a_i, b_i)\n",
    "    cov = np.eye(2 * n_terms) * 4  # initial covariance matrix\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    best, stats = cross_entropy_numba(\n",
    "        d=d, target=target, w=w,\n",
    "        n_terms=n_terms,\n",
    "        max_iter=10000,\n",
    "        pop_size=10000,\n",
    "        elite_frac=0.1,\n",
    "        mean=means,\n",
    "        cov=cov\n",
    "    )\n",
    "\n",
    "    print(\"Best score :\", stats[\"best_score\"])\n",
    "    print(\"Best params:\", best)\n",
    "    print(\"Iterations :\", stats[\"iterations\"])\n",
    "    print(\"Runtime    :\", stats[\"runtime\"], \"s\")\n",
    "\n",
    "    first_half = best[:n_terms]\n",
    "    second_half = best[n_terms:]\n",
    "\n",
    "    # Original equidistant indices (0 to n_terms-1), new indices from 0 to n_terms\n",
    "    x_old = np.linspace(0, 1, n_terms)\n",
    "    x_new = np.linspace(0, 1, n_terms + 1)\n",
    "\n",
    "    interp_first = np.interp(x_new, x_old, first_half)\n",
    "    interp_second = np.interp(x_new, x_old, second_half)\n",
    "\n",
    "    best_init = np.hstack([interp_first, interp_second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d75dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(stats[\"history\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.title(\"Convergence of the Cross-Entropy Method\")\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best[n_terms:], 'o-')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"b_i (log scale)\")\n",
    "plt.title(\"Fitted b_i parameters\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f276bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import lsq_linear\n",
    "\n",
    "# compute using least squares the coefficients for approximation with squared exponential kernels\n",
    "# Construct the design matrix for squared exponential kernels\n",
    "A = np.zeros((len(d), n_terms))\n",
    "b_params = np.exp(best[n_terms:])\n",
    "\n",
    "for i in range(n_terms):\n",
    "    A[:, i] = np.exp(-b_params[i] * d**2)\n",
    "\n",
    "# Solve the weighted least squares problem: minimize ||W(Aa - target)||^2\n",
    "W = np.sqrt(w)\n",
    "Aw = A * W[:, None]\n",
    "tw = target * W\n",
    "\n",
    "res = lsq_linear(Aw, tw, bounds=(0, np.inf))\n",
    "squared_exponential_coeffs = res.x\n",
    "print(\"Squared exponential coefficients:\", np.log(squared_exponential_coeffs))\n",
    "\n",
    "# compute fit with this coefficients\n",
    "params = np.hstack([np.log(squared_exponential_coeffs), np.log(b_params)])\n",
    "\n",
    "res = objective_numba(np.array([params]), d, target, w)\n",
    "print(\"Objective value with least squares coefficients:\", res[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "f = lambda t: np.exp(-t)\n",
    "a, b, info = fit_exp_sum_ce(5, x, w, f, iterations=2000, pop_size=1000)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('best_score:', info.best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
