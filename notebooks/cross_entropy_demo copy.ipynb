{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy kernel fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path('..') / 'src'))\n",
    "from kl_decomposition import rectangle_rule\n",
    "from kl_decomposition.kernel_fit import fit_exp_sum_ce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def objective_numba(params: np.ndarray,\n",
    "                    d: np.ndarray,\n",
    "                    target: np.ndarray,\n",
    "                    w: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorised objective for K parameter sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params  : (K, 2*n_terms) log-space parameters\n",
    "    d       : (N,)           data values (distances); will use d**2\n",
    "    target  : (N,)           reference curve\n",
    "    w       : (N,)           weights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    obj_val : (K,)           objective for each parameter row\n",
    "    \"\"\"\n",
    "    K, P = params.shape\n",
    "    n_terms = P // 2\n",
    "    N = d.size\n",
    "    d2 = d * d                      # pre-compute d²\n",
    "    out = np.empty(K, dtype=params.dtype)\n",
    "\n",
    "    for k in prange(K):                  # loop over parameter sets in parallel\n",
    "        # split, exponentiate (back to linear scale)\n",
    "        a = np.exp(params[k, :n_terms])\n",
    "        b = np.exp(params[k, n_terms:])\n",
    "\n",
    "        acc = 0.0                        # accumulate Σ w*(pred-target)²\n",
    "        for j in range(N):               # loop over data points\n",
    "            dj2 = d2[j]\n",
    "            predj = 0.0\n",
    "            for i in range(n_terms):     # Σ_i a_i * exp(-b_i * d²)\n",
    "                predj += a[i] * np.exp(-b[i] * dj2)\n",
    "            diff = predj - target[j]\n",
    "            acc += w[j] * diff * diff\n",
    "\n",
    "        out[k] = acc\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8852ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Tuple, Dict, Any, Optional\n",
    "\n",
    "\n",
    "def cross_entropy(\n",
    "    user_fn: Callable[[np.ndarray], Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],\n",
    "    initial_mean: np.ndarray,\n",
    "    initial_std: np.ndarray,\n",
    "    pop_size: int = 100,\n",
    "    elite_frac: float = 0.2,\n",
    "    alpha_mean: float = 0.7,\n",
    "    alpha_std: float = 0.7,\n",
    "    n_iters: int = 100,\n",
    "    random_state: Optional[int] = None,\n",
    "    verbose: bool = False,\n",
    "    tol_std: float = 1e-6,\n",
    "    tol_g: float = 1e-6\n",
    ") -> Tuple[np.ndarray, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Method with independent Gaussian distribution and stopping criteria.\n",
    "\n",
    "    Args:\n",
    "        user_fn: function taking samples (shape Nxd) and returning\n",
    "            (updated_samples, scores, info_dict)\n",
    "        initial_mean: initial mean vector (d,)\n",
    "        initial_std: initial std deviation vector (d,)\n",
    "        pop_size: number of samples per iteration\n",
    "        elite_frac: fraction of samples selected as elites\n",
    "        alpha_mean: learning rate for mean update\n",
    "        alpha_std: learning rate for std update\n",
    "        n_iters: maximum number of iterations\n",
    "        random_state: RNG seed for reproducibility\n",
    "        verbose: print debug info each iteration\n",
    "        tol_std: stop if max(std) <= tol_std\n",
    "        tol_g: stop if info_dict['normg'] <= tol_g\n",
    "\n",
    "    Returns:\n",
    "        best_sample: best found sample (d,)\n",
    "        best_score: best objective value\n",
    "        info: dict with:\n",
    "            'iter': iteration index at stop\n",
    "            'best_score_history': list of best_score after each iteration\n",
    "            'max_std_history': list of max_std after each iteration\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    mean = np.array(initial_mean, dtype=float)\n",
    "    std = np.array(initial_std, dtype=float)\n",
    "    n_elite = max(1, int(np.ceil(pop_size * elite_frac)))\n",
    "    best_sample: Optional[np.ndarray] = None\n",
    "    best_score = np.infb\n",
    "\n",
    "    # history lists\n",
    "    best_score_history: list = []\n",
    "    max_std_history: list = []\n",
    "\n",
    "    for iteration in range(n_iters):\n",
    "        samples = rng.randn(pop_size, mean.size) * std + mean\n",
    "        samples, scores, info_dict = user_fn(samples)\n",
    "        idx = int(np.argmin(scores))\n",
    "        # update best if improved\n",
    "        if scores[idx] < best_score:\n",
    "            best_score = float(scores[idx])\n",
    "            best_sample = samples[idx]\n",
    "        # select elites and update distribution\n",
    "        elite = samples[np.argsort(scores)[:n_elite]]\n",
    "        elite_mean = elite.mean(axis=0)\n",
    "        elite_std = elite.std(axis=0, ddof=0)\n",
    "        mean = alpha_mean * elite_mean + (1 - alpha_mean) * mean\n",
    "        std = alpha_std * elite_std + (1 - alpha_std) * std\n",
    "        # record history\n",
    "        max_std = float(np.max(std))\n",
    "        best_score_history.append(best_score)\n",
    "        max_std_history.append(max_std)\n",
    "        # debug print\n",
    "        if verbose:\n",
    "            print(f\"Iter {iteration}: best_score={best_score}, max_std={max_std}, info={info_dict}\")\n",
    "        # stopping criteria\n",
    "        if max_std <= tol_std:\n",
    "            break\n",
    "        if info_dict.get('normg', np.inf) <= tol_g:\n",
    "            break\n",
    "\n",
    "    info = {\n",
    "        'iter': iteration,\n",
    "        'best_score_history': best_score_history,\n",
    "        'max_std_history': max_std_history\n",
    "    }\n",
    "    return best_sample, best_score, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84814de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from kl_decomposition.kernel_fit import rectangle_rule\n",
    "\n",
    "# Enable 64-bit (double) precision in JAX\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# setup squared exponential approximation\n",
    "x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "target = np.exp(-x)\n",
    "\n",
    "# Convert to JAX arrays with float64 dtype\n",
    "x_j = jnp.array(x, dtype=jnp.float64)\n",
    "w_j = jnp.array(w, dtype=jnp.float64)\n",
    "target_j = jnp.array(target, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def obj_jax(p):\n",
    "    # p is already float64\n",
    "    n_param = p.shape[0] // 2\n",
    "    a = p[:n_param]\n",
    "    b = p[n_param:]\n",
    "    # compute prediction and weighted RMS error\n",
    "    pred = jnp.sum(a[:, None] * jnp.exp(-b[:, None] * x_j[None, :] ** 2), axis=0)\n",
    "    diff = pred - target_j\n",
    "    return jnp.sqrt(jnp.sum(w_j * diff ** 2))\n",
    "\n",
    "\n",
    "# gradients and Hessians in double precision\n",
    "grad_jax = jax.jit(jax.grad(obj_jax))\n",
    "hess_jax = jax.jit(jax.hessian(obj_jax))\n",
    "obj_jax_jit = jax.jit(obj_jax)\n",
    "\n",
    "\n",
    "def obj(p):\n",
    "    # ensure NumPy→JAX conversion to float64\n",
    "    return float(obj_jax_jit(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def grad(p):\n",
    "    return np.array(grad_jax(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def hess(p):\n",
    "    return np.array(hess_jax(jnp.array(p, dtype=jnp.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def golden_section_search(f, a=0.0, b=1.0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Golden-section search to find the minimum of a unimodal function f on [a, b].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        The objective function to minimize.\n",
    "    a : float\n",
    "        Left endpoint of the initial interval.\n",
    "    b : float\n",
    "        Right endpoint of the initial interval.\n",
    "    tol : float\n",
    "        Tolerance for the interval width (stopping criterion).\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_min : float\n",
    "        Estimated position of the minimum.\n",
    "    f_min : float\n",
    "        Value of f at x_min.\n",
    "    \"\"\"\n",
    "    # Golden ratio constant\n",
    "    phi = (np.sqrt(5.0) - 1) / 2\n",
    "\n",
    "    # Initialize interior points\n",
    "    c = b - phi * (b - a)\n",
    "    d = a + phi * (b - a)\n",
    "    f_c = f(c)\n",
    "    f_d = f(d)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        if abs(b - a) < tol:\n",
    "            break\n",
    "\n",
    "        if f_c < f_d:\n",
    "            b, d, f_d = d, c, f_c\n",
    "            c = b - phi * (b - a)\n",
    "            f_c = f(c)\n",
    "        else:\n",
    "            a, c, f_c = c, d, f_d\n",
    "            d = a + phi * (b - a)\n",
    "            f_d = f(d)\n",
    "\n",
    "    # Choose the best of the final points\n",
    "    if f_c < f_d:\n",
    "        x_min, f_min = c, f_c\n",
    "    else:\n",
    "        x_min, f_min = d, f_d\n",
    "    # print(f\"Iterations: {_ + 1}\")\n",
    "    return x_min, f_min\n",
    "\n",
    "\n",
    "def newton(f, grad, hess, x0,\n",
    "                                   tol_grad=1e-6,\n",
    "                                   tol_step=1e-8,\n",
    "                                   stall_iter=5,\n",
    "                                   max_iter=100,\n",
    "                                   ls_bounds=(-1.0, 1.0),\n",
    "                                   verbose=False):\n",
    "    \"\"\"\n",
    "    Newton's method with backtracking line search and fallback to gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Objective function f(x).\n",
    "    grad : callable\n",
    "        Gradient of f: grad(x).\n",
    "    hess : callable\n",
    "        Hessian of f: hess(x).\n",
    "    x0 : ndarray\n",
    "        Initial guess.\n",
    "    tol_grad : float, optional\n",
    "        Tolerance for gradient norm.\n",
    "    tol_step : float, optional\n",
    "        Tolerance for step size norm for stall criterion.\n",
    "    stall_iter : int, optional\n",
    "        Number of consecutive small steps to trigger stop.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations.\n",
    "    ls_bounds : (float, float), optional\n",
    "        Initial interval for line search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : ndarray\n",
    "        Estimated minimizer.\n",
    "    grad_norm : float\n",
    "        Norm of gradient at the solution.\n",
    "    f_history : list of float\n",
    "        History of f values.\n",
    "    grad_norm_history : list of float\n",
    "        History of gradient norms.\n",
    "    \"\"\"\n",
    "    x = x0.astype(float)\n",
    "    f_history = []\n",
    "    grad_norm_history = []\n",
    "    fx = f(x)\n",
    "\n",
    "    stall_count = 0\n",
    "    trust = 1e-6 * np.eye(len(x))  # trust region to ensure positive definiteness\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        gx = grad(x)\n",
    "        grad_norm = np.linalg.norm(gx)\n",
    "\n",
    "        f_history.append(fx)\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "        # Check gradient convergence\n",
    "        if grad_norm < tol_grad:\n",
    "            break\n",
    "\n",
    "        # Try Newton step\n",
    "        try:\n",
    "            Hx = hess(x)\n",
    "            p = -np.linalg.solve(Hx + trust, gx)\n",
    "            trust /= 2\n",
    "            # ensure descent direction\n",
    "            # if np.dot(p, gx) >= 0:\n",
    "            #     raise np.linalg.LinAlgError\n",
    "        except np.linalg.LinAlgError:\n",
    "            # fallback to steepest descent\n",
    "            print(\"Hessian not positive definite, using gradient descent\")\n",
    "            p = -gx\n",
    "            trust *= 2\n",
    "        # Backtracking line search\n",
    "        alpha, fx = golden_section_search(\n",
    "            lambda alpha: f(x + alpha * p),\n",
    "            a=ls_bounds[0], b=ls_bounds[1], tol=1e-6, max_iter=100\n",
    "        )\n",
    "\n",
    "        # Update\n",
    "        x_new = x + alpha * p\n",
    "        step_norm = np.linalg.norm(x_new - x)\n",
    "        x = x_new\n",
    "\n",
    "        # Stall criterion\n",
    "        if step_norm < tol_step:\n",
    "            stall_count += 1\n",
    "            if stall_count >= stall_iter:\n",
    "                break\n",
    "        else:\n",
    "            stall_count = 0\n",
    "\n",
    "        # print debug info\n",
    "        if verbose:\n",
    "            print(f\"Iter {k}: f={fx:.6e}, grad_norm={grad_norm:.6e}, step_norm={step_norm:.6e}, alpha={alpha:.6f}\")\n",
    "\n",
    "    return x, grad_norm, f_history, grad_norm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "\n",
    "def optimal_a(d: np.ndarray,\n",
    "              w: np.ndarray,\n",
    "              target: np.ndarray,\n",
    "              b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve for a >= 0 minimizing\n",
    "        sum_j w[j] * (sum_k a[k] * exp(-b[k] * d[j]**2) - target[j])**2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : (J,) array\n",
    "        Sample points x_j.\n",
    "    w : (J,) array\n",
    "        Quadrature weights.\n",
    "    target : (J,) array\n",
    "        Target values at each d[j].\n",
    "    b : (K,) array\n",
    "        Exponents b_k.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : (K,) array\n",
    "        Non-negative coefficient vector minimizing the weighted LS error.\n",
    "    \"\"\"\n",
    "    # Build design matrix Φ[j,k] = exp(-b[k] * d[j]**2)\n",
    "    # Note: np.outer(d**2, b) yields shape (J, K)\n",
    "    Phi = np.exp(-np.outer(d**2, b))     # shape (J, K)\n",
    "\n",
    "    # Incorporate weights by scaling rows\n",
    "    W_sqrt = np.sqrt(w)                  # shape (J,)\n",
    "    Phi_w = Phi * W_sqrt[:, None]        # shape (J, K)\n",
    "    y_w = target * W_sqrt              # shape (J,)\n",
    "\n",
    "    a, _ = nnls(Phi_w, y_w)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270cb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- synthetic data ---------------------------------------------------\n",
    "n_terms = 8               # number of (a_i, b_i) pairs\n",
    "N       = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "\n",
    "b = np.linspace(-2, 14.5, n_terms)\n",
    "a = optimal_a(d, w, target, np.exp(b))\n",
    "print(\"Optimal a:\", a)\n",
    "\n",
    "means = np.concatenate((a, np.exp(b)))  # initial guess in log-space\n",
    "\n",
    "# --- optimiser call ---------------------------------------------------\n",
    "x, grad_norm, f_history, grad_norm_history = newton(\n",
    "    f=obj,\n",
    "    grad=grad,\n",
    "    hess=hess,\n",
    "    x0=means,\n",
    "    tol_grad=1e-5,\n",
    "    tol_step=1e-8,\n",
    "    stall_iter=50,\n",
    "    max_iter=3000,\n",
    "    ls_bounds=(-2.0, 2.0)\n",
    ")\n",
    "\n",
    "print(\"Optimised parameters:\", x)\n",
    "print(\"Final gradient norm:\", grad_norm)\n",
    "print(\"Final objective value:\", f_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log(x[:n_terms]), label='a_i')\n",
    "plt.plot(np.log(x[n_terms:]), label='b_i')\n",
    "plt.xlabel('Parameter index')\n",
    "plt.ylabel('Parameter value (log scale for a_i)')\n",
    "plt.title('Optimised Parameters')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# Plot the convergence of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = grad(x)\n",
    "H = hess(x)\n",
    "d = np.linalg.solve(H, -g)  # Newton step\n",
    "dir_obj = lambda alpha: obj(x + alpha * d)\n",
    "alphas = np.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(alphas, [dir_obj(alpha) for alpha in alphas])\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Objective along Newton direction\")\n",
    "plt.title(\"Line search objective along Newton step\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae89cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 5.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "f_min_all = []\n",
    "min_b = -1.0\n",
    "max_b = 1.5\n",
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "for n_terms in range(2, 16):               # number of (a_i, b_i) pairs\n",
    "\n",
    "    b = np.linspace(min_b * 1.05, max_b * 1.1, n_terms)\n",
    "    a = optimal_a(d, w, target, np.exp(b))\n",
    "    # print(\"Optimal a:\", a)\n",
    "\n",
    "    means = np.concatenate((a, np.exp(b)))  # initial guess in log-space\n",
    "\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    x, grad_norm, f_history, grad_norm_history = newton(\n",
    "        f=obj,\n",
    "        grad=grad,\n",
    "        hess=hess,\n",
    "        x0=means,\n",
    "        tol_grad=1e-5,\n",
    "        tol_step=1e-8,\n",
    "        stall_iter=50,\n",
    "        max_iter=1000,\n",
    "        ls_bounds=(-2.0, 2.0)\n",
    "    )\n",
    "    min_b = min(np.log(x[n_terms:]))\n",
    "    max_b = max(np.log(x[n_terms:]))\n",
    "    print(f\"Min a log: {min_b}, Max b log: {max_b}\")\n",
    "    print(\"Final objective value:\", f_history[-1])\n",
    "    f_min_all.append(f_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f_min_all)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "\n",
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "best_init = np.array([0,0])\n",
    "\n",
    "for n_terms in range(1,10):                 # number of (a_i, b_i) pairs\n",
    "    means = best_init  # initial mean for (a_i, b_i)\n",
    "    cov = np.eye(2 * n_terms) * 4  # initial covariance matrix\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    best, stats = cross_entropy_numba(\n",
    "        d=d, target=target, w=w,\n",
    "        n_terms=n_terms,\n",
    "        max_iter=10000,\n",
    "        pop_size=10000,\n",
    "        elite_frac=0.1,\n",
    "        mean=means,\n",
    "        cov=cov\n",
    "    )\n",
    "\n",
    "    print(\"Best score :\", stats[\"best_score\"])\n",
    "    print(\"Best params:\", best)\n",
    "    print(\"Iterations :\", stats[\"iterations\"])\n",
    "    print(\"Runtime    :\", stats[\"runtime\"], \"s\")\n",
    "\n",
    "    first_half = best[:n_terms]\n",
    "    second_half = best[n_terms:]\n",
    "\n",
    "    # Original equidistant indices (0 to n_terms-1), new indices from 0 to n_terms\n",
    "    x_old = np.linspace(0, 1, n_terms)\n",
    "    x_new = np.linspace(0, 1, n_terms + 1)\n",
    "\n",
    "    interp_first = np.interp(x_new, x_old, first_half)\n",
    "    interp_second = np.interp(x_new, x_old, second_half)\n",
    "\n",
    "    best_init = np.hstack([interp_first, interp_second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d75dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(stats[\"history\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.title(\"Convergence of the Cross-Entropy Method\")\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best[n_terms:], 'o-')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"b_i (log scale)\")\n",
    "plt.title(\"Fitted b_i parameters\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f276bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import lsq_linear\n",
    "\n",
    "# compute using least squares the coefficients for approximation with squared exponential kernels\n",
    "# Construct the design matrix for squared exponential kernels\n",
    "A = np.zeros((len(d), n_terms))\n",
    "b_params = np.exp(best[n_terms:])\n",
    "\n",
    "for i in range(n_terms):\n",
    "    A[:, i] = np.exp(-b_params[i] * d**2)\n",
    "\n",
    "# Solve the weighted least squares problem: minimize ||W(Aa - target)||^2\n",
    "W = np.sqrt(w)\n",
    "Aw = A * W[:, None]\n",
    "tw = target * W\n",
    "\n",
    "res = lsq_linear(Aw, tw, bounds=(0, np.inf))\n",
    "squared_exponential_coeffs = res.x\n",
    "print(\"Squared exponential coefficients:\", np.log(squared_exponential_coeffs))\n",
    "\n",
    "# compute fit with this coefficients\n",
    "params = np.hstack([np.log(squared_exponential_coeffs), np.log(b_params)])\n",
    "\n",
    "res = objective_numba(np.array([params]), d, target, w)\n",
    "print(\"Objective value with least squares coefficients:\", res[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "f = lambda t: np.exp(-t)\n",
    "a, b, info = fit_exp_sum_ce(5, x, w, f, iterations=2000, pop_size=1000)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('best_score:', info.best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
