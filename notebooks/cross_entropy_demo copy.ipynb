{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy kernel fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from typing import Callable, Tuple, Dict, Any, Optional\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path('..') / 'src'))\n",
    "# Enable 64-bit (double) precision in JAX\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3602bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kl_decomposition import rectangle_rule, gauss_legendre_rule_multilevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8852ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(\n",
    "    user_fn: Callable[[np.ndarray], Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],\n",
    "    initial_mean: np.ndarray,\n",
    "    initial_std: np.ndarray,\n",
    "    pop_size: int = 100,\n",
    "    elite_frac: float = 0.2,\n",
    "    alpha_mean: float = 0.7,\n",
    "    alpha_std: float = 0.7,\n",
    "    n_iters: int = 100,\n",
    "    random_state: Optional[int] = None,\n",
    "    verbose: bool = False,\n",
    "    tol_std: float = 1e-6,\n",
    "    tol_g: float = 1e-6\n",
    ") -> Tuple[np.ndarray, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Cross-Entropy Method with independent Gaussian distribution and stopping criteria.\n",
    "\n",
    "    Args:\n",
    "        user_fn: function taking samples (shape Nxd) and returning\n",
    "            (updated_samples, scores, info_dict)\n",
    "        initial_mean: initial mean vector (d,)\n",
    "        initial_std: initial std deviation vector (d,)\n",
    "        pop_size: number of samples per iteration\n",
    "        elite_frac: fraction of samples selected as elites\n",
    "        alpha_mean: learning rate for mean update\n",
    "        alpha_std: learning rate for std update\n",
    "        n_iters: maximum number of iterations\n",
    "        random_state: RNG seed for reproducibility\n",
    "        verbose: print debug info each iteration\n",
    "        tol_std: stop if max(std) <= tol_std\n",
    "        tol_g: stop if info_dict['normg'] <= tol_g\n",
    "\n",
    "    Returns:\n",
    "        best_sample: best found sample (d,)\n",
    "        best_score: best objective value\n",
    "        info: dict with:\n",
    "            'iter': iteration index at stop\n",
    "            'best_score_history': list of best_score after each iteration\n",
    "            'max_std_history': list of max_std after each iteration\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    mean = np.array(initial_mean, dtype=float)\n",
    "    std = np.array(initial_std, dtype=float)\n",
    "    n_elite = max(1, int(np.ceil(pop_size * elite_frac)))\n",
    "    best_sample: Optional[np.ndarray] = None\n",
    "    best_score = np.inf\n",
    "\n",
    "    # history lists\n",
    "    best_score_history: list = []\n",
    "    max_std_history: list = []\n",
    "\n",
    "    for iteration in range(n_iters):\n",
    "        samples = rng.randn(pop_size, mean.size) * std + mean\n",
    "        samples, scores, info_dict = user_fn(samples)\n",
    "        idx = int(np.argmin(scores))\n",
    "        # update best if improved\n",
    "        if scores[idx] < best_score:\n",
    "            best_score = float(scores[idx])\n",
    "            best_sample = samples[idx]\n",
    "        # select elites and update distribution\n",
    "        elite = samples[np.argsort(scores)[:n_elite]]\n",
    "        elite_mean = elite.mean(axis=0)\n",
    "        elite_std = elite.std(axis=0, ddof=0)\n",
    "        mean = alpha_mean * elite_mean + (1 - alpha_mean) * mean\n",
    "        std = alpha_std * elite_std + (1 - alpha_std) * std\n",
    "        # record history\n",
    "        max_std = float(np.max(std))\n",
    "        best_score_history.append(best_score)\n",
    "        max_std_history.append(max_std)\n",
    "        # debug print\n",
    "        if verbose:\n",
    "            print(f\"Iter {iteration}: best_score={best_score}, max_std={max_std}, info={info_dict}\")\n",
    "        # stopping criteria\n",
    "        if max_std <= tol_std:\n",
    "            break\n",
    "        if info_dict.get('normg', np.inf) <= tol_g:\n",
    "            break\n",
    "\n",
    "    info = {\n",
    "        'iter': iteration,\n",
    "        'best_score_history': best_score_history,\n",
    "        'max_std_history': max_std_history\n",
    "    }\n",
    "    return best_sample, best_score, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a84814de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-06-18 19:32:46,795:jax._src.xla_bridge:967: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "# setup squared exponential approximation\n",
    "# x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "x, w = gauss_legendre_rule_multilevel(0.0, 1.0, 100, L=5, ratio=0.2)\n",
    "\n",
    "target = np.exp(-x)\n",
    "\n",
    "# Convert to JAX arrays with float64 dtype\n",
    "x_j = jnp.array(x, dtype=jnp.float64)\n",
    "w_j = jnp.array(w, dtype=jnp.float64)\n",
    "target_j = jnp.array(target, dtype=jnp.float64)\n",
    "\n",
    "\n",
    "def obj_jax(p):\n",
    "    # p is already float64\n",
    "    n_param = p.shape[0] // 2\n",
    "    a = p[:n_param]\n",
    "    b = p[n_param:]\n",
    "    # compute prediction and weighted RMS error\n",
    "    pred = jnp.sum(a[:, None] * jnp.exp(-b[:, None] * x_j[None, :] ** 2), axis=0)\n",
    "    diff = pred - target_j\n",
    "    return jnp.sqrt(jnp.sum(w_j * diff ** 2))\n",
    "\n",
    "\n",
    "# gradients and Hessians in double precision\n",
    "grad_jax = jax.jit(jax.grad(obj_jax))\n",
    "hess_jax = jax.jit(jax.hessian(obj_jax))\n",
    "obj_jax_jit = jax.jit(obj_jax)\n",
    "\n",
    "\n",
    "def obj(p):\n",
    "    # ensure NumPyâ†’JAX conversion to float64\n",
    "    return float(obj_jax_jit(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def grad(p):\n",
    "    return np.array(grad_jax(jnp.array(p, dtype=jnp.float64)))\n",
    "\n",
    "\n",
    "def hess(p):\n",
    "    return np.array(hess_jax(jnp.array(p, dtype=jnp.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e6be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def golden_section_search(f, a=0.0, b=1.0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Golden-section search to find the minimum of a unimodal function f on [a, b].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        The objective function to minimize.\n",
    "    a : float\n",
    "        Left endpoint of the initial interval.\n",
    "    b : float\n",
    "        Right endpoint of the initial interval.\n",
    "    tol : float\n",
    "        Tolerance for the interval width (stopping criterion).\n",
    "    max_iter : int\n",
    "        Maximum number of iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_min : float\n",
    "        Estimated position of the minimum.\n",
    "    f_min : float\n",
    "        Value of f at x_min.\n",
    "    \"\"\"\n",
    "    # Golden ratio constant\n",
    "    phi = (np.sqrt(5.0) - 1) / 2\n",
    "\n",
    "    # Initialize interior points\n",
    "    c = b - phi * (b - a)\n",
    "    d = a + phi * (b - a)\n",
    "    f_c = f(c)\n",
    "    f_d = f(d)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        if abs(b - a) < tol:\n",
    "            break\n",
    "\n",
    "        if f_c < f_d:\n",
    "            b, d, f_d = d, c, f_c\n",
    "            c = b - phi * (b - a)\n",
    "            f_c = f(c)\n",
    "        else:\n",
    "            a, c, f_c = c, d, f_d\n",
    "            d = a + phi * (b - a)\n",
    "            f_d = f(d)\n",
    "\n",
    "    # Choose the best of the final points\n",
    "    if f_c < f_d:\n",
    "        x_min, f_min = c, f_c\n",
    "    else:\n",
    "        x_min, f_min = d, f_d\n",
    "    # print(f\"Iterations: {_ + 1}\")\n",
    "    return x_min, f_min\n",
    "\n",
    "\n",
    "def newton(f, grad, hess, x0,\n",
    "                                   tol_grad=1e-6,\n",
    "                                   tol_step=1e-8,\n",
    "                                   stall_iter=5,\n",
    "                                   max_iter=100,\n",
    "                                   ls_bounds=(-1.0, 1.0),\n",
    "                                   verbose=False):\n",
    "    \"\"\"\n",
    "    Newton's method with backtracking line search and fallback to gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        Objective function f(x).\n",
    "    grad : callable\n",
    "        Gradient of f: grad(x).\n",
    "    hess : callable\n",
    "        Hessian of f: hess(x).\n",
    "    x0 : ndarray\n",
    "        Initial guess.\n",
    "    tol_grad : float, optional\n",
    "        Tolerance for gradient norm.\n",
    "    tol_step : float, optional\n",
    "        Tolerance for step size norm for stall criterion.\n",
    "    stall_iter : int, optional\n",
    "        Number of consecutive small steps to trigger stop.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations.\n",
    "    ls_bounds : (float, float), optional\n",
    "        Initial interval for line search.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : ndarray\n",
    "        Estimated minimizer.\n",
    "    grad_norm : float\n",
    "        Norm of gradient at the solution.\n",
    "    f_history : list of float\n",
    "        History of f values.\n",
    "    grad_norm_history : list of float\n",
    "        History of gradient norms.\n",
    "    \"\"\"\n",
    "    x = x0.astype(float)\n",
    "    f_history = []\n",
    "    grad_norm_history = []\n",
    "    fx = f(x)\n",
    "\n",
    "    stall_count = 0\n",
    "    trust = 1e-6 * np.eye(len(x))  # trust region to ensure positive definiteness\n",
    "\n",
    "    for k in range(max_iter):\n",
    "        gx = grad(x)\n",
    "        grad_norm = np.linalg.norm(gx)\n",
    "\n",
    "        f_history.append(fx)\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "        # Check gradient convergence\n",
    "        if grad_norm < tol_grad:\n",
    "            break\n",
    "\n",
    "        # Try Newton step\n",
    "        try:\n",
    "            Hx = hess(x)\n",
    "            p = -np.linalg.solve(Hx + trust, gx)\n",
    "            trust /= 2\n",
    "            # ensure descent direction\n",
    "            # if np.dot(p, gx) >= 0:\n",
    "            #     raise np.linalg.LinAlgError\n",
    "        except np.linalg.LinAlgError:\n",
    "            # fallback to steepest descent\n",
    "            print(\"Hessian not positive definite, using gradient descent\")\n",
    "            p = -gx\n",
    "            trust *= 2\n",
    "        # Backtracking line search\n",
    "        alpha, fx = golden_section_search(\n",
    "            lambda alpha: f(x + alpha * p),\n",
    "            a=ls_bounds[0], b=ls_bounds[1], tol=1e-6, max_iter=100\n",
    "        )\n",
    "\n",
    "        # Update\n",
    "        x_new = x + alpha * p\n",
    "        step_norm = np.linalg.norm(x_new - x)\n",
    "        x = x_new\n",
    "\n",
    "        # Stall criterion\n",
    "        if step_norm < tol_step:\n",
    "            stall_count += 1\n",
    "            if stall_count >= stall_iter:\n",
    "                break\n",
    "        else:\n",
    "            stall_count = 0\n",
    "\n",
    "        # print debug info\n",
    "        if verbose:\n",
    "            print(f\"Iter {k}: f={fx:.6e}, grad_norm={grad_norm:.6e}, step_norm={step_norm:.6e}, alpha={alpha:.6f}\")\n",
    "\n",
    "    return x, grad_norm, f_history, grad_norm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910c5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "\n",
    "def optimal_a(d: np.ndarray,\n",
    "              w: np.ndarray,\n",
    "              target: np.ndarray,\n",
    "              b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve for a >= 0 minimizing\n",
    "        sum_j w[j] * (sum_k a[k] * exp(-b[k] * d[j]**2) - target[j])**2\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : (J,) array\n",
    "        Sample points x_j.\n",
    "    w : (J,) array\n",
    "        Quadrature weights.\n",
    "    target : (J,) array\n",
    "        Target values at each d[j].\n",
    "    b : (K,) array\n",
    "        Exponents b_k.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : (K,) array\n",
    "        Non-negative coefficient vector minimizing the weighted LS error.\n",
    "    \"\"\"\n",
    "    # Build design matrix Î¦[j,k] = exp(-b[k] * d[j]**2)\n",
    "    # Note: np.outer(d**2, b) yields shape (J, K)\n",
    "    Phi = np.exp(-np.outer(d**2, b))     # shape (J, K)\n",
    "\n",
    "    # Incorporate weights by scaling rows\n",
    "    W_sqrt = np.sqrt(w)                  # shape (J,)\n",
    "    Phi_w = Phi * W_sqrt[:, None]        # shape (J, K)\n",
    "    y_w = target * W_sqrt              # shape (J,)\n",
    "\n",
    "    a, _ = nnls(Phi_w, y_w)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "270cb850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal a: [0.30869798 0.29722181 0.20486527 0.08991123 0.05465842 0.02056889\n",
      " 0.0121404  0.00894426]\n",
      "Optimised parameters: [5.36016433e-01 2.43148741e-01 1.01184826e-01 5.53101285e-02\n",
      " 3.21566751e-02 1.79634057e-02 9.23876638e-03 4.19158797e-03\n",
      " 4.13332858e-01 2.90748541e+00 1.15984093e+01 3.95650773e+01\n",
      " 1.45261148e+02 6.41030505e+02 3.88789708e+03 4.72272741e+04]\n",
      "Final gradient norm: 4.084250808483441e-06\n",
      "Final objective value: 3.313403590764534e-05\n"
     ]
    }
   ],
   "source": [
    "# --- synthetic data ---------------------------------------------------\n",
    "n_terms = 8               # number of (a_i, b_i) pairs\n",
    "N       = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "\n",
    "b = np.linspace(-1.5, 8.5, n_terms)\n",
    "a = optimal_a(d, w, target, np.exp(b))\n",
    "print(\"Optimal a:\", a)\n",
    "\n",
    "means = np.concatenate((a, np.exp(b)))  # initial guess in log-space\n",
    "\n",
    "# --- optimiser call ---------------------------------------------------\n",
    "x, grad_norm, f_history, grad_norm_history = newton(\n",
    "    f=obj,\n",
    "    grad=grad,\n",
    "    hess=hess,\n",
    "    x0=means,\n",
    "    tol_grad=1e-5,\n",
    "    tol_step=1e-8,\n",
    "    stall_iter=50,\n",
    "    max_iter=3000,\n",
    "    ls_bounds=(-2.0, 2.0)\n",
    ")\n",
    "\n",
    "print(\"Optimised parameters:\", x)\n",
    "print(\"Final gradient norm:\", grad_norm)\n",
    "print(\"Final objective value:\", f_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dddd19f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: best_score=0.0013950703793328882, max_std=1.1356440572972426, info={'normg': 0.13175763560330805}\n",
      "Iter 1: best_score=0.0009889552061308979, max_std=1.2447310414460087, info={'normg': 0.13790330909674978}\n",
      "Iter 2: best_score=0.0009889552061308979, max_std=1.3869532696309697, info={'normg': 0.07160743968188284}\n",
      "Iter 3: best_score=0.0009889552061308979, max_std=1.4030271446417653, info={'normg': 0.1294265030283102}\n",
      "Iter 4: best_score=0.0009889552061308979, max_std=1.3402764583545088, info={'normg': 0.08211164220393306}\n",
      "Iter 5: best_score=0.0009889552061308979, max_std=1.3223914089623938, info={'normg': 0.08433113742867276}\n",
      "Iter 6: best_score=0.0009889552061308979, max_std=1.2070336778372304, info={'normg': 0.08518818383235606}\n",
      "Iter 7: best_score=0.0009889552061308979, max_std=1.1772958148914703, info={'normg': 0.09000993917449461}\n",
      "Iter 8: best_score=0.0006586318020339291, max_std=1.024109070485738, info={'normg': 0.06882914264805107}\n",
      "Iter 9: best_score=0.0006586318020339291, max_std=1.0974909127010353, info={'normg': 0.0922613830345869}\n",
      "Iter 10: best_score=0.0006586318020339291, max_std=0.9518810190479945, info={'normg': 0.08999283289956882}\n",
      "Iter 11: best_score=0.0006586318020339291, max_std=0.8945710810052168, info={'normg': 0.10144263225661267}\n",
      "Iter 12: best_score=0.0006586318020339291, max_std=0.7288917997108284, info={'normg': 0.07536397248296436}\n",
      "Iter 13: best_score=0.0003649344987902314, max_std=0.8316380091520996, info={'normg': 0.08090243376291358}\n",
      "Iter 14: best_score=0.0003649344987902314, max_std=0.7590712849856411, info={'normg': 0.0663452028119802}\n",
      "Iter 15: best_score=0.0003649344987902314, max_std=0.6438844169211918, info={'normg': 0.02195853472836657}\n",
      "Iter 16: best_score=0.00027520291622068347, max_std=0.5772602881151687, info={'normg': 0.020240989776893377}\n",
      "Iter 17: best_score=0.00027520291622068347, max_std=0.5198112840635483, info={'normg': 0.055626152767178784}\n",
      "Iter 18: best_score=0.00027520291622068347, max_std=0.47828866009306037, info={'normg': 0.018289957233250925}\n",
      "Iter 19: best_score=0.00027520291622068347, max_std=0.4657386034391213, info={'normg': 0.008798821911480086}\n",
      "Iter 20: best_score=0.00027520291622068347, max_std=0.4864422085830476, info={'normg': 0.02212214816122104}\n",
      "Iter 21: best_score=0.00027520291622068347, max_std=0.3951664314703573, info={'normg': 0.018413543068519916}\n",
      "Iter 22: best_score=0.0002529649162292488, max_std=0.3898275641285565, info={'normg': 0.01830454787713021}\n",
      "Iter 23: best_score=0.0002529649162292488, max_std=0.35054391162387055, info={'normg': 0.02432657680357303}\n",
      "Iter 24: best_score=0.00017834828893185072, max_std=0.3061107913569592, info={'normg': 0.025994209289568755}\n",
      "Iter 25: best_score=0.00017834828893185072, max_std=0.2968817458616282, info={'normg': 0.018019235749541052}\n",
      "Iter 26: best_score=0.0001725140335924764, max_std=0.3171815366419863, info={'normg': 0.01970893003617089}\n",
      "Iter 27: best_score=0.0001725140335924764, max_std=0.34167295263640585, info={'normg': 0.026123146153912806}\n",
      "Iter 28: best_score=0.0001442628792103085, max_std=0.377041941965736, info={'normg': 0.021995029544414797}\n",
      "Iter 29: best_score=0.0001442628792103085, max_std=0.3743206382800154, info={'normg': 0.022469221459378408}\n",
      "Iter 30: best_score=0.0001442628792103085, max_std=0.3490019317443735, info={'normg': 0.017335436753151884}\n",
      "Iter 31: best_score=0.0001419202944357068, max_std=0.35773905078287366, info={'normg': 0.058207534518330994}\n",
      "Iter 32: best_score=0.00013165414659736116, max_std=0.3441304071569542, info={'normg': 0.012469368114269757}\n",
      "Iter 33: best_score=0.00011895342216793968, max_std=0.3198551967745153, info={'normg': 0.015208359251041517}\n",
      "Iter 34: best_score=0.00011771000691453098, max_std=0.2746806798685587, info={'normg': 0.03838387199400342}\n",
      "Iter 35: best_score=0.00011771000691453098, max_std=0.2515185948227494, info={'normg': 0.0158848944244542}\n",
      "Iter 36: best_score=0.00011771000691453098, max_std=0.22065037008706478, info={'normg': 0.02343128579138646}\n",
      "Iter 37: best_score=0.00011771000691453098, max_std=0.21402228260238265, info={'normg': 0.01662194094932163}\n",
      "Iter 38: best_score=0.00011771000691453098, max_std=0.2054204029439822, info={'normg': 0.03974239763866255}\n",
      "Iter 39: best_score=0.00011338499978256332, max_std=0.18019781510181837, info={'normg': 0.014006543037400891}\n",
      "Iter 40: best_score=0.00010743881778296173, max_std=0.1686823133700479, info={'normg': 0.012513706558422065}\n",
      "Iter 41: best_score=0.00010743881778296173, max_std=0.15429913068985862, info={'normg': 0.021996637168465186}\n",
      "Iter 42: best_score=0.00010743881778296173, max_std=0.13349846667872223, info={'normg': 0.028648389392416675}\n",
      "Iter 43: best_score=0.00010743881778296173, max_std=0.12813425519525012, info={'normg': 0.015893352249817413}\n",
      "Iter 44: best_score=0.00010743881778296173, max_std=0.11568725175385608, info={'normg': 0.02280953913930005}\n",
      "Iter 45: best_score=0.00010608013128255365, max_std=0.10746946179238849, info={'normg': 0.020569403956420606}\n",
      "Iter 46: best_score=0.00010608013128255365, max_std=0.09710128155889342, info={'normg': 0.01728541313082231}\n",
      "Iter 47: best_score=0.00010433929967314676, max_std=0.1049473440655298, info={'normg': 0.019714400721037067}\n",
      "Iter 48: best_score=0.00010433929967314676, max_std=0.10797196125908048, info={'normg': 0.01985701533990359}\n",
      "Iter 49: best_score=0.00010433929967314676, max_std=0.10441411919759053, info={'normg': 0.018845991210579845}\n",
      "Iter 50: best_score=0.00010433929967314676, max_std=0.10961645049137426, info={'normg': 0.01597900566431279}\n",
      "Iter 51: best_score=0.00010433929967314676, max_std=0.10746351653211221, info={'normg': 0.02273117463255378}\n",
      "Iter 52: best_score=0.00010433929967314676, max_std=0.09812058092785991, info={'normg': 0.02330149029331124}\n",
      "Iter 53: best_score=0.00010433929967314676, max_std=0.0895880091281768, info={'normg': 0.018473794624666588}\n",
      "Iter 54: best_score=0.00010433929967314676, max_std=0.08054119684302014, info={'normg': 0.024410506728171723}\n",
      "Iter 55: best_score=0.00010433929967314676, max_std=0.0828727574143199, info={'normg': 0.030101239509298803}\n",
      "Iter 56: best_score=0.00010433929967314676, max_std=0.0816398196968065, info={'normg': 0.02647752357167129}\n",
      "Iter 57: best_score=0.00010433929967314676, max_std=0.07676762791781322, info={'normg': 0.02691831622224683}\n",
      "Iter 58: best_score=0.00010433929967314676, max_std=0.0857999035981617, info={'normg': 0.02965312381717002}\n",
      "Iter 59: best_score=0.00010433929967314676, max_std=0.06972531818239541, info={'normg': 0.022087634595819222}\n",
      "Iter 60: best_score=0.00010433929967314676, max_std=0.06849262118790346, info={'normg': 0.018029015261081683}\n",
      "Iter 61: best_score=0.00010433929967314676, max_std=0.0682739959747552, info={'normg': 0.02719377423437835}\n",
      "Iter 62: best_score=0.00010433929967314676, max_std=0.0711636363880432, info={'normg': 0.029064960273335036}\n",
      "Iter 63: best_score=0.00010433929967314676, max_std=0.06368262491125078, info={'normg': 0.024914163944022707}\n",
      "Iter 64: best_score=0.00010433929967314676, max_std=0.05855545411660247, info={'normg': 0.024293384437693685}\n",
      "Iter 65: best_score=0.00010433929967314676, max_std=0.05573355918316836, info={'normg': 0.02448832431780155}\n",
      "Iter 66: best_score=0.00010386859547291, max_std=0.06281829816280707, info={'normg': 0.024524446202855908}\n",
      "Iter 67: best_score=0.00010386859547291, max_std=0.06210135432459693, info={'normg': 0.04020654287181511}\n",
      "Iter 68: best_score=0.00010386859547291, max_std=0.054241599454030114, info={'normg': 0.0256909308687281}\n",
      "Iter 69: best_score=0.00010386859547291, max_std=0.0507612119847823, info={'normg': 0.02062196914176199}\n",
      "Iter 70: best_score=0.00010386859547291, max_std=0.04685304466265447, info={'normg': 0.02988431495536179}\n",
      "Iter 71: best_score=0.0001032935141957278, max_std=0.04431208817809035, info={'normg': 0.025619737046073093}\n",
      "Iter 72: best_score=0.0001032935141957278, max_std=0.04539093697661515, info={'normg': 0.028427781108761967}\n",
      "Iter 73: best_score=0.0001032935141957278, max_std=0.046445463766226075, info={'normg': 0.027018140643773787}\n",
      "Iter 74: best_score=0.0001032935141957278, max_std=0.04610397597268412, info={'normg': 0.028954822385130235}\n",
      "Iter 75: best_score=0.00010310595434220874, max_std=0.04567730988035591, info={'normg': 0.0288262359534993}\n",
      "Iter 76: best_score=0.00010298713014151404, max_std=0.039037962166892724, info={'normg': 0.01798452501383518}\n",
      "Iter 77: best_score=0.00010298713014151404, max_std=0.03653013992455177, info={'normg': 0.026356193465093648}\n",
      "Iter 78: best_score=0.00010298713014151404, max_std=0.03779978679232422, info={'normg': 0.025990651035906365}\n",
      "Iter 79: best_score=0.00010298713014151404, max_std=0.03757989156679588, info={'normg': 0.019915967574923817}\n",
      "Iter 80: best_score=0.00010298713014151404, max_std=0.03741698644663133, info={'normg': 0.028651478704283834}\n",
      "Iter 81: best_score=0.00010298713014151404, max_std=0.03757754048929098, info={'normg': 0.025521976190903687}\n",
      "Iter 82: best_score=0.00010298713014151404, max_std=0.03783385747563173, info={'normg': 0.02915869540236606}\n",
      "Iter 83: best_score=0.00010298713014151404, max_std=0.03750650487110886, info={'normg': 0.02864817720201088}\n",
      "Iter 84: best_score=0.00010298713014151404, max_std=0.03707155479395953, info={'normg': 0.026915113639420944}\n",
      "Iter 85: best_score=0.00010298713014151404, max_std=0.0342116435860137, info={'normg': 0.02593059275851097}\n",
      "Iter 86: best_score=0.00010228826075560054, max_std=0.031931399849613844, info={'normg': 0.022467880021841125}\n",
      "Iter 87: best_score=0.00010228826075560054, max_std=0.0321260835344579, info={'normg': 0.022426877884841483}\n",
      "Iter 88: best_score=0.00010228826075560054, max_std=0.025921393130573282, info={'normg': 0.0209968640564787}\n",
      "Iter 89: best_score=0.00010228826075560054, max_std=0.024471442553168327, info={'normg': 0.025554669040661563}\n",
      "Iter 90: best_score=0.00010191583295801586, max_std=0.026179333272425838, info={'normg': 0.018931135102261763}\n",
      "Iter 91: best_score=0.00010081687547465548, max_std=0.023597911284871154, info={'normg': 0.02391769227384642}\n",
      "Iter 92: best_score=0.00010081687547465548, max_std=0.022812051365643373, info={'normg': 0.01722279586503422}\n",
      "Iter 93: best_score=0.00010081687547465548, max_std=0.02475977822965187, info={'normg': 0.021790448528103558}\n",
      "Iter 94: best_score=0.00010005211703311421, max_std=0.02878868887315161, info={'normg': 0.016052765848921915}\n",
      "Iter 95: best_score=0.00010005211703311421, max_std=0.028694507136740835, info={'normg': 0.01930933776837147}\n",
      "Iter 96: best_score=0.00010005211703311421, max_std=0.033922586800357216, info={'normg': 0.02479611205433533}\n",
      "Iter 97: best_score=0.00010005211703311421, max_std=0.032496385916647934, info={'normg': 0.034387569749943}\n",
      "Iter 98: best_score=0.00010005211703311421, max_std=0.033987959824697586, info={'normg': 0.02892971230529507}\n",
      "Iter 99: best_score=0.00010005211703311421, max_std=0.03431636579273249, info={'normg': 0.025787036941218675}\n",
      "Iter 100: best_score=0.00010005211703311421, max_std=0.034768800795158636, info={'normg': 0.029784429555841745}\n",
      "Iter 101: best_score=0.00010005211703311421, max_std=0.03167347109644153, info={'normg': 0.02645270061219264}\n",
      "Iter 102: best_score=0.00010005211703311421, max_std=0.029842306117473728, info={'normg': 0.037364043372152024}\n",
      "Iter 103: best_score=0.00010005211703311421, max_std=0.030153513984303874, info={'normg': 0.026247768950421347}\n",
      "Iter 104: best_score=0.00010005211703311421, max_std=0.03130513500776884, info={'normg': 0.02798222098106784}\n",
      "Iter 105: best_score=0.00010005211703311421, max_std=0.03215820698041356, info={'normg': 0.017929092058801405}\n",
      "Iter 106: best_score=0.00010005211703311421, max_std=0.02845927334443727, info={'normg': 0.028732903311159286}\n",
      "Iter 107: best_score=0.00010005211703311421, max_std=0.02618491372800301, info={'normg': 0.02669051546092208}\n",
      "Iter 108: best_score=0.00010005211703311421, max_std=0.02536007659100034, info={'normg': 0.0249714826160117}\n",
      "Iter 109: best_score=0.00010005211703311421, max_std=0.025279897195920414, info={'normg': 0.03072370948847722}\n",
      "Iter 110: best_score=0.00010005211703311421, max_std=0.028226228034761103, info={'normg': 0.029702868085843156}\n",
      "Iter 111: best_score=0.00010005211703311421, max_std=0.026497079254770645, info={'normg': 0.02807198250461675}\n",
      "Iter 112: best_score=0.00010005211703311421, max_std=0.025916452801121766, info={'normg': 0.02585839052685066}\n",
      "Iter 113: best_score=0.00010005211703311421, max_std=0.025007826432778962, info={'normg': 0.03611237706614483}\n",
      "Iter 114: best_score=0.00010005211703311421, max_std=0.02614313373104712, info={'normg': 0.05344911966379878}\n",
      "Iter 115: best_score=0.00010005211703311421, max_std=0.026591959908466765, info={'normg': 0.026428110755358933}\n",
      "Iter 116: best_score=0.00010005211703311421, max_std=0.027154561057288983, info={'normg': 0.025449720682712305}\n",
      "Iter 117: best_score=0.00010005211703311421, max_std=0.024140424442294736, info={'normg': 0.029629250179662378}\n",
      "Iter 118: best_score=0.00010005211703311421, max_std=0.025024918939983813, info={'normg': 0.019684819545181254}\n",
      "Iter 119: best_score=0.00010005211703311421, max_std=0.024840842224283056, info={'normg': 0.019318038497173496}\n",
      "Iter 120: best_score=0.00010005211703311421, max_std=0.021781937159787337, info={'normg': 0.019653484372760113}\n",
      "Iter 121: best_score=0.00010005211703311421, max_std=0.02014192354847771, info={'normg': 0.027381203429567152}\n",
      "Iter 122: best_score=0.00010005211703311421, max_std=0.021197458380087533, info={'normg': 0.022667867712428246}\n",
      "Iter 123: best_score=0.00010005211703311421, max_std=0.019068417902903534, info={'normg': 0.026918941395635197}\n",
      "Iter 124: best_score=0.00010005211703311421, max_std=0.018039575280799396, info={'normg': 0.01761834132387018}\n",
      "Iter 125: best_score=0.00010005211703311421, max_std=0.01819825794003661, info={'normg': 0.028426152874923245}\n",
      "Iter 126: best_score=0.00010005211703311421, max_std=0.019786045111697077, info={'normg': 0.02009033250431195}\n",
      "Iter 127: best_score=0.00010005211703311421, max_std=0.019583199668531403, info={'normg': 0.040072666864666456}\n",
      "Iter 128: best_score=0.00010005211703311421, max_std=0.021182748692393685, info={'normg': 0.028441462988048116}\n",
      "Iter 129: best_score=0.00010005211703311421, max_std=0.02139512176836886, info={'normg': 0.028358096867359232}\n",
      "Iter 130: best_score=0.00010005211703311421, max_std=0.023184798128945736, info={'normg': 0.03400639732256696}\n",
      "Iter 131: best_score=0.00010005211703311421, max_std=0.02076577380234453, info={'normg': 0.030703044691189604}\n",
      "Iter 132: best_score=0.00010005211703311421, max_std=0.02176882722763879, info={'normg': 0.02428521621424746}\n",
      "Iter 133: best_score=0.00010005211703311421, max_std=0.020963490351931936, info={'normg': 0.027383855937701464}\n",
      "Iter 134: best_score=0.00010005211703311421, max_std=0.02058878065070385, info={'normg': 0.027984993014760397}\n",
      "Iter 135: best_score=0.00010005211703311421, max_std=0.022499085394687594, info={'normg': 0.026894172261864833}\n",
      "Iter 136: best_score=0.00010005211703311421, max_std=0.02103565272181178, info={'normg': 0.02535149522259491}\n",
      "Iter 137: best_score=0.00010005211703311421, max_std=0.020342049661887508, info={'normg': 0.017730473715935138}\n",
      "Iter 138: best_score=0.00010005211703311421, max_std=0.02136695935682673, info={'normg': 0.05311492296833243}\n",
      "Iter 139: best_score=0.00010005211703311421, max_std=0.02040387181766167, info={'normg': 0.03391900081417769}\n",
      "Iter 140: best_score=0.00010005211703311421, max_std=0.018130569179122473, info={'normg': 0.025261554797660225}\n",
      "Iter 141: best_score=0.00010005211703311421, max_std=0.019741254043653726, info={'normg': 0.02106530272173308}\n",
      "Iter 142: best_score=0.00010005211703311421, max_std=0.018510509473350152, info={'normg': 0.016978251422405107}\n",
      "Iter 143: best_score=0.00010005211703311421, max_std=0.020445357950046858, info={'normg': 0.02091398891478596}\n",
      "Iter 144: best_score=0.00010005211703311421, max_std=0.020029067835368272, info={'normg': 0.025909060519794386}\n",
      "Iter 145: best_score=0.00010005211703311421, max_std=0.01912062762427632, info={'normg': 0.021235502374249485}\n",
      "Iter 146: best_score=0.00010005211703311421, max_std=0.018422799281320783, info={'normg': 0.024579336084585568}\n",
      "Iter 147: best_score=0.00010005211703311421, max_std=0.017509684086980146, info={'normg': 0.03092683230728817}\n",
      "Iter 148: best_score=0.00010005211703311421, max_std=0.01946936191309693, info={'normg': 0.02916701042001943}\n",
      "Iter 149: best_score=0.00010005211703311421, max_std=0.020732833528364095, info={'normg': 0.025573081478616547}\n",
      "Iter 150: best_score=0.00010005211703311421, max_std=0.018542225405044938, info={'normg': 0.027338027592097008}\n",
      "Iter 151: best_score=0.00010005211703311421, max_std=0.016949726735289054, info={'normg': 0.02642935632711804}\n",
      "Iter 152: best_score=0.00010005211703311421, max_std=0.015092802147419856, info={'normg': 0.026513960045080418}\n",
      "Iter 153: best_score=0.00010005211703311421, max_std=0.01763735594157446, info={'normg': 0.028158304068074774}\n",
      "Iter 154: best_score=0.00010005211703311421, max_std=0.01810586899784648, info={'normg': 0.03362467708039484}\n",
      "Iter 155: best_score=0.00010005211703311421, max_std=0.016925324993499694, info={'normg': 0.01935240905573013}\n",
      "Iter 156: best_score=0.00010005211703311421, max_std=0.017986759377477818, info={'normg': 0.02225140152558786}\n",
      "Iter 157: best_score=0.00010005211703311421, max_std=0.020684880205950582, info={'normg': 0.02958995931985706}\n",
      "Iter 158: best_score=0.00010005211703311421, max_std=0.01815390756122521, info={'normg': 0.026144716903337816}\n",
      "Iter 159: best_score=0.00010005211703311421, max_std=0.021270074449549994, info={'normg': 0.025466335849073717}\n",
      "Iter 160: best_score=0.00010005211703311421, max_std=0.021663424980501982, info={'normg': 0.028143470856137724}\n",
      "Iter 161: best_score=0.00010005211703311421, max_std=0.02039926696254589, info={'normg': 0.025536194098666063}\n",
      "Iter 162: best_score=0.00010005211703311421, max_std=0.02232045727717272, info={'normg': 0.018833266542014303}\n",
      "Iter 163: best_score=0.00010005211703311421, max_std=0.01881611960208475, info={'normg': 0.028218356686977884}\n",
      "Iter 164: best_score=0.00010005211703311421, max_std=0.018730549416177, info={'normg': 0.025373557405104157}\n",
      "Iter 165: best_score=0.00010005211703311421, max_std=0.018030631294517433, info={'normg': 0.045620228141257005}\n",
      "Iter 166: best_score=9.986170686447568e-05, max_std=0.01844566330748549, info={'normg': 0.022540938522116718}\n",
      "Iter 167: best_score=9.986170686447568e-05, max_std=0.020949346833998312, info={'normg': 0.026144679320733957}\n",
      "Iter 168: best_score=9.986170686447568e-05, max_std=0.01980896107333036, info={'normg': 0.02452963812695316}\n",
      "Iter 169: best_score=9.986170686447568e-05, max_std=0.01939057270957873, info={'normg': 0.02336540140719774}\n",
      "Iter 170: best_score=9.986170686447568e-05, max_std=0.02008983406163088, info={'normg': 0.03153419868851727}\n",
      "Iter 171: best_score=9.986170686447568e-05, max_std=0.019745719681940114, info={'normg': 0.026486170989302643}\n",
      "Iter 172: best_score=9.986170686447568e-05, max_std=0.021572930181881392, info={'normg': 0.027554037526905745}\n",
      "Iter 173: best_score=9.986170686447568e-05, max_std=0.022354936019552962, info={'normg': 0.024102633097366228}\n",
      "Iter 174: best_score=9.986170686447568e-05, max_std=0.023097811663236732, info={'normg': 0.02319006669971717}\n",
      "Iter 175: best_score=9.986170686447568e-05, max_std=0.02289073412535228, info={'normg': 0.02933133582167447}\n",
      "Iter 176: best_score=9.986170686447568e-05, max_std=0.022184964710139962, info={'normg': 0.029441089427288594}\n",
      "Iter 177: best_score=9.986170686447568e-05, max_std=0.022492185670331948, info={'normg': 0.02750047225898483}\n",
      "Iter 178: best_score=9.986170686447568e-05, max_std=0.020954331467031784, info={'normg': 0.025419414911830175}\n",
      "Iter 179: best_score=9.986170686447568e-05, max_std=0.01987799904938012, info={'normg': 0.05163420384887799}\n",
      "Iter 180: best_score=9.986170686447568e-05, max_std=0.019062667588595112, info={'normg': 0.022673422614564582}\n",
      "Iter 181: best_score=9.986170686447568e-05, max_std=0.019448304706644673, info={'normg': 0.02862082800266544}\n",
      "Iter 182: best_score=9.986170686447568e-05, max_std=0.019888550722397792, info={'normg': 0.02556828745575793}\n",
      "Iter 183: best_score=9.986170686447568e-05, max_std=0.01978358993022124, info={'normg': 0.025681506700701774}\n",
      "Iter 184: best_score=9.986170686447568e-05, max_std=0.018838634310699952, info={'normg': 0.024760669683514362}\n",
      "Iter 185: best_score=9.986170686447568e-05, max_std=0.021440479905043747, info={'normg': 0.02197790854334558}\n",
      "Iter 186: best_score=9.986170686447568e-05, max_std=0.02233315566216236, info={'normg': 0.029328494239209454}\n",
      "Iter 187: best_score=9.986170686447568e-05, max_std=0.022001254094238837, info={'normg': 0.025320589754417228}\n",
      "Iter 188: best_score=9.986170686447568e-05, max_std=0.022755249114333517, info={'normg': 0.02670530430709334}\n",
      "Iter 189: best_score=9.986170686447568e-05, max_std=0.020030310945365663, info={'normg': 0.022604977710110954}\n",
      "Iter 190: best_score=9.986170686447568e-05, max_std=0.021084639633463888, info={'normg': 0.024461335211266127}\n",
      "Iter 191: best_score=9.986170686447568e-05, max_std=0.019652436793776516, info={'normg': 0.019572977163249097}\n",
      "Iter 192: best_score=9.986170686447568e-05, max_std=0.021124055591098638, info={'normg': 0.02579648535621582}\n",
      "Iter 193: best_score=9.986170686447568e-05, max_std=0.02063393044485811, info={'normg': 0.031407432080886415}\n",
      "Iter 194: best_score=9.986170686447568e-05, max_std=0.018964262659351673, info={'normg': 0.026142372919839146}\n",
      "Iter 195: best_score=9.986170686447568e-05, max_std=0.018346279827161464, info={'normg': 0.027782825120651545}\n",
      "Iter 196: best_score=9.986170686447568e-05, max_std=0.01854913791096527, info={'normg': 0.02327406358240834}\n",
      "Iter 197: best_score=9.986170686447568e-05, max_std=0.017297474863330168, info={'normg': 0.018546526669038645}\n",
      "Iter 198: best_score=9.986170686447568e-05, max_std=0.016806296642440828, info={'normg': 0.01875470693288915}\n",
      "Iter 199: best_score=9.986170686447568e-05, max_std=0.019492659101661272, info={'normg': 0.025386306044515167}\n",
      "Iter 200: best_score=9.986170686447568e-05, max_std=0.0205189273343393, info={'normg': 0.020143343516269306}\n",
      "Iter 201: best_score=9.986170686447568e-05, max_std=0.018132783792441858, info={'normg': 0.033157854144766886}\n",
      "Iter 202: best_score=9.986170686447568e-05, max_std=0.019021314931560014, info={'normg': 0.024114798753477343}\n",
      "Iter 203: best_score=9.986170686447568e-05, max_std=0.0195020170025446, info={'normg': 0.02557761897457271}\n",
      "Iter 204: best_score=9.986170686447568e-05, max_std=0.019474425742081115, info={'normg': 0.02069557661048484}\n",
      "Iter 205: best_score=9.986170686447568e-05, max_std=0.02116525669198421, info={'normg': 0.018747589427235492}\n",
      "Iter 206: best_score=9.986170686447568e-05, max_std=0.021540503480384686, info={'normg': 0.027004877941465937}\n",
      "Iter 207: best_score=9.986170686447568e-05, max_std=0.01983943601233209, info={'normg': 0.023073234899593786}\n",
      "Iter 208: best_score=9.986170686447568e-05, max_std=0.017416751300432517, info={'normg': 0.026038767186116363}\n",
      "Iter 209: best_score=9.986170686447568e-05, max_std=0.02043585801684767, info={'normg': 0.027798370176257137}\n",
      "Iter 210: best_score=9.986170686447568e-05, max_std=0.019608353697285023, info={'normg': 0.018878211116297772}\n",
      "Iter 211: best_score=9.986170686447568e-05, max_std=0.024278119746655343, info={'normg': 0.023615671914258708}\n",
      "Iter 212: best_score=9.986170686447568e-05, max_std=0.023828384878332748, info={'normg': 0.02833010232533649}\n",
      "Iter 213: best_score=9.986170686447568e-05, max_std=0.024867778354740026, info={'normg': 0.02937270036220048}\n",
      "Iter 214: best_score=9.986170686447568e-05, max_std=0.025343956226613954, info={'normg': 0.02553155631451223}\n",
      "Iter 215: best_score=9.986170686447568e-05, max_std=0.02099619964154139, info={'normg': 0.02757723626850047}\n",
      "Iter 216: best_score=9.986170686447568e-05, max_std=0.01892804947291079, info={'normg': 0.023461007704879376}\n",
      "Iter 217: best_score=9.986170686447568e-05, max_std=0.020311655013864643, info={'normg': 0.02659670564264292}\n",
      "Iter 218: best_score=9.986170686447568e-05, max_std=0.02275089108470925, info={'normg': 0.0275372041758234}\n",
      "Iter 219: best_score=9.986170686447568e-05, max_std=0.019512622482188664, info={'normg': 0.030273349073437764}\n",
      "Iter 220: best_score=9.986170686447568e-05, max_std=0.02067091136915991, info={'normg': 0.025102778298594672}\n",
      "Iter 221: best_score=9.986170686447568e-05, max_std=0.018089732562413682, info={'normg': 0.03110087862212731}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     82\u001b[39m init_std = np.full(n_terms, \u001b[32m1.0\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m#  Run Cross-Entropy\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m best_b_log, best_score, ce_info = \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mce_user_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43melite_frac\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha_std\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1234\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_g\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_std\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# prints per-iteration progress\u001b[39;49;00m\n\u001b[32m    100\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m#  Report results\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------------- #\u001b[39;00m\n\u001b[32m    105\u001b[39m best_b = np.exp(best_b_log)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(user_fn, initial_mean, initial_std, pop_size, elite_frac, alpha_mean, alpha_std, n_iters, random_state, verbose, tol_std, tol_g)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iters):\n\u001b[32m     53\u001b[39m     samples = rng.randn(pop_size, mean.size) * std + mean\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     samples, scores, info_dict = \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     idx = \u001b[38;5;28mint\u001b[39m(np.argmin(scores))\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# update best if improved\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mce_user_fn\u001b[39m\u001b[34m(b_log_samples)\u001b[39m\n\u001b[32m     41\u001b[39m p0 = np.concatenate([a_opt, b_pos])      \u001b[38;5;66;03m# shape (2K,)\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# ---- (2) limited Newton polishing ----------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m p_new, gnorm, *_ = \u001b[43mnewton\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewton_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_step\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstall_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewton_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mls_bounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     55\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.any(np.isnan(p_new)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.any(np.isinf(p_new)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.any(p_new < \u001b[32m0\u001b[39m):\n\u001b[32m     58\u001b[39m     \u001b[38;5;66;03m# ---- (3) extract & sort b (log) ------------------------------------\u001b[39;00m\n\u001b[32m     59\u001b[39m     b_new_pos = np.clip(p_new[K:], \u001b[32m1e-12\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# keep positivity\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mnewton\u001b[39m\u001b[34m(f, grad, hess, x0, tol_grad, tol_step, stall_iter, max_iter, ls_bounds, verbose)\u001b[39m\n\u001b[32m    130\u001b[39m     trust *= \u001b[32m2\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Backtracking line search\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m alpha, fx = \u001b[43mgolden_section_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mls_bounds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mls_bounds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m    135\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Update\u001b[39;00m\n\u001b[32m    138\u001b[39m x_new = x + alpha * p\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgolden_section_search\u001b[39m\u001b[34m(f, a, b, tol, max_iter)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgolden_section_search\u001b[39m(f, a=\u001b[32m0.0\u001b[39m, b=\u001b[32m1.0\u001b[39m, tol=\u001b[32m1e-6\u001b[39m, max_iter=\u001b[32m100\u001b[39m):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    Golden-section search to find the minimum of a unimodal function f on [a, b].\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33;03m        Value of f at x_min.\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Golden ratio constant\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "#  Cross-Entropy search over log-b with internal Newton polishing\n",
    "###############################################################################\n",
    "import numpy as np\n",
    "\n",
    "# --- problem size -----------------------------------------------------------\n",
    "n_terms = 8          # K  (= length of b and a)\n",
    "newton_steps = 1     # how many Newton iterations per sample\n",
    "pop_size = 100       # CE population\n",
    "np.random.seed(0)    # reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  helper: CE user-defined update function operating on log-b only\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def ce_user_fn(b_log_samples: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b_log_samples : (N, K) ndarray\n",
    "        Current population in *log* space.\n",
    "    Returns\n",
    "    -------\n",
    "    updated_log_b : (N, K) ndarray\n",
    "        Polished & sorted log-b that CE will keep using.\n",
    "    scores        : (N,)  ndarray\n",
    "        Objective values after polishing (smaller is better).\n",
    "    info          : dict\n",
    "        Contains minimum gradient norm among all samples (for CE stopping).\n",
    "    \"\"\"\n",
    "    N, K = b_log_samples.shape\n",
    "    updated = np.empty_like(b_log_samples)\n",
    "    scores = np.empty(N)\n",
    "    grad_norm = np.empty(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        # ---- (1) build starting point --------------------------------------\n",
    "        b_pos = np.exp(b_log_samples[i])            # (K,)  positive\n",
    "        a_opt = optimal_a(d, w, target, b_pos)      # (K,)\n",
    "        p0 = np.concatenate([a_opt, b_pos])      # shape (2K,)\n",
    "\n",
    "        # ---- (2) limited Newton polishing ----------------------------------\n",
    "        p_new, gnorm, *_ = newton(\n",
    "            f=obj,\n",
    "            grad=grad,\n",
    "            hess=hess,\n",
    "            x0=p0,\n",
    "            max_iter=newton_steps,\n",
    "            tol_grad=1e-6,\n",
    "            tol_step=1e-6,\n",
    "            stall_iter=newton_steps,\n",
    "            ls_bounds=(-2., 2.),\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        if not np.any(np.isnan(p_new)) and not np.any(np.isinf(p_new)) and not np.any(p_new < 0):\n",
    "            # ---- (3) extract & sort b (log) ------------------------------------\n",
    "            b_new_pos = np.clip(p_new[K:], 1e-12, None)  # keep positivity\n",
    "            b_new_log = np.log(b_new_pos)\n",
    "            order = np.argsort(b_new_log)\n",
    "            a_opt = optimal_a(d, w, target, b_new_pos)      # (K,)\n",
    "            p0 = np.concatenate([a_opt, b_new_pos])      # shape (2K,)\n",
    "\n",
    "            updated[i] = b_new_log[order]\n",
    "            scores[i] = obj(p0)       # score *after* polishing\n",
    "            grad_norm[i] = np.linalg.norm(grad(p0))\n",
    "        else:\n",
    "            # if polishing failed, keep the original sample\n",
    "            updated[i] = b_log_samples[i]\n",
    "            scores[i] = np.inf\n",
    "            grad_norm[i] = np.inf\n",
    "\n",
    "    info = {'normg': float(grad_norm.min())}\n",
    "    return updated, scores, info\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  CE initial distribution over log-b\n",
    "# --------------------------------------------------------------------------- #\n",
    "init_mean = np.linspace(-1.5, 9.3, n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 1.0)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Run Cross-Entropy\n",
    "# --------------------------------------------------------------------------- #\n",
    "best_b_log, best_score, ce_info = cross_entropy(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    elite_frac=0.2,\n",
    "    alpha_mean=0.5,\n",
    "    alpha_std=0.5,\n",
    "    n_iters=2000,\n",
    "    random_state=1234,\n",
    "    tol_g=1e-6,\n",
    "    tol_std=1e-6,\n",
    "    verbose=True      # prints per-iteration progress\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Report results\n",
    "# --------------------------------------------------------------------------- #\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = obj(np.concatenate([best_a, best_b]))\n",
    "\n",
    "print(\"\\n====================  CE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"CE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"stopped at iteration\", ce_info['iter'],\n",
    "      \"| max Ïƒ =\", ce_info['max_std_history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "830c3d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.54387056, -0.0805901 ,  1.13304965,  2.26870079,  3.46671661,\n",
       "        4.7994511 ,  6.30903264,  8.00312236, 10.14581073])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_b_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f43c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#  Differential-Evolution optimiser that works with the same `user_fn`\n",
    "###############################################################################\n",
    "from typing import Callable, Tuple, Dict, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def differential_evolution(\n",
    "    user_fn: Callable[[np.ndarray],\n",
    "                      Tuple[np.ndarray, np.ndarray, Dict[str, Any]]],\n",
    "    initial_mean: np.ndarray,\n",
    "    initial_std: np.ndarray,\n",
    "    pop_size: int = 100,\n",
    "    F: float = 0.8,                 # differential weight\n",
    "    CR: float = 0.9,                # crossover probability\n",
    "    n_iters: int = 500,\n",
    "    random_state: Optional[int] = None,\n",
    "    tol_g: float = 1e-6,            # stop if minâ€–gâ€– drops below this\n",
    "    stall_generations: int = 50,    # stop if no improvement for so many gens\n",
    "    verbose: bool = False\n",
    ") -> Tuple[np.ndarray, float, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Differential Evolution (DE/rand/1/bin) with the CE-style `user_fn`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_sample : ndarray\n",
    "        The best parameter vector found (d,).\n",
    "    best_score  : float\n",
    "        Objective value of `best_sample`.\n",
    "    info        : dict\n",
    "        Diagnostics (`iter`, `best_score_history`).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    d = initial_mean.size\n",
    "\n",
    "    # --- initial population --------------------------------------------------\n",
    "    pop = rng.randn(pop_size, d) * initial_std + initial_mean       # (N, d)\n",
    "    pop, scores, info = user_fn(pop)                               # polish & score\n",
    "    best_idx = int(np.argmin(scores))\n",
    "    best_sample = pop[best_idx].copy()\n",
    "    best_score = float(scores[best_idx])\n",
    "\n",
    "    best_score_history = [best_score]\n",
    "    no_improve = 0\n",
    "\n",
    "    for it in range(1, n_iters + 1):\n",
    "        # ------------ mutation & crossover ----------------------------------\n",
    "        trial = np.empty_like(pop)\n",
    "        for i in range(pop_size):\n",
    "            # choose three *distinct* indices â‰  i\n",
    "            r1, r2, r3 = rng.choice([j for j in range(pop_size) if j != i],\n",
    "                                    size=3, replace=False)\n",
    "            mutant = pop[r1] + F * (pop[r2] - pop[r3])\n",
    "\n",
    "            # binomial crossover\n",
    "            cross_pts = rng.rand(d) < CR\n",
    "            cross_pts[rng.randint(0, d)] = True      # ensure at least one gene\n",
    "            trial[i] = np.where(cross_pts, mutant, pop[i])\n",
    "\n",
    "        # ------------ evaluate trial population ------------------------------\n",
    "        trial, trial_scores, info_trial = user_fn(trial)\n",
    "\n",
    "        # ------------ selection ----------------------------------------------\n",
    "        improved = trial_scores < scores\n",
    "        pop[improved] = trial[improved]\n",
    "        scores[improved] = trial_scores[improved]\n",
    "\n",
    "        # ------------ keep track of best -------------------------------------\n",
    "        gen_best_idx = int(np.argmin(scores))\n",
    "        gen_best_score = float(scores[gen_best_idx])\n",
    "        if gen_best_score + 1e-12 < best_score:      # strict improvement\n",
    "            best_score = gen_best_score\n",
    "            best_sample = pop[gen_best_idx].copy()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        best_score_history.append(best_score)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Gen {it:4d}  best={best_score:.4e} \"\n",
    "                  f\"minâ€–gâ€–={info_trial.get('normg', np.inf):.2e}\")\n",
    "\n",
    "        # ------------ stopping criteria --------------------------------------\n",
    "        if info_trial.get('normg', np.inf) <= tol_g:\n",
    "            if verbose:\n",
    "                print(\"Stopping: gradient norm below tol_g\")\n",
    "            break\n",
    "        if no_improve >= stall_generations:\n",
    "            if verbose:\n",
    "                print(\"Stopping: no improvement for\",\n",
    "                      stall_generations, \"generations\")\n",
    "            break\n",
    "\n",
    "    info = {\n",
    "        'iter': it,\n",
    "        'best_score_history': best_score_history\n",
    "    }\n",
    "    return best_sample, best_score, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1ddd9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen    1  best=7.7872e-04 minâ€–gâ€–=3.18e-02\n",
      "Gen    2  best=7.2092e-04 minâ€–gâ€–=1.36e-02\n",
      "Gen    3  best=7.2092e-04 minâ€–gâ€–=2.41e-02\n",
      "Gen    4  best=6.6108e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen    5  best=6.6108e-04 minâ€–gâ€–=6.44e-03\n",
      "Gen    6  best=6.5149e-04 minâ€–gâ€–=2.31e-02\n",
      "Gen    7  best=3.8498e-04 minâ€–gâ€–=1.83e-02\n",
      "Gen    8  best=3.8498e-04 minâ€–gâ€–=1.70e-02\n",
      "Gen    9  best=3.8498e-04 minâ€–gâ€–=2.20e-02\n",
      "Gen   10  best=3.8498e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   11  best=3.8498e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   12  best=3.8498e-04 minâ€–gâ€–=1.08e-02\n",
      "Gen   13  best=3.8498e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   14  best=3.8498e-04 minâ€–gâ€–=1.55e-02\n",
      "Gen   15  best=3.8498e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   16  best=3.8498e-04 minâ€–gâ€–=1.53e-02\n",
      "Gen   17  best=3.8498e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   18  best=3.2335e-04 minâ€–gâ€–=7.07e-03\n",
      "Gen   19  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   20  best=3.2335e-04 minâ€–gâ€–=9.91e-03\n",
      "Gen   21  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   22  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   23  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   24  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   25  best=3.2335e-04 minâ€–gâ€–=1.04e-02\n",
      "Gen   26  best=3.2335e-04 minâ€–gâ€–=1.32e-02\n",
      "Gen   27  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   28  best=3.2335e-04 minâ€–gâ€–=1.36e-02\n",
      "Gen   29  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   30  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   31  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   32  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   33  best=3.2335e-04 minâ€–gâ€–=8.35e-03\n",
      "Gen   34  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   35  best=3.2335e-04 minâ€–gâ€–=1.16e-02\n",
      "Gen   36  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   37  best=3.2335e-04 minâ€–gâ€–=2.64e-02\n",
      "Gen   38  best=3.2335e-04 minâ€–gâ€–=7.18e-03\n",
      "Gen   39  best=3.2335e-04 minâ€–gâ€–=4.29e-03\n",
      "Gen   40  best=3.2335e-04 minâ€–gâ€–=1.37e-02\n",
      "Gen   41  best=3.2335e-04 minâ€–gâ€–=9.57e-03\n",
      "Gen   42  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   43  best=3.2335e-04 minâ€–gâ€–=6.32e-03\n",
      "Gen   44  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   45  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   46  best=3.2335e-04 minâ€–gâ€–=1.29e-02\n",
      "Gen   47  best=3.2335e-04 minâ€–gâ€–=0.00e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/scipy/optimize/_nnls.py:93: RuntimeWarning: invalid value encountered in multiply\n",
      "  x, rnorm, info = _nnls(A, b, maxiter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen   48  best=3.2335e-04 minâ€–gâ€–=nan\n",
      "Gen   49  best=3.2335e-04 minâ€–gâ€–=2.47e-02\n",
      "Gen   50  best=3.2335e-04 minâ€–gâ€–=8.68e-03\n",
      "Gen   51  best=3.2335e-04 minâ€–gâ€–=8.56e-03\n",
      "Gen   52  best=3.2335e-04 minâ€–gâ€–=7.14e-03\n",
      "Gen   53  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   54  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   55  best=1.9488e-04 minâ€–gâ€–=7.60e-03\n",
      "Gen   56  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   57  best=1.9488e-04 minâ€–gâ€–=4.49e-03\n",
      "Gen   58  best=1.9488e-04 minâ€–gâ€–=5.57e-03\n",
      "Gen   59  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   60  best=1.9488e-04 minâ€–gâ€–=1.08e-02\n",
      "Gen   61  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   62  best=1.9488e-04 minâ€–gâ€–=7.90e-03\n",
      "Gen   63  best=1.9488e-04 minâ€–gâ€–=2.32e-02\n",
      "Gen   64  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   65  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   66  best=1.9488e-04 minâ€–gâ€–=1.44e-02\n",
      "Gen   67  best=1.9488e-04 minâ€–gâ€–=6.18e-03\n",
      "Gen   68  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   69  best=1.9488e-04 minâ€–gâ€–=2.30e-03\n",
      "Gen   70  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   71  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   72  best=1.9488e-04 minâ€–gâ€–=1.30e-02\n",
      "Gen   73  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   74  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   75  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   76  best=1.9488e-04 minâ€–gâ€–=1.07e-02\n",
      "Gen   77  best=1.9488e-04 minâ€–gâ€–=2.69e-03\n",
      "Gen   78  best=1.9488e-04 minâ€–gâ€–=6.87e-03\n",
      "Gen   79  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   80  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   81  best=1.9488e-04 minâ€–gâ€–=5.79e-03\n",
      "Gen   82  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   83  best=1.9488e-04 minâ€–gâ€–=1.18e-02\n",
      "Gen   84  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   85  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   86  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   87  best=1.9488e-04 minâ€–gâ€–=1.07e-02\n",
      "Gen   88  best=1.9488e-04 minâ€–gâ€–=3.13e-03\n",
      "Gen   89  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   90  best=1.9488e-04 minâ€–gâ€–=7.60e-03\n",
      "Gen   91  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   92  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   93  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   94  best=1.9488e-04 minâ€–gâ€–=6.21e-03\n",
      "Gen   95  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen   96  best=1.9488e-04 minâ€–gâ€–=nan\n",
      "Gen   97  best=1.9488e-04 minâ€–gâ€–=9.47e-03\n",
      "Gen   98  best=1.9488e-04 minâ€–gâ€–=6.21e-03\n",
      "Gen   99  best=1.9488e-04 minâ€–gâ€–=3.81e-03\n",
      "Gen  100  best=1.9488e-04 minâ€–gâ€–=1.50e-02\n",
      "Gen  101  best=1.9488e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  102  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  103  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  104  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  105  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  106  best=1.8855e-04 minâ€–gâ€–=8.60e-03\n",
      "Gen  107  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  108  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  109  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  110  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  111  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  112  best=1.8855e-04 minâ€–gâ€–=4.15e-03\n",
      "Gen  113  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  114  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  115  best=1.8855e-04 minâ€–gâ€–=7.76e-03\n",
      "Gen  116  best=1.8855e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  117  best=1.8818e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  118  best=1.8818e-04 minâ€–gâ€–=1.11e-02\n",
      "Gen  119  best=1.8818e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  120  best=1.8818e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  121  best=1.8818e-04 minâ€–gâ€–=1.24e-02\n",
      "Gen  122  best=1.8818e-04 minâ€–gâ€–=8.08e-03\n",
      "Gen  123  best=1.7513e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  124  best=1.4252e-04 minâ€–gâ€–=1.56e-03\n",
      "Gen  125  best=1.4252e-04 minâ€–gâ€–=7.52e-03\n",
      "Gen  126  best=1.4252e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  127  best=1.4252e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  128  best=1.4252e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  129  best=1.4252e-04 minâ€–gâ€–=7.02e-03\n",
      "Gen  130  best=1.4252e-04 minâ€–gâ€–=1.03e-02\n",
      "Gen  131  best=1.4252e-04 minâ€–gâ€–=8.65e-03\n",
      "Gen  132  best=1.4252e-04 minâ€–gâ€–=2.68e-03\n",
      "Gen  133  best=1.4252e-04 minâ€–gâ€–=7.78e-03\n",
      "Gen  134  best=1.4252e-04 minâ€–gâ€–=5.72e-03\n",
      "Gen  135  best=1.4252e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  136  best=1.4252e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  137  best=1.4252e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  138  best=1.4252e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  139  best=1.4172e-04 minâ€–gâ€–=8.14e-03\n",
      "Gen  140  best=1.4172e-04 minâ€–gâ€–=1.16e-02\n",
      "Gen  141  best=1.4172e-04 minâ€–gâ€–=6.44e-03\n",
      "Gen  142  best=1.4172e-04 minâ€–gâ€–=2.01e-03\n",
      "Gen  143  best=1.4172e-04 minâ€–gâ€–=5.18e-03\n",
      "Gen  144  best=1.4172e-04 minâ€–gâ€–=5.50e-03\n",
      "Gen  145  best=1.4172e-04 minâ€–gâ€–=4.39e-03\n",
      "Gen  146  best=1.4172e-04 minâ€–gâ€–=1.09e-02\n",
      "Gen  147  best=1.4172e-04 minâ€–gâ€–=7.84e-03\n",
      "Gen  148  best=1.4172e-04 minâ€–gâ€–=4.17e-03\n",
      "Gen  149  best=1.4172e-04 minâ€–gâ€–=3.36e-03\n",
      "Gen  150  best=1.4172e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  151  best=1.4172e-04 minâ€–gâ€–=2.50e-03\n",
      "Gen  152  best=1.4172e-04 minâ€–gâ€–=5.51e-03\n",
      "Gen  153  best=1.2178e-04 minâ€–gâ€–=1.39e-02\n",
      "Gen  154  best=1.2178e-04 minâ€–gâ€–=2.97e-03\n",
      "Gen  155  best=1.2178e-04 minâ€–gâ€–=6.36e-03\n",
      "Gen  156  best=1.1367e-04 minâ€–gâ€–=1.42e-03\n",
      "Gen  157  best=1.1367e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  158  best=1.1367e-04 minâ€–gâ€–=8.99e-03\n",
      "Gen  159  best=1.1367e-04 minâ€–gâ€–=8.36e-03\n",
      "Gen  160  best=1.1367e-04 minâ€–gâ€–=4.51e-03\n",
      "Gen  161  best=1.1367e-04 minâ€–gâ€–=5.26e-03\n",
      "Gen  162  best=1.1367e-04 minâ€–gâ€–=2.69e-03\n",
      "Gen  163  best=1.1367e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  164  best=1.1367e-04 minâ€–gâ€–=8.59e-03\n",
      "Gen  165  best=1.1367e-04 minâ€–gâ€–=2.50e-03\n",
      "Gen  166  best=1.1367e-04 minâ€–gâ€–=4.46e-03\n",
      "Gen  167  best=1.1367e-04 minâ€–gâ€–=6.37e-03\n",
      "Gen  168  best=1.1367e-04 minâ€–gâ€–=9.02e-03\n",
      "Gen  169  best=1.1367e-04 minâ€–gâ€–=9.12e-03\n",
      "Gen  170  best=1.1367e-04 minâ€–gâ€–=1.43e-02\n",
      "Gen  171  best=1.1367e-04 minâ€–gâ€–=4.09e-03\n",
      "Gen  172  best=1.1367e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  173  best=1.1367e-04 minâ€–gâ€–=0.00e+00\n",
      "Gen  174  best=1.1367e-04 minâ€–gâ€–=1.89e-02\n",
      "Gen  175  best=1.1367e-04 minâ€–gâ€–=9.09e-03\n",
      "Gen  176  best=1.1367e-04 minâ€–gâ€–=4.62e-03\n",
      "Gen  177  best=9.3738e-05 minâ€–gâ€–=0.00e+00\n",
      "Gen  178  best=9.3738e-05 minâ€–gâ€–=1.58e-02\n",
      "Gen  179  best=9.3738e-05 minâ€–gâ€–=8.10e-03\n",
      "Gen  180  best=9.3738e-05 minâ€–gâ€–=1.29e-02\n",
      "Gen  181  best=9.3738e-05 minâ€–gâ€–=5.49e-03\n",
      "Gen  182  best=9.3738e-05 minâ€–gâ€–=6.57e-03\n",
      "Gen  183  best=9.3738e-05 minâ€–gâ€–=9.60e-03\n",
      "Gen  184  best=9.3738e-05 minâ€–gâ€–=8.81e-03\n",
      "Gen  185  best=9.3738e-05 minâ€–gâ€–=1.27e-02\n",
      "Gen  186  best=9.3738e-05 minâ€–gâ€–=9.54e-03\n",
      "Gen  187  best=9.3738e-05 minâ€–gâ€–=9.02e-03\n",
      "Gen  188  best=9.3738e-05 minâ€–gâ€–=9.09e-03\n",
      "Gen  189  best=9.3738e-05 minâ€–gâ€–=7.19e-03\n",
      "Gen  190  best=9.3738e-05 minâ€–gâ€–=1.25e-02\n",
      "Gen  191  best=9.3738e-05 minâ€–gâ€–=3.69e-03\n",
      "Gen  192  best=9.3738e-05 minâ€–gâ€–=1.13e-02\n",
      "Gen  193  best=9.3738e-05 minâ€–gâ€–=2.38e-02\n",
      "Gen  194  best=9.3738e-05 minâ€–gâ€–=4.67e-03\n",
      "Gen  195  best=9.3738e-05 minâ€–gâ€–=1.02e-02\n",
      "Gen  196  best=9.3738e-05 minâ€–gâ€–=8.27e-03\n",
      "Gen  197  best=9.3738e-05 minâ€–gâ€–=5.21e-03\n",
      "Gen  198  best=9.3738e-05 minâ€–gâ€–=6.72e-03\n",
      "Gen  199  best=9.3738e-05 minâ€–gâ€–=9.28e-03\n",
      "Gen  200  best=9.3738e-05 minâ€–gâ€–=9.53e-03\n",
      "Gen  201  best=9.3738e-05 minâ€–gâ€–=5.05e-03\n",
      "Gen  202  best=9.3738e-05 minâ€–gâ€–=1.29e-02\n",
      "Gen  203  best=9.3738e-05 minâ€–gâ€–=1.23e-02\n",
      "Gen  204  best=9.3738e-05 minâ€–gâ€–=1.01e-02\n",
      "Gen  205  best=9.3738e-05 minâ€–gâ€–=1.62e-02\n",
      "Gen  206  best=9.3738e-05 minâ€–gâ€–=6.85e-03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m init_mean = np.linspace(-\u001b[32m1.6\u001b[39m, \u001b[32m9.3\u001b[39m, n_terms)   \u001b[38;5;66;03m# rough guess â‡’ b in ~[0.1, 400]\u001b[39;00m\n\u001b[32m     85\u001b[39m init_std = np.full(n_terms, \u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m best_b_log, best_score, de_info = \u001b[43mdifferential_evolution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mce_user_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_std\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCR\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_g\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstall_generations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     99\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# --- final reconstruction of a and objective ----------------------------\u001b[39;00m\n\u001b[32m    102\u001b[39m best_b = np.exp(best_b_log)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mdifferential_evolution\u001b[39m\u001b[34m(user_fn, initial_mean, initial_std, pop_size, F, CR, n_iters, random_state, tol_g, stall_generations, verbose)\u001b[39m\n\u001b[32m     59\u001b[39m     trial[i] = np.where(cross_pts, mutant, pop[i])\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# ------------ evaluate trial population ------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m trial, trial_scores, info_trial = \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# ------------ selection ----------------------------------------------\u001b[39;00m\n\u001b[32m     65\u001b[39m improved = trial_scores < scores\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mce_user_fn\u001b[39m\u001b[34m(b_log_samples)\u001b[39m\n\u001b[32m     45\u001b[39m p0 = np.concatenate([a_opt, b_pos])      \u001b[38;5;66;03m# shape (2K,)\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# ---- (2) limited Newton polishing ----------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m p_new, gnorm, *_ = \u001b[43mnewton\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewton_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol_step\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstall_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewton_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mls_bounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.any(np.isnan(p_new)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.any(np.isinf(p_new)):\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# ---- (3) extract & sort b (log) ------------------------------------\u001b[39;00m\n\u001b[32m     63\u001b[39m     b_new_pos = np.clip(p_new[K:], \u001b[32m1e-12\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# keep positivity\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mnewton\u001b[39m\u001b[34m(f, grad, hess, x0, tol_grad, tol_step, stall_iter, max_iter, ls_bounds, verbose)\u001b[39m\n\u001b[32m    130\u001b[39m     trust *= \u001b[32m2\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Backtracking line search\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m alpha, fx = \u001b[43mgolden_section_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mls_bounds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mls_bounds\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m    135\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Update\u001b[39;00m\n\u001b[32m    138\u001b[39m x_new = x + alpha * p\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mgolden_section_search\u001b[39m\u001b[34m(f, a, b, tol, max_iter)\u001b[39m\n\u001b[32m     44\u001b[39m         a, c, f_c = c, d, f_d\n\u001b[32m     45\u001b[39m         d = a + phi * (b - a)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m         f_d = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Choose the best of the final points\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f_c < f_d:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mnewton.<locals>.<lambda>\u001b[39m\u001b[34m(alpha)\u001b[39m\n\u001b[32m    130\u001b[39m     trust *= \u001b[32m2\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Backtracking line search\u001b[39;00m\n\u001b[32m    132\u001b[39m alpha, fx = golden_section_search(\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m alpha: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    134\u001b[39m     a=ls_bounds[\u001b[32m0\u001b[39m], b=ls_bounds[\u001b[32m1\u001b[39m], tol=\u001b[32m1e-6\u001b[39m, max_iter=\u001b[32m100\u001b[39m\n\u001b[32m    135\u001b[39m )\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Update\u001b[39;00m\n\u001b[32m    138\u001b[39m x_new = x + alpha * p\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mobj\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobj\u001b[39m(p):\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# ensure NumPyâ†’JAX conversion to float64\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(obj_jax_jit(\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:5507\u001b[39m, in \u001b[36marray\u001b[39m\u001b[34m(object, dtype, copy, order, ndmin, device)\u001b[39m\n\u001b[32m   5503\u001b[39m       device_id = cuda_plugin_extension.get_device_ordinal(cai[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m   5504\u001b[39m     \u001b[38;5;28mobject\u001b[39m = xc._xla.cuda_array_interface_to_buffer(\n\u001b[32m   5505\u001b[39m         cai=cai, gpu_backend=backend, device_id=device_id)\n\u001b[32m-> \u001b[39m\u001b[32m5507\u001b[39m \u001b[38;5;28mobject\u001b[39m = \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mleaf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__jax_array__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5508\u001b[39m \u001b[43m                  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleaf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m__jax_array__\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mleaf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   5509\u001b[39m leaves = tree_leaves(\u001b[38;5;28mobject\u001b[39m, is_leaf=\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   5510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(leaf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m leaf \u001b[38;5;129;01min\u001b[39;00m leaves):\n\u001b[32m   5511\u001b[39m   \u001b[38;5;66;03m# Added Nov 16 2023\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/jax/_src/tree_util.py:358\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(f, tree, is_leaf, *rest)\u001b[39m\n\u001b[32m    356\u001b[39m leaves, treedef = tree_flatten(tree, is_leaf)\n\u001b[32m    357\u001b[39m all_leaves = [leaves] + [treedef.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[43m.\u001b[49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "#  TEST SCRIPT  â€”  optimise log-b with the same `ce_user_fn`\n",
    "###############################################################################\n",
    "\n",
    "# --- problem & helper already defined earlier ---------------------------\n",
    "# n_terms, ce_user_fn, d, w, target, obj â€¦ are assumed to be in scope\n",
    "\n",
    "pop_size = 100\n",
    "rng_seed = 1234\n",
    "\n",
    "# --- problem size -----------------------------------------------------------\n",
    "n_terms = 8          # K  (= length of b and a)\n",
    "newton_steps = 1     # how many Newton iterations per sample\n",
    "np.random.seed(1)    # reproducibility\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  helper: CE user-defined update function operating on log-b only\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def ce_user_fn(b_log_samples: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    b_log_samples : (N, K) ndarray\n",
    "        Current population in *log* space.\n",
    "    Returns\n",
    "    -------\n",
    "    updated_log_b : (N, K) ndarray\n",
    "        Polished & sorted log-b that CE will keep using.\n",
    "    scores        : (N,)  ndarray\n",
    "        Objective values after polishing (smaller is better).\n",
    "    info          : dict\n",
    "        Contains minimum gradient norm among all samples (for CE stopping).\n",
    "    \"\"\"\n",
    "    N, K = b_log_samples.shape\n",
    "    updated = np.empty_like(b_log_samples)\n",
    "    scores = np.empty(N)\n",
    "    grad_norm = np.empty(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        # ---- (1) build starting point --------------------------------------\n",
    "        b_pos = np.exp(b_log_samples[i])            # (K,)  positive\n",
    "        a_opt = optimal_a(d, w, target, b_pos)      # (K,)\n",
    "        p0 = np.concatenate([a_opt, b_pos])      # shape (2K,)\n",
    "\n",
    "        # ---- (2) limited Newton polishing ----------------------------------\n",
    "        p_new, gnorm, *_ = newton(\n",
    "            f=obj,\n",
    "            grad=grad,\n",
    "            hess=hess,\n",
    "            x0=p0,\n",
    "            max_iter=newton_steps,\n",
    "            tol_grad=1e-9,\n",
    "            tol_step=1e-9,\n",
    "            stall_iter=newton_steps,\n",
    "            ls_bounds=(0., 1.),\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        if not np.any(np.isnan(p_new)) and not np.any(np.isinf(p_new)):\n",
    "            # ---- (3) extract & sort b (log) ------------------------------------\n",
    "            b_new_pos = np.clip(p_new[K:], 1e-12, None)  # keep positivity\n",
    "            b_new_log = np.log(b_new_pos)\n",
    "            order = np.argsort(b_new_log)\n",
    "            a_opt = optimal_a(d, w, target, b_new_pos)      # (K,)\n",
    "            p0 = np.concatenate([a_opt, b_new_pos])      # shape (2K,)\n",
    "\n",
    "            updated[i] = b_new_log[order]\n",
    "            scores[i] = obj(p0)       # score *after* polishing\n",
    "            grad_norm[i] = np.linalg.norm(grad(p0))\n",
    "        else:\n",
    "            # if polishing failed, keep the original sample\n",
    "            updated[i] = b_log_samples[i]\n",
    "            scores[i] = np.inf\n",
    "            grad_norm[i] = np.inf\n",
    "\n",
    "    info = {'normg': float(grad_norm.min())}\n",
    "    return updated, scores, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "init_mean = np.linspace(-1.6, 9.3, n_terms)   # rough guess â‡’ b in ~[0.1, 400]\n",
    "init_std = np.full(n_terms, 1.0)\n",
    "\n",
    "best_b_log, best_score, de_info = differential_evolution(\n",
    "    user_fn=ce_user_fn,\n",
    "    initial_mean=init_mean,\n",
    "    initial_std=init_std,\n",
    "    pop_size=pop_size,\n",
    "    F=0.8,\n",
    "    CR=0.9,\n",
    "    n_iters=5000,\n",
    "    random_state=rng_seed,\n",
    "    tol_g=-1e-9,\n",
    "    stall_generations=75,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- final reconstruction of a and objective ----------------------------\n",
    "best_b = np.exp(best_b_log)\n",
    "best_a = optimal_a(d, w, target, best_b)\n",
    "best_obj = obj(np.concatenate([best_a, best_b]))\n",
    "\n",
    "print(\"\\n====================  DE RESULT  ====================\")\n",
    "print(\"best log-b :\", best_b_log)\n",
    "print(\"best b     :\", best_b)\n",
    "print(\"best a     :\", best_a)\n",
    "print(\"DE score   :\", best_score)\n",
    "print(\"check obj  :\", best_obj)\n",
    "print(\"generations:\", de_info['iter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.log(x[:n_terms]), label='a_i')\n",
    "plt.plot(np.log(x[n_terms:]), label='b_i')\n",
    "plt.xlabel('Parameter index')\n",
    "plt.ylabel('Parameter value (log scale for a_i)')\n",
    "plt.title('Optimised Parameters')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# Plot the convergence of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = grad(x)\n",
    "H = hess(x)\n",
    "d = np.linalg.solve(H, -g)  # Newton step\n",
    "dir_obj = lambda alpha: obj(x + alpha * d)\n",
    "alphas = np.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(alphas, [dir_obj(alpha) for alpha in alphas])\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"Objective along Newton direction\")\n",
    "plt.title(\"Line search objective along Newton step\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae89cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 5.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "f_min_all = []\n",
    "min_b = -1.0\n",
    "max_b = 1.5\n",
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "for n_terms in range(2, 16):               # number of (a_i, b_i) pairs\n",
    "\n",
    "    b = np.linspace(min_b * 1.05, max_b * 1.1, n_terms)\n",
    "    a = optimal_a(d, w, target, np.exp(b))\n",
    "    # print(\"Optimal a:\", a)\n",
    "\n",
    "    means = np.concatenate((a, np.exp(b)))  # initial guess in log-space\n",
    "\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    x, grad_norm, f_history, grad_norm_history = newton(\n",
    "        f=obj,\n",
    "        grad=grad,\n",
    "        hess=hess,\n",
    "        x0=means,\n",
    "        tol_grad=1e-5,\n",
    "        tol_step=1e-8,\n",
    "        stall_iter=50,\n",
    "        max_iter=1000,\n",
    "        ls_bounds=(-2.0, 2.0)\n",
    "    )\n",
    "    min_b = min(np.log(x[n_terms:]))\n",
    "    max_b = max(np.log(x[n_terms:]))\n",
    "    print(f\"Min a log: {min_b}, Max b log: {max_b}\")\n",
    "    print(\"Final objective value:\", f_history[-1])\n",
    "    f_min_all.append(f_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f_min_all)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1020ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- synthetic data ---------------------------------------------------\n",
    "\n",
    "N = 500               # data points\n",
    "\n",
    "d, w = rectangle_rule(0.0, 2.0, N)\n",
    "target = np.exp(-d)                  # any reference curve you like\n",
    "best_init = np.array([0,0])\n",
    "\n",
    "for n_terms in range(1,10):                 # number of (a_i, b_i) pairs\n",
    "    means = best_init  # initial mean for (a_i, b_i)\n",
    "    cov = np.eye(2 * n_terms) * 4  # initial covariance matrix\n",
    "    # --- optimiser call ---------------------------------------------------\n",
    "    best, stats = cross_entropy_numba(\n",
    "        d=d, target=target, w=w,\n",
    "        n_terms=n_terms,\n",
    "        max_iter=10000,\n",
    "        pop_size=10000,\n",
    "        elite_frac=0.1,\n",
    "        mean=means,\n",
    "        cov=cov\n",
    "    )\n",
    "\n",
    "    print(\"Best score :\", stats[\"best_score\"])\n",
    "    print(\"Best params:\", best)\n",
    "    print(\"Iterations :\", stats[\"iterations\"])\n",
    "    print(\"Runtime    :\", stats[\"runtime\"], \"s\")\n",
    "\n",
    "    first_half = best[:n_terms]\n",
    "    second_half = best[n_terms:]\n",
    "\n",
    "    # Original equidistant indices (0 to n_terms-1), new indices from 0 to n_terms\n",
    "    x_old = np.linspace(0, 1, n_terms)\n",
    "    x_new = np.linspace(0, 1, n_terms + 1)\n",
    "\n",
    "    interp_first = np.interp(x_new, x_old, first_half)\n",
    "    interp_second = np.interp(x_new, x_old, second_half)\n",
    "\n",
    "    best_init = np.hstack([interp_first, interp_second])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d75dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(stats[\"history\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Best score\")\n",
    "plt.title(\"Convergence of the Cross-Entropy Method\")\n",
    "plt.grid()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best[n_terms:], 'o-')\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"b_i (log scale)\")\n",
    "plt.title(\"Fitted b_i parameters\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f276bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import lsq_linear\n",
    "\n",
    "# compute using least squares the coefficients for approximation with squared exponential kernels\n",
    "# Construct the design matrix for squared exponential kernels\n",
    "A = np.zeros((len(d), n_terms))\n",
    "b_params = np.exp(best[n_terms:])\n",
    "\n",
    "for i in range(n_terms):\n",
    "    A[:, i] = np.exp(-b_params[i] * d**2)\n",
    "\n",
    "# Solve the weighted least squares problem: minimize ||W(Aa - target)||^2\n",
    "W = np.sqrt(w)\n",
    "Aw = A * W[:, None]\n",
    "tw = target * W\n",
    "\n",
    "res = lsq_linear(Aw, tw, bounds=(0, np.inf))\n",
    "squared_exponential_coeffs = res.x\n",
    "print(\"Squared exponential coefficients:\", np.log(squared_exponential_coeffs))\n",
    "\n",
    "# compute fit with this coefficients\n",
    "params = np.hstack([np.log(squared_exponential_coeffs), np.log(b_params)])\n",
    "\n",
    "res = objective_numba(np.array([params]), d, target, w)\n",
    "print(\"Objective value with least squares coefficients:\", res[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, w = rectangle_rule(0.0, 2.0, 500)\n",
    "f = lambda t: np.exp(-t)\n",
    "a, b, info = fit_exp_sum_ce(5, x, w, f, iterations=2000, pop_size=1000)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('best_score:', info.best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
